---
description: 
globs: 
alwaysApply: true
---
(Files content cropped to 300k characters, download full ingest to see more)
================================================
FILE: README.md
================================================
# 🚀 Syncable IaC CLI

> Automatically generate optimized Docker, Kubernetes, and cloud infrastructure configurations by analyzing your codebase.

[![Rust](https://img.shields.io/badge/rust-%23000000.svg?style=for-the-badge&logo=rust&logoColor=white)](https://www.rust-lang.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


## ⚡ Quick Start
[![Crates.io Downloads](https://img.shields.io/crates/d/syncable-cli)](https://crates.io/crates/syncable-cli)

**Syncable IaC CLI** analyzes your project and automatically generates production-ready infrastructure configurations. Supporting **260+ technologies** across 5 major language ecosystems, it understands your stack and creates optimized IaC files tailored to your specific needs.

## ⚡ Quick Start


```bash
# Install (Cross-platform)
cargo install syncable-cli

# Windows users can also use:
# powershell -c "iwr -useb https://raw.githubusercontent.com/syncable-dev/syncable-cli/main/install.ps1 | iex"

# Analyze any project
sync-ctl analyze /path/to/your/project  # Unix/Linux/macOS
sync-ctl analyze C:\path\to\your\project  # Windows

# Check for vulnerabilities
sync-ctl vulnerabilities

# Run security analysis (multiple modes available)
sync-ctl security                   # Thorough scan (default)
sync-ctl security --mode lightning  # Ultra-fast critical files only
sync-ctl security --mode paranoid   # Most comprehensive scan

# Force update check (clears cache)
sync-ctl --clear-update-cache analyze .


# Get help with any command
sync-ctl --help                     # Show all available commands
sync-ctl analyze --help            # Show analyze command options
sync-ctl security --help           # Show security scanning options
sync-ctl vulnerabilities --help    # Show vulnerability check options
```

That's it! The CLI will detect your languages, frameworks, dependencies, and provide detailed insights about your project structure. The tool includes smart update notifications to keep you on the latest version.

## 🎯 What It Does

Syncable IaC CLI is like having a DevOps expert analyze your codebase:

1. **📊 Analyzes** - Detects languages, frameworks, dependencies, ports, and architecture patterns
2. **🔍 Audits** - Checks for security vulnerabilities and configuration issues  
3. **🚀 Generates** - Creates optimized Dockerfiles, Compose files, and Terraform configs (coming soon)

### Example Output

```bash
$ sync-ctl analyze ./my-express-app

═══════════════════════════════════════════════════════════════════════════════════════════════════
📊 PROJECT ANALYSIS DASHBOARD
═══════════════════════════════════════════════════════════════════════════════════════════════════

┌─ Architecture Overview ──────────────────────────────────────────────────────┐
│ Type:                                                         Single Project │
│ Pattern:                                                           Fullstack │
│ Full-stack app with frontend/backend  separation                             │
└──────────────────────────────────────────────────────────────────────────────┘

┌─ Technology Stack ───────────────────────────────────────────────────────────┐
│ Languages:                                           JavaScript, TypeScript  │
│ Frameworks:                                    Express, React, Tailwind CSS  │
│ Databases:                                                PostgreSQL, Redis  │
└──────────────────────────────────────────────────────────────────────────────┘
```

## 📋 Key Features

### 🔍 Comprehensive Analysis
- **Multi-language support** - JavaScript/TypeScript, Python, Rust, Go, Java/Kotlin
- **260+ technologies** - From React to Spring Boot, Django to Actix-web
- **Architecture detection** - Monolithic, microservices, serverless, and more
- **Monorepo support** - Analyzes complex multi-project repositories

### 🛡️ Turbo Security Engine (Covering Javascript / Python ---- Rust-, Go- & Java- Coming soon)
- **10-100x faster scanning** - Rust-powered multi-pattern matching with smart file discovery
- **5 scan modes** - From lightning-fast critical checks to comprehensive audits
- **Smart gitignore analysis** - Understands git status and provides risk assessments
- **260+ secret patterns** - Detects API keys, tokens, certificates, and credentials
- **Zero false positives** - Advanced context-aware filtering excludes test data and documentation

### 🐳 Docker Intelligence
- **Dockerfile analysis** - Understand existing Docker configurations
- **Multi-stage detection** - Identifies build optimization patterns
- **Service mapping** - Traces dependencies between containers
- **Network topology** - Visualizes service communication

### 🔄 Smart Update System
- **Intelligent caching** - Checks every 2 hours when no update available
- **Immediate notifications** - Shows updates instantly when available
- **Clear instructions** - Provides multiple update methods with step-by-step guidance
- **Zero-maintenance** - Automatically keeps you informed of new releases

## 🛠️ Installation

### Via Cargo (Recommended - Cross Platform)
```bash
cargo install syncable-cli
```

### Quick Install Scripts

#### Linux/macOS
```bash
curl -sSL https://install.syncable.dev | sh
```

#### Windows (PowerShell)
```powershell
# Download and run the PowerShell installer
iwr -useb https://raw.githubusercontent.com/syncable-dev/syncable-cli/main/install.ps1 | iex

# Or download first and run (safer)
Invoke-WebRequest -Uri https://raw.githubusercontent.com/syncable-dev/syncable-cli/main/install.ps1 -OutFile install.ps1
powershell -ExecutionPolicy Bypass -File install.ps1
```

### From Source
```bash
git clone https://github.com/syncable-dev/syncable-cli.git
cd syncable-cli
cargo install --path .
```

### Platform-Specific Notes

**Windows Users:**
- **Rust**: Install from [rustup.rs](https://rustup.rs/) if you don't have it
- **PATH**: Cargo installs to `%USERPROFILE%\.cargo\bin` - add to PATH if needed
- **Tools**: Some security tools may require manual installation or package managers like Scoop/Chocolatey

**Linux/macOS Users:**
- Most security tools can be auto-installed via the installer script
- Tools are installed to `~/.local/bin` which may need to be added to your PATH

## 📖 Usage Guide

### Basic Commands

```bash
# Analyze with different display formats
sync-ctl analyze                    # Matrix view (default)
sync-ctl analyze --display detailed  # Detailed view
sync-ctl analyze --json             # JSON output

# Vulnerabilities analysis
sync-ctl vulnerabilities            # Dependency vulnerability scan

# Security analysis with turbo engine (10-100x faster)
sync-ctl security                   # Thorough scan (default) 
sync-ctl security --mode lightning  # Critical files only (.env, configs)
sync-ctl security --mode fast       # Smart sampling with priority patterns
sync-ctl security --mode balanced   # Good coverage with optimizations
sync-ctl security --mode paranoid   # Most comprehensive including low-severity
sync-ctl vulnerabilities            # Dependency vulnerability scan

# Dependency analysis
sync-ctl dependencies --licenses    # Show license information
sync-ctl dependencies --vulnerabilities  # Check for known CVEs
```

### Security Scan Modes

The turbo security engine offers 5 scan modes optimized for different use cases:

| Mode | Speed | Coverage | Use Case | Typical Time |
|------|-------|----------|----------|--------------|
| **Lightning** | 🚀 Fastest | Critical files only | Pre-commit hooks, CI checks 
| **Fast** | ⚡ Very Fast | Smart sampling | Development workflow 
| **Balanced** | 🎯 Optimized | Good coverage | Regular security checks 
| **Thorough** | 🔍 Complete | Comprehensive | Security audits (default) 
| **Paranoid** | 🕵️ Maximum | Everything + low severity | Compliance, releases 

## 🛡️ Security Detection Deep Dive

### What We Detect

The turbo security engine scans for 260+ secret patterns across multiple categories:

#### 🔑 API Keys & Tokens
- **Cloud Providers**: AWS Access Keys, GCP Service Account Keys, Azure Storage Keys
- **Services**: Stripe API Keys, Twilio Auth Tokens, GitHub Personal Access Tokens
- **Databases**: MongoDB Connection Strings, Redis URLs, PostgreSQL passwords
- **CI/CD**: Jenkins API Tokens, CircleCI Keys, GitLab CI Variables

#### 🔐 Cryptographic Material  
- **Private Keys**: RSA, ECDSA, Ed25519 private keys (.pem, .key files)
- **Certificates**: X.509 certificates, SSL/TLS certs
- **Keystores**: Java KeyStore files, PKCS#12 files
- **SSH Keys**: OpenSSH private keys, SSH certificates

#### 📧 Authentication Secrets
- **JWT Secrets**: JSON Web Token signing keys
- **OAuth**: Client secrets, refresh tokens
- **SMTP**: Email server credentials, SendGrid API keys
- **LDAP**: Bind credentials, directory service passwords

#### 🌐 Environment Variables
- **Suspicious Names**: Any variable containing "password", "secret", "key", "token"
- **Base64 Encoded**: Automatically detects encoded secrets
- **URLs with Auth**: Database URLs, API endpoints with embedded credentials

### Smart Git Status Analysis

Our security engine provides intelligent risk assessment based on git status:

| Status | Risk Level | Meaning | Action Needed |
|--------|------------|---------|---------------|
| 🟢 **SAFE** | Low | File properly ignored by .gitignore | ✅ No action needed |
| 🔵 **OK** | Low | File appears safe for version control | ✅ Monitor for changes |
| 🟡 **EXPOSED** | High | Contains secrets but NOT in .gitignore | ⚠️ Add to .gitignore immediately |
| 🔴 **TRACKED** | Critical | Contains secrets AND tracked by git | 🚨 Remove from git history |

#### Why Some Files Are "OK" Despite Not Being Gitignored

Files are marked as **OK** when they contain patterns that look like secrets but are actually safe:

- **Documentation**: Code in README files, API examples, tutorials
- **Test Data**: Mock API keys, placeholder values, example configurations  
- **Source Code**: String literals that match patterns but aren't real secrets
- **Lock Files**: Package hashes in `package-lock.json`, `pnpm-lock.yaml`, `cargo.lock`
- **Build Artifacts**: Compiled code, minified files, generated documentation

### Advanced False Positive Filtering

Our engine uses sophisticated techniques to minimize false positives:

#### 🎯 Context-Aware Detection
```bash
# ❌ FALSE POSITIVE - Will be ignored
const API_KEY = "your_api_key_here";  // Documentation example
const EXAMPLE_TOKEN = "sk-example123"; // Clearly a placeholder

# ✅ REAL SECRET - Will be detected  
const STRIPE_KEY = "sk_live_4eC39HqLyjWDarjtT1zdp7dc";
```

#### 📝 Documentation Exclusions
- Comments in any language (`//`, `#`, `/* */`, `<!-- -->`)
- Markdown code blocks and documentation files
- README files, CHANGELOG, API docs
- Example configurations and sample files

#### 🧪 Test Data Recognition
- Files in `/test/`, `/tests/`, `/spec/`, `__test__` directories
- Filenames containing "test", "spec", "mock", "fixture", "example"
- Common test patterns like "test123", "dummy", "fake"

#### 📦 Dependency File Intelligence
- Automatically excludes: `node_modules/`, `vendor/`, `target/`
- Recognizes lock files: `yarn.lock`, `pnpm-lock.yaml`, `go.sum`
- Skips binary files, images, and compiled artifacts

### Display Modes

Choose the output format that works best for you:

- **Matrix** (default) - Compact dashboard view
- **Detailed** - Comprehensive vertical layout  
- **Summary** - Brief overview for CI/CD
- **JSON** - Machine-readable format

### Example Security Output

```bash
$ sync-ctl security --mode thorough

🛡️  Security Analysis Results
════════════════════════════════════════════════════════════════════════════════

┌─ Security Summary ───────────────────────────────────────┐
│ Overall Score:                                    85/100 │
│ Risk Level:                                        High  │ 
│ Total Findings:                                        3 │
│ Files Analyzed:                                       47 │
│ Scan Mode:                                      Thorough │
└──────────────────────────────────────────────────────────┘

┌─ Security Findings ────────────────────────────────────────────────────────┐
│ 1. ./.env.local                                                            │
│    Type: ENV VAR | Severity: Critical | Position: 3:15 | Status: EXPOSED   │
│                                                                            │
│ 2. ./config/database.js                                                    │
│    Type: API KEY | Severity: High | Position: 12:23 | Status: TRACKED      │
│                                                                            │
│ 3. ./docs/api-example.md                                                   │
│    Type: API KEY | Severity: Critical | Position: 45:8 | Status: OK        │
└────────────────────────────────────────────────────────────────────────────┘

┌─ Key Recommendations ───────────────────────────────────────────────────────┐
│ 1. 🚨 Add .env.local to .gitignore immediately                              │
│ 2. 🔐 Move database credentials to environment variables                    │
│ 3. ✅ API example in docs is safely documented                              │
└─────────────────────────────────────────────────────────────────────────────┘

════════════════════════════════════════════════════════════════════════════════
```



### Advanced Configuration

Create `.syncable.toml` in your project root:

```toml
[analysis]
include_dev_dependencies = true
ignore_patterns = ["vendor", "node_modules", "target"]

[security]
# Scan configuration
default_mode = "thorough"              # Default scan mode
fail_on_high_severity = true           # Exit with error on high/critical findings
check_secrets = true                   # Enable secret detection
check_code_patterns = true             # Enable code security pattern analysis

# Performance tuning
max_file_size_mb = 10                  # Skip files larger than 10MB
worker_threads = 0                     # Auto-detect CPU cores (0 = auto)
enable_cache = true                    # Enable result caching
cache_size_mb = 100                    # Cache size limit

# Pattern filtering
priority_extensions = [                # Scan these extensions first
  "env", "key", "pem", "json", "yml", "yaml", 
  "toml", "ini", "conf", "config"
]
```

#### Command-Line Options

```bash
# Scan mode selection
sync-ctl security --mode lightning    # Fastest, critical files only
sync-ctl security --mode paranoid     # Slowest, most comprehensive

# Output control
sync-ctl security --json              # JSON output for automation
sync-ctl security --output report.json # Save to file

# Filtering options  
sync-ctl security --include-low       # Include low-severity findings
sync-ctl security --no-secrets        # Skip secret detection
sync-ctl security --no-code-patterns  # Skip code pattern analysis

# CI/CD integration
sync-ctl security --fail-on-findings  # Exit with error code if issues found
```

## 🌟 Technology Coverage

<details>
<summary><b>View Supported Technologies (260+)</b></summary>

### By Language

- **JavaScript/TypeScript** (46) - React, Vue, Angular, Next.js, Express, Nest.js, and more
- **Python** (76) - Django, Flask, FastAPI, NumPy, TensorFlow, PyTorch, and more
- **Java/JVM** (98) - Spring Boot, Micronaut, Hibernate, Kafka, Elasticsearch, and more
- **Go** (21) - Gin, Echo, Fiber, gRPC, Kubernetes client, and more
- **Rust** (20) - Actix-web, Axum, Rocket, Tokio, SeaORM, and more

### Package Managers
- npm, yarn, pnpm, bun (JavaScript)
- pip, poetry, pipenv, conda (Python)
- Maven, Gradle (Java)
- Cargo (Rust)
- Go modules (Go)

</details>

## 🚀 Roadmap

### ✅ Phase 1: Analysis Engine (Complete)
- Project analysis and technology detection
- Vulnerability scanning with 260+ supported packages
- Turbo Security Engine turbo-fast scanning with 5 modes

### 🔄 Phase 2: AI-Powered Generation (In Progress)
- Smart Dockerfile generation
- Intelligent Docker Compose creation
- Cloud-optimized configurations

### 📅 Future Phases
- Kubernetes manifests & Helm charts
- Terraform modules for AWS/GCP/Azure
- CI/CD pipeline generation
- Real-time monitoring setup

## 🤝 Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

```bash
# Run tests
cargo test

# Check code quality
cargo clippy

# Format code
cargo fmt
```


## 📄 License

MIT License - see [LICENSE](LICENSE) for details.

## 🙏 Acknowledgments

Built with Rust 🦀 and powered by the open-source community.

---

**Need help?** Check our [documentation](https://github.com/syncable-dev/syncable-cli/wiki) or [open an issue](https://github.com/syncable-dev/syncable-cli/issues).

[![Star on GitHub](https://img.shields.io/github/stars/syncable-dev/syncable-cli?style=social)](https://github.com/syncable-dev/syncable-cli)




================================================
FILE: Cargo.toml
================================================
[package]
name = "syncable-cli"
version = "0.9.8"
edition = "2024"
authors = ["Syncable Team"]
description = "A Rust-based CLI that analyzes code repositories and generates Infrastructure as Code configurations"
license = "MIT OR Apache-2.0"
repository = "https://github.com/syncable-dev/syncable-cli"
keywords = ["iac", "infrastructure", "docker", "terraform", "cli"]
categories = ["command-line-utilities", "development-tools"]
readme = "README.md"

# Platform support
[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu", "x86_64-pc-windows-msvc", "x86_64-apple-darwin"]

[[bin]]
name = "sync-ctl"
path = "src/main.rs"



[dependencies]
clap = { version = "4", features = ["derive", "env", "cargo"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
serde_yaml = "0.9"
toml = "0.8"
log = "0.4"
env_logger = "0.11"
thiserror = "2"
walkdir = "2"
tera = "1"
indicatif = "0.17"
regex = "1"
glob = "0.3"
once_cell = "1"
rayon = "1.7"
termcolor = "1"
chrono = { version = "0.4", features = ["serde"] }
colored = "3"
prettytable = "0.10"
term_size = "0.3"

# Vulnerability checking dependencies
rustsec = "0.30"
reqwest = { version = "0.12", features = ["json", "blocking"] }
tokio = { version = "1", features = ["rt", "macros", "rt-multi-thread"] }
textwrap = "0.16"
tempfile = "3"
dirs = "6"

# Performance dependencies for turbo security analyzer
aho-corasick = "1.1"              # Multi-pattern string matching
memmap2 = "0.9"                   # Memory-mapped file I/O
dashmap = "5"                     # Concurrent hashmap for caching
crossbeam = { version = "0.8", features = ["crossbeam-channel"] }  # High-performance channels
blake3 = "1.5"                    # Fast hashing for cache keys
regex-automata = "0.4"            # Compiled regex sets
num_cpus = "1.16"                 # CPU count detection
parking_lot = "0.12"              # Faster mutex/rwlock
ahash = "0.8"                     # Fast hash function
bstr = "1.9"                      # Byte string utilities
simdutf8 = "0.1"                  # SIMD UTF-8 validation

[dev-dependencies]
assert_cmd = "2"
predicates = "3"
tempfile = "3"
proptest = "1"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
panic = "abort"

[[example]]
name = "check_vulnerabilities"
path = "examples/check_vulnerabilities.rs"

[[example]]
name = "security_analysis"
path = "examples/security_analysis.rs"



================================================
FILE: cargo_audit_sample.json
================================================
{
  "database": {
    "advisory-count": 785,
    "last-commit": "a1f651cba8bf224f52c5d55d8182b3bb0ebce49e",
    "last-updated": "2025-06-03T13:30:36+02:00"
  },
  "lockfile": {
    "dependency-count": 437
  },
  "settings": {
    "target_arch": [],
    "target_os": [],
    "severity": null,
    "ignore": [],
    "informational_warnings": [
      "unmaintained",
      "unsound",
      "notice"
    ]
  },
  "vulnerabilities": {
    "found": true,
    "count": 2,
    "list": [
      {
        "advisory": {
          "id": "RUSTSEC-2025-0021",
          "package": "gix-features",
          "title": "SHA-1 collision attacks are not detected",
          "description": "### Summary\ngitoxide uses SHA-1 hash implementations without any collision detection, leaving it vulnerable to hash collision attacks.\n\n### Details\ngitoxide uses the `sha1_smol` or `sha1` crate, both of which implement standard SHA-1 without any mitigations for collision attacks. This means that two distinct Git objects with colliding SHA-1 hashes would break the Git object model and integrity checks when used with gitoxide.\n\nThe SHA-1 function is considered cryptographically insecure. However, in the wake of the SHAttered attacks, this issue was mitigated in Git 2.13.0 in 2017 by using the [sha1collisiondetection](https://github.com/crmarcstevens/sha1collisiondetection) algorithm by default and producing an error when known SHA-1 collisions are detected. Git is in the process of migrating to using SHA-256 for object hashes, but this has not been rolled out widely yet and gitoxide does not support SHA-256 object hashes.\n\n### PoC\nThe following program demonstrates the problem, using the two [SHAttered PDFs](https://shattered.io/):\n\n```rust\nuse sha1_checked::{CollisionResult, Digest};\n\nfn sha1_oid_of_file(filename: &str) -> gix::ObjectId {\n    let mut hasher = gix::features::hash::hasher(gix::hash::Kind::Sha1);\n    hasher.update(&std::fs::read(filename).unwrap());\n    gix::ObjectId::Sha1(hasher.digest())\n}\n\nfn sha1dc_oid_of_file(filename: &str) -> Result<gix::ObjectId, String> {\n    // Matches Git’s behaviour.\n    let mut hasher = sha1_checked::Builder::default().safe_hash(false).build();\n    hasher.update(&std::fs::read(filename).unwrap());\n    match hasher.try_finalize() {\n        CollisionResult::Ok(digest) => Ok(gix::ObjectId::Sha1(digest.into())),\n        CollisionResult::Mitigated(_) => unreachable!(),\n        CollisionResult::Collision(digest) => Err(format!(\n            \"Collision attack: {}\",\n            gix::ObjectId::Sha1(digest.into()).to_hex()\n        )),\n    }\n}\n\nfn main() {\n    dbg!(sha1_oid_of_file(\"shattered-1.pdf\"));\n    dbg!(sha1_oid_of_file(\"shattered-2.pdf\"));\n    dbg!(sha1dc_oid_of_file(\"shattered-1.pdf\"));\n    dbg!(sha1dc_oid_of_file(\"shattered-2.pdf\"));\n}\n```\n\nThe output is as follows:\n\n```\n[src/main.rs:24:5] sha1_oid_of_file(\"shattered-1.pdf\") = Sha1(38762cf7f55934b34d179ae6a4c80cadccbb7f0a)\n[src/main.rs:25:5] sha1_oid_of_file(\"shattered-2.pdf\") = Sha1(38762cf7f55934b34d179ae6a4c80cadccbb7f0a)\n[src/main.rs:26:5] sha1dc_oid_of_file(\"shattered-1.pdf\") = Err(\n    \"Collision attack: 38762cf7f55934b34d179ae6a4c80cadccbb7f0a\",\n)\n[src/main.rs:27:5] sha1dc_oid_of_file(\"shattered-2.pdf\") = Err(\n    \"Collision attack: 38762cf7f55934b34d179ae6a4c80cadccbb7f0a\",\n)\n```\n\nThe latter behaviour matches Git.\n\nSince the SHAttered PDFs are not in a valid format for Git objects, a direct proof‐of‐concept using higher‐level APIs cannot be immediately demonstrated without significant computational resources.\n\n### Impact\nAn attacker with the ability to mount a collision attack on SHA-1 like the [SHAttered](https://shattered.io/) or [SHA-1 is a Shambles](https://sha-mbles.github.io/) attacks could create two distinct Git objects with the same hash. This is becoming increasingly affordable for well‐resourced attackers, with the Shambles researchers in 2020 estimating $45k for a chosen‐prefix collision or $11k for a classical collision, and projecting less than $10k for a chosen‐prefix collision by 2025. The result could be used to disguise malicious repository contents, or potentially exploit assumptions in the logic of programs using gitoxide to cause further vulnerabilities.\n\nThis vulnerability affects any user of gitoxide, including `gix-*` library crates, that reads or writes Git objects.",
          "date": "2025-04-03",
          "aliases": [
            "CVE-2025-31130",
            "GHSA-2frx-2596-x5r6"
          ],
          "related": [],
          "collection": "crates",
          "categories": [
            "crypto-failure"
          ],
          "keywords": [
            "hash-collision",
            "sha-1",
            "weak-hash"
          ],
          "cvss": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:C/C:N/I:H/A:N",
          "informational": null,
          "references": [
            "https://github.com/advisories/GHSA-2frx-2596-x5r6",
            "https://nvd.nist.gov/vuln/detail/CVE-2025-31130"
          ],
          "source": null,
          "url": "https://github.com/GitoxideLabs/gitoxide/security/advisories/GHSA-2frx-2596-x5r6",
          "withdrawn": null,
          "license": "CC0-1.0"
        },
        "versions": {
          "patched": [
            ">=0.41.0"
          ],
          "unaffected": []
        },
        "affected": {
          "arch": [],
          "os": [],
          "functions": {
            "gix_features::hash::Hasher::digest": [
              "<0.41.0"
            ],
            "gix_features::hash::Hasher::update": [
              "<0.41.0"
            ],
            "gix_features::hash::Write::flush": [
              "<0.41.0"
            ],
            "gix_features::hash::Write::new": [
              "<0.41.0"
            ],
            "gix_features::hash::Write::write": [
              "<0.41.0"
            ],
            "gix_features::hash::bytes": [
              "<0.41.0"
            ],
            "gix_features::hash::bytes_of_file": [
              "<0.41.0"
            ],
            "gix_features::hash::bytes_with_hasher": [
              "<0.41.0"
            ],
            "gix_features::hash::hasher": [
              "<0.41.0"
            ]
          }
        },
        "package": {
          "name": "gix-features",
          "version": "0.38.2",
          "source": "registry+https://github.com/rust-lang/crates.io-index",
          "checksum": "ac7045ac9fe5f9c727f38799d002a7ed3583cd777e3322a7c4b43e3cf437dc69",
          "dependencies": [
            {
              "name": "bytes",
              "version": "1.10.1",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "crc32fast",
              "version": "1.4.2",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "crossbeam-channel",
              "version": "0.5.15",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "flate2",
              "version": "1.1.1",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-hash",
              "version": "0.14.2",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-trace",
              "version": "0.1.12",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-utils",
              "version": "0.1.14",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "jwalk",
              "version": "0.8.1",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "libc",
              "version": "0.2.172",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "once_cell",
              "version": "1.21.3",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "parking_lot",
              "version": "0.12.4",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "prodash",
              "version": "28.0.0",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "sha1_smol",
              "version": "1.0.1",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "thiserror",
              "version": "1.0.69",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "walkdir",
              "version": "2.5.0",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            }
          ],
          "replace": null
        }
      },
      {
        "advisory": {
          "id": "RUSTSEC-2025-0001",
          "package": "gix-worktree-state",
          "title": "gix-worktree-state nonexclusive checkout sets executable files world-writable",
          "description": "### Summary\n\n`gix-worktree-state` specifies 0777 permissions when checking out executable files, intending that the umask will restrict them appropriately. But one of the strategies it uses to set permissions is not subject to the umask. This causes files in a repository to be world-writable in some situations.\n\n### Details\n\nGit repositories track executable bits for regular files. In tree objects and the index, regular file modes are stored as 0644 if not executable, or 0755 if executable. But this is independent of how the permissions are set in the filesystem (where supported).\n\n[`gix_worktree_state::checkout`](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/function.rs#L8-L35) has two strategies for checking out a file and marking it executable on a Unix-like operating system, one of which is vulnerable:\n\n- If the file is created by assuming it does not already exist, correct permissions are applied, because permissions specified when opening a file are subject to the umask.\n- If the file is considered possibly already to exist—even in a clean checkout if the application does not specify the option to treat the destination directory as empty—then permissions conferring unrestricted access to any user account on the system are wrongly applied, because permissions specified when calling chmod on an existing file are not subject to the umask. \n\nSpecifically, [`checkout::entry::checkout`](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/entry.rs#L56-L191) chooses the strategy for each file. The same strategy is usually chosen for each executable file, if no [process](https://github.com/git/git/blob/a60673e9252b08d4eca90543b3729f4798b9aafd/Documentation/RelNotes/2.11.0.txt#L149-L154) (i.e. [long running](https://github.com/GitoxideLabs/gitoxide/discussions/996)) smudge filter is in use. The strategy depends on the [`checkout::Options::destination_is_initially_empty`](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/mod.rs#L50-L53) value, which is passed along to [`checkout::entry::open_file`](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/entry.rs#L253-L277), whose return value includes a flag indicating whether permissions still need to be set:\n\n- With `destination_is_initially_empty: true`, executable permissions are specified when opening the file, via [`OpenOptionsEx::mode`](https://doc.rust-lang.org/std/os/unix/fs/trait.OpenOptionsExt.html#tymethod.mode), by its effect on the behavior of [`OpenOptions::open`](https://doc.rust-lang.org/std/fs/struct.OpenOptions.html#method.open). A mode of 0777 is safe here, for the same reason the default mode of 0666 is safe. When creating a file, the applied mode is the specified mode with any bits unset from it that are set in the umask.\n\n   <https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/entry.rs#L265-L268>\n\n  The `set_executable_after_creation` flag in the `open_file` return value is then `false`.\n\n- With `destination_is_initially_empty: false`, executable permissions are set in a separate step, via [`PermissionsExt::set_mode`](https://doc.rust-lang.org/beta/std/os/unix/fs/trait.PermissionsExt.html#tymethod.set_mode) and [`set_permissions`](https://doc.rust-lang.org/beta/std/fs/fn.set_permissions.html). A mode of 0777 is not safe here, because the umask is not applied. The vulnerable code appears in [`checkout::entry::finalize_entry`](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/entry.rs#L279-L299), which receives the `set_executable_after_creation` flag originally from `open_file`:\n\n  <https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/entry.rs#L288-L293>\n\n  The file has unrestricted permissions.\n\n`finalize_entry` is [likewise called](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/chunk.rs#L229-L236) from [`checkout::chunk::process_delayed_filter_results`](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/chunk.rs#L157-L259).\n\n### PoC\n\n1. On a Unix-like system such as GNU/Linux or macOS, create a new project and define its dependencies. While the vulnerability is in `gix-worktree-state`, this example will use vulnerable code through the `gix` crate, which exposes it. Run:\n\n   ```sh\n   cargo new checkout-index\n   cd checkout-index\n   cargo add gix gix-object\n   ```\n\n2. In the `checkout-index` directory, edit `src/main.rs` so that its entire contents are:\n\n   ```rust\n   fn main() -> Result<(), Box<dyn std::error::Error>> {\n       let repo = gix::discover(\"has-executable\")?;\n       let mut index = repo.open_index()?;\n       gix::worktree::state::checkout(\n           &mut index,\n           repo.work_dir().ok_or(\"need non-bare repo\")?,\n           gix_object::find::Never, // Can also use: repo.objects.clone()\n           &gix::progress::Discard,\n           &gix::progress::Discard,\n           &Default::default(),\n           Default::default(),\n       )?;\n       Ok(())\n   }\n   ```\n\n3. Create the test repository that the vulnerable program will operate on. Still in the `checkout-index` directory, run:\n\n   ```sh\n   git init has-executable\n   touch has-executable/a has-executable/b\n   chmod +x has-executable/b\n   git -C has-executable add .\n   ```\n\n   It is not necessary to commit the changes, only to stage them, since the test program will check  out the index.\n\n4. *Optionally*, run `rm has-executable/[ab]` to remove the staged files from disk.\n\n5. Run the program by issuing `cargo run`. The program uses `gix-worktree-state` to check out the index. It should terminate successfully and not issue any errors.\n\n6. Run `ls -l has-executable` to inspect the permissions of the checked out files. Observe that owner, group, and other all have read, write, and execute permissions on `b`.\n\n   ```text\n   -rw-r--r-- 1 ek ek 0 Jan  9 03:38 a\n   -rwxrwxrwx 1 ek ek 0 Jan  9 03:38 b\n   ```\n\n   With affected versions of `gix-worktree-state`, the output shows `-rwxrwxrwx` for `b`, whether the files were removed in step 4 or not.\n\n7. It was not necessary to set `destination_is_initially_empty` to `false` explicitly to trigger the bug, because that is its default value. If desired, modify the program to pass `true` and rerun the experiment to verify that `b` is no longer created with excessive permissions. The modified program would change the last `checkout` argument from `Default::default(),` to:\n\n   ```rust\n           gix::worktree::state::checkout::Options {\n               destination_is_initially_empty: true,\n               ..Default::default()\n           },\n   ```\n\n### Impact\n\nSetting unlimited file permissions is a problem on systems where a user account exists on the system that should not have the ability to access and modify the files. That applies to multi-user systems, or when an account is used to run software with reduced abilities. (Some programs may also treat broad write permissions to mean less validation is required.)\n\nThis bug affects Unix-like systems but not Windows. The `gix clone` command is not believed to be affected, due to [`checkout_exclusive`](https://github.com/GitoxideLabs/gitoxide/blob/af704f57bb9480c47cdd393465264d586f1d4562/gitoxide-core/src/index/checkout.rs#L14-L172)'s [use](https://github.com/GitoxideLabs/gitoxide/blob/af704f57bb9480c47cdd393465264d586f1d4562/gitoxide-core/src/index/checkout.rs#L61) of `destination_is_initially_empty: true`. Specialized uses in which repositories are known never to have any files marked executable are unaffected. Repositories that no untrusted users can access, due to not having the ability to traverse the directories to them or due to sufficiently restrictive ACLs, are likewise unaffected.\n\nThe default value of `destination_is_initially_empty` is `false`, so some applications may be affected even if they don't attempt checkouts in nonempty directories. The 0777 permissions are applied to files that are created earlier in the same checkout, as well as those that already existed, regardless of their prior permissions. On preexisting files, 0777 is set *even if [`overwrite_existing`](https://github.com/GitoxideLabs/gitoxide/blob/8d84818240d44e1f5fe78a231b5d9bffd0283918/gix-worktree-state/src/checkout/mod.rs#L54-L58) is `false`*, as that prevents the checkout from changing file contents but not permissions.\n\nFiles not tracked/staged as executable are not checked out with insecure permissions. Such a file that previously existed keeps its old permissions. However, this may include executable permissions that no longer match repository metadata, as well as undesired write permissions acquired from a previous vulnerable checkout. `set_mode(0o777)` clears other bits, so the bug is not exacerbated by the presence of setuid/setgid bits. In some applications, the vulnerable strategy may be used only for files rewritten by a [long running](https://git-scm.com/docs/gitattributes/2.40.0#_long_running_filter_process) smudge filter or only in the presence of [delays](https://git-scm.com/docs/gitattributes/2.40.0#_delay).",
          "date": "2025-01-18",
          "aliases": [
            "CVE-2025-22620",
            "GHSA-fqmf-w4xh-33rh"
          ],
          "related": [],
          "collection": "crates",
          "categories": [],
          "keywords": [
            "permissions"
          ],
          "cvss": "CVSS:3.1/AV:L/AC:L/PR:L/UI:R/S:U/C:N/I:H/A:N",
          "informational": null,
          "references": [
            "https://github.com/advisories/GHSA-fqmf-w4xh-33rh",
            "https://nvd.nist.gov/vuln/detail/CVE-2025-22620"
          ],
          "source": null,
          "url": "https://github.com/GitoxideLabs/gitoxide/security/advisories/GHSA-fqmf-w4xh-33rh",
          "withdrawn": null,
          "license": "CC0-1.0"
        },
        "versions": {
          "patched": [
            ">=0.17.0"
          ],
          "unaffected": []
        },
        "affected": {
          "arch": [],
          "os": [],
          "functions": {
            "gix_worktree_state::checkout": [
              "<0.17.0"
            ]
          }
        },
        "package": {
          "name": "gix-worktree-state",
          "version": "0.11.1",
          "source": "registry+https://github.com/rust-lang/crates.io-index",
          "checksum": "39ed6205b5f51067a485b11843babcf3304c0799e265a06eb0dde7f69cd85cd8",
          "dependencies": [
            {
              "name": "bstr",
              "version": "1.12.0",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-features",
              "version": "0.38.2",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-filter",
              "version": "0.11.3",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-fs",
              "version": "0.11.3",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-glob",
              "version": "0.16.5",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-hash",
              "version": "0.14.2",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-index",
              "version": "0.33.1",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-object",
              "version": "0.42.3",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-path",
              "version": "0.10.18",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "gix-worktree",
              "version": "0.34.1",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "io-close",
              "version": "0.3.7",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            },
            {
              "name": "thiserror",
              "version": "1.0.69",
              "source": "registry+https://github.com/rust-lang/crates.io-index"
            }
          ],
          "replace": null
        }
      }
    ]
  },
  "warnings": {}
}



================================================
FILE: CHANGELOG.md
================================================
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/), and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

## [0.9.8](https://github.com/syncable-dev/syncable-cli/compare/v0.9.7...v0.9.8) - 2025-06-12

### Other

- *(deps)* bump env_logger from 0.10.2 to 0.11.8

## [0.9.7](https://github.com/syncable-dev/syncable-cli/compare/v0.9.6...v0.9.7) - 2025-06-11

### Fixed

- improved security cmd, for further false postitive in terms of:

## [0.9.6](https://github.com/syncable-dev/syncable-cli/compare/v0.9.5...v0.9.6) - 2025-06-11

### Other

- *(deps)* bump rustsec from 0.29.3 to 0.30.2

## [0.9.5](https://github.com/syncable-dev/syncable-cli/compare/v0.9.4...v0.9.5) - 2025-06-10

### Other

- update Cargo.lock dependencies

## [0.9.4](https://github.com/syncable-dev/syncable-cli/compare/v0.9.3...v0.9.4) - 2025-06-10

### Added

- feat added windows support

## [0.9.3](https://github.com/syncable-dev/syncable-cli/compare/v0.9.2...v0.9.3) - 2025-06-10

### Other

- *(deps)* bump thiserror from 1.0.69 to 2.0.12

## [0.9.2](https://github.com/syncable-dev/syncable-cli/compare/v0.9.1...v0.9.2) - 2025-06-10

### Other

- update Cargo.lock dependencies

## [0.9.1](https://github.com/syncable-dev/syncable-cli/compare/v0.9.0...v0.9.1) - 2025-06-10

### Added

- readme updates

## [0.9.0](https://github.com/syncable-dev/syncable-cli/compare/v0.8.1...v0.9.0) - 2025-06-09

### Added

- huge improvements towards security scanning and performance
- feat added python security scanning catching generat exposure secrets similar to javascript version

### Other

- Merge branch 'main' of github.com:syncable-dev/syncable-cli into develop
- README.md duplicate phrases updated

## [0.8.1](https://github.com/syncable-dev/syncable-cli/compare/v0.8.0...v0.8.1) - 2025-06-09

### Other

- Develop ([#61](https://github.com/syncable-dev/syncable-cli/pull/61))

## [0.8.0](https://github.com/syncable-dev/syncable-cli/compare/v0.7.0...v0.8.0) - 2025-06-08

### Added

- feat added python security scanning catching generat exposure secrets similar to javascript version

## [0.7.0](https://github.com/syncable-dev/syncable-cli/compare/v0.6.0...v0.7.0) - 2025-06-08

### Added

- huge improvements towards security and secret variable detection.

### Other

- updated cli-display-modes.md file for better visualization

## [0.6.0](https://github.com/syncable-dev/syncable-cli/compare/v0.5.4...v0.6.0) - 2025-06-07

### Added

- improved readme

### Fixed

- release-plz structure to avoid quick bump

### Other

- fix releaze-pls, proper section structure
- wrong release-plz setting
- small updates of unnused variables - cleanup
- updated release cycles and rules

## [0.5.4](https://github.com/syncable-dev/syncable-cli/compare/v0.5.3...v0.5.4) - 2025-06-07

### Other

- Update README.md
- Update README.md

## [0.5.3](https://github.com/syncable-dev/syncable-cli/compare/v0.5.2...v0.5.3) - 2025-06-07

### Other

- Develop ([#47](https://github.com/syncable-dev/syncable-cli/pull/47))
- Update README.md

## [0.5.2](https://github.com/syncable-dev/syncable-cli/compare/v0.5.1...v0.5.2) - 2025-06-07

### Other

- Develop ([#44](https://github.com/syncable-dev/syncable-cli/pull/44))

## [0.5.1](https://github.com/syncable-dev/syncable-cli/compare/v0.5.0...v0.5.1) - 2025-06-07

### Added

- improved README.md

## [0.5.0](https://github.com/syncable-dev/syncable-cli/compare/v0.4.2...v0.5.0) - 2025-06-06

### Other

- HOTFIX - hoping auto update becomes available

## [0.4.2](https://github.com/syncable-dev/syncable-cli/compare/v0.4.1...v0.4.2) - 2025-06-06

### Other

- Feature/improve framework and language tool detection ([#37](https://github.com/syncable-dev/syncable-cli/pull/37))

## [0.4.1](https://github.com/syncable-dev/syncable-cli/compare/v0.4.0...v0.4.1) - 2025-06-06

### Other

- Develop ([#33](https://github.com/syncable-dev/syncable-cli/pull/33))

## [0.4.0](https://github.com/syncable-dev/syncable-cli/compare/v0.3.0...v0.4.0) - 2025-06-06

### Other

- Feature/condense overview with new representation ([#29](https://github.com/syncable-dev/syncable-cli/pull/29))

## [0.3.0](https://github.com/syncable-dev/syncable-cli/compare/v0.2.1...v0.3.0) - 2025-06-06

### Added

- Added tool install verifier with cli calls ([#14](https://github.com/syncable-dev/syncable-cli/pull/14))

### Other

- Feature/extendsive docker compose and docker scan ([#25](https://github.com/syncable-dev/syncable-cli/pull/25))
- Feature/add automatic cli update ([#22](https://github.com/syncable-dev/syncable-cli/pull/22))
- Feature/update dependabot ([#11](https://github.com/syncable-dev/syncable-cli/pull/11))

## [0.2.1](https://github.com/syncable-dev/syncable-cli/compare/v0.2.0...v0.2.1) - 2025-06-06

### Other

- Feature/add automatic cli update ([#22](https://github.com/syncable-dev/syncable-cli/pull/22))

## [0.2.0](https://github.com/syncable-dev/syncable-cli/compare/v0.1.5...v0.2.0) - 2025-06-06

### Added

- Added tool install verifier with cli calls ([#14](https://github.com/syncable-dev/syncable-cli/pull/14))

## [0.1.5](https://github.com/syncable-dev/syncable-cli/compare/v0.1.4...v0.1.5) - 2025-06-06

### Added

- cargo lock update

### Other

- Feature/update dependabot ([#11](https://github.com/syncable-dev/syncable-cli/pull/11))
- Update README.md
- Update README.md
- *(deps)* bump reqwest from 0.11.27 to 0.12.19
- *(deps)* bump dirs from 5.0.1 to 6.0.0
- Feature/dependabot ([#3](https://github.com/syncable-dev/syncable-cli/pull/3))

## [0.1.4](https://github.com/syncable-dev/syncable-cli/compare/v0.1.3...v0.1.4) - 2025-06-05

### Added

- added cargo isntall command for readme
- Add new features and improvements here.

## [0.1.3] - 2024-06-05
### Added
- Initial release of `syncable-cli`.
- Analyze code repositories to detect languages, frameworks, and dependencies.
- Generate Infrastructure as Code (IaC) configurations: Dockerfile, Docker Compose, and Terraform.
- Modular architecture for extensibility and maintainability.
- CLI interface with `analyze` and `generate` commands.
- Basic security and performance analysis. 


================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Syncable CLI

Thank you for your interest in contributing to the Syncable Infrastructure-as-Code CLI! This document provides guidelines and instructions for contributing.

## 🤝 Code of Conduct

We are committed to providing a welcoming and inclusive environment. Please be respectful and constructive in all interactions.

## 🚀 Getting Started

### Prerequisites

- Rust 1.70.0 or later
- Git
- A code editor (we recommend VS Code with rust-analyzer)

### Setting Up Development Environment

1. Fork the repository on GitHub
2. Clone your fork:
   ```bash
   git clone https://github.com/YOUR-USERNAME/syncable-cli.git
   cd syncable-cli
   ```
3. Add the upstream repository:
   ```bash
   git remote add upstream https://github.com/syncable/syncable-cli.git
   ```
4. Install development tools:
   ```bash
   rustup component add rustfmt clippy
   ```

## 📝 Development Workflow

### 1. Create a Feature Branch

```bash
git checkout -b feature/your-feature-name
```

### 2. Make Your Changes

- Follow the existing code style and patterns
- Add tests for new functionality
- Update documentation as needed

### 3. Run Tests

```bash
# Run all tests
cargo test

# Run specific test
cargo test test_name

# Run tests with output
cargo test -- --nocapture
```

### 4. Check Code Quality

```bash
# Format code
cargo fmt

# Run linter
cargo clippy -- -D warnings

# Check for security issues
cargo audit
```

### 5. Commit Your Changes

We follow conventional commit messages:

- `feat:` New feature
- `fix:` Bug fix
- `docs:` Documentation changes
- `test:` Test additions or modifications
- `refactor:` Code refactoring
- `perf:` Performance improvements
- `chore:` Maintenance tasks

Example:
```bash
git commit -m "feat: add support for Ruby language detection"
```

## 🔍 Areas for Contribution

### High Priority

1. **Language Support**: Add detection for new languages (Ruby, PHP, C#)
2. **Framework Detection**: Expand framework detection patterns
3. **Security Scanning**: Integrate additional vulnerability databases
4. **Documentation**: Improve user guides and API documentation
5. **Test Coverage**: Add more unit and integration tests

### Feature Ideas

- Cloud provider integrations (AWS, GCP, Azure)
- Kubernetes manifest generation
- Interactive configuration wizard
- Performance optimizations
- New IaC output formats

## 📋 Pull Request Process

1. **Update Your Branch**:
   ```bash
   git fetch upstream
   git rebase upstream/main
   ```

2. **Push to Your Fork**:
   ```bash
   git push origin feature/your-feature-name
   ```

3. **Create Pull Request**:
   - Go to the original repository on GitHub
   - Click "New Pull Request"
   - Select your branch
   - Fill out the PR template

4. **PR Requirements**:
   - Clear description of changes
   - Tests pass (`cargo test`)
   - Code is formatted (`cargo fmt`)
   - No clippy warnings (`cargo clippy`)
   - Documentation updated if needed

## 🧪 Testing Guidelines

### Unit Tests

Place unit tests in the same file as the code:

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_function_name() {
        // Test implementation
    }
}
```

### Integration Tests

Add integration tests in `tests/integration/`:

```rust
use assert_cmd::Command;

#[test]
fn test_cli_analyze() {
    let mut cmd = Command::cargo_bin("sync-ctl").unwrap();
    cmd.arg("analyze")
        .arg("tests/fixtures/sample_project")
        .assert()
        .success();
}
```

### Test Fixtures

Add test projects in `tests/fixtures/` with appropriate structure for testing.

## 📁 Project Structure

Key directories:

- `src/analyzer/`: Language and framework detection
- `src/generator/`: IaC file generation (Phase 2)
- `src/common/`: Shared utilities
- `templates/`: IaC templates
- `tests/`: Test suite
- `docs/`: Documentation

## 🐛 Reporting Issues

### Bug Reports

Include:
- Rust version (`rustc --version`)
- OS and version
- Steps to reproduce
- Expected vs actual behavior
- Error messages

### Feature Requests

Include:
- Use case description
- Expected behavior
- Example scenarios
- Alternative solutions considered

## 💡 Tips for Contributors

### Understanding the Codebase

1. Start with `src/main.rs` to understand the CLI structure
2. Review `src/analyzer/mod.rs` for the analysis pipeline
3. Check existing tests for usage examples

### Common Patterns

- Use `Result<T, E>` for error handling
- Implement traits for extensibility
- Use `log` crate for debugging
- Follow the builder pattern for complex structs

### Performance Considerations

- Use `rayon` for parallel processing
- Cache expensive computations
- Avoid unnecessary allocations
- Profile before optimizing

## 📞 Getting Help

- **Discord**: Join our community server
- **GitHub Discussions**: Ask questions
- **Issues**: Report bugs or request features

## 🎉 Recognition

Contributors will be:
- Listed in CONTRIBUTORS.md
- Mentioned in release notes
- Given credit in relevant documentation

## 📄 License

By contributing, you agree that your contributions will be licensed under the MIT License.

---

Thank you for contributing to Syncable CLI! 🚀 


================================================
FILE: install.ps1
================================================
# PowerShell Installation Script for Syncable CLI on Windows
# Usage: powershell -ExecutionPolicy Bypass -File install.ps1

param(
    [string]$Version = "latest",
    [string]$InstallDir = "$env:USERPROFILE\.local\bin",
    [switch]$Force = $false,
    [switch]$Help = $false
)

# Color functions for better output
function Write-Success {
    param([string]$Message)
    Write-Host "✅ $Message" -ForegroundColor Green
}

function Write-Info {
    param([string]$Message)
    Write-Host "ℹ️  $Message" -ForegroundColor Blue
}

function Write-Warning {
    param([string]$Message)
    Write-Host "⚠️  $Message" -ForegroundColor Yellow
}

function Write-Error {
    param([string]$Message)
    Write-Host "❌ $Message" -ForegroundColor Red
}

function Write-Step {
    param([string]$Message)
    Write-Host "🔧 $Message" -ForegroundColor Cyan
}

# Help function
function Show-Help {
    Write-Host @"
Syncable CLI Installer for Windows

Usage: powershell -ExecutionPolicy Bypass -File install.ps1 [OPTIONS]

Options:
  -Version <version>     Install specific version (default: latest)
  -InstallDir <path>     Installation directory (default: %USERPROFILE%\.local\bin)
  -Force                 Force installation even if already installed
  -Help                  Show this help message

Examples:
  .\install.ps1                          # Install latest version
  .\install.ps1 -Version "0.9.0"         # Install specific version
  .\install.ps1 -Force                   # Force reinstall
  .\install.ps1 -InstallDir "C:\tools"   # Custom installation directory

"@
}

# Check if help is requested
if ($Help) {
    Show-Help
    exit 0
}

Write-Host @"
🚀 Syncable CLI Installer for Windows
====================================
"@ -ForegroundColor Magenta

# Check if running as administrator (optional, for system-wide installs)
$isAdmin = ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")
if ($isAdmin) {
    Write-Info "Running as Administrator - can install system-wide"
} else {
    Write-Info "Running as regular user - installing to user directory"
}

# Check if cargo is available
Write-Step "Checking for Rust/Cargo installation..."
try {
    $cargoVersion = cargo --version 2>$null
    if ($LASTEXITCODE -eq 0) {
        Write-Success "Found Cargo: $cargoVersion"
        $hasRust = $true
    } else {
        $hasRust = $false
    }
} catch {
    $hasRust = $false
}

if (-not $hasRust) {
    Write-Warning "Rust/Cargo not found. Installing via cargo is not available."
    Write-Info "To install Rust, visit: https://rustup.rs/"
    Write-Info "Or download pre-built binaries from: https://github.com/syncable-dev/syncable-cli/releases"
    
    # Offer to open browser
    $response = Read-Host "Would you like to open the Rust installation page? (y/N)"
    if ($response -eq "y" -or $response -eq "Y") {
        Start-Process "https://rustup.rs/"
    }
    exit 1
}

# Check if sync-ctl is already installed
Write-Step "Checking for existing installation..."
try {
    $existingVersion = sync-ctl --version 2>$null
    if ($LASTEXITCODE -eq 0) {
        Write-Info "Found existing installation: $existingVersion"
        if (-not $Force) {
            $response = Read-Host "sync-ctl is already installed. Reinstall? (y/N)"
            if ($response -ne "y" -and $response -ne "Y") {
                Write-Info "Installation cancelled."
                exit 0
            }
        }
    }
} catch {
    Write-Info "No existing installation found."
}

# Install via cargo
Write-Step "Installing Syncable CLI via Cargo..."
Write-Info "This may take a few minutes..."

try {
    if ($Version -eq "latest") {
        Write-Info "Installing latest version from crates.io..."
        $installResult = cargo install syncable-cli 2>&1
    } else {
        Write-Info "Installing version $Version from crates.io..."
        $installResult = cargo install syncable-cli --version $Version 2>&1
    }
    
    if ($LASTEXITCODE -eq 0) {
        Write-Success "Syncable CLI installed successfully!"
    } else {
        Write-Error "Installation failed. Cargo output:"
        Write-Host $installResult -ForegroundColor Red
        exit 1
    }
} catch {
    Write-Error "Installation failed: $_"
    exit 1
}

# Verify installation
Write-Step "Verifying installation..."
try {
    $version = sync-ctl --version 2>$null
    if ($LASTEXITCODE -eq 0) {
        Write-Success "Installation verified: $version"
    } else {
        Write-Warning "Installation may have issues. sync-ctl command not found."
    }
} catch {
    Write-Warning "Could not verify installation."
}

# Check PATH
Write-Step "Checking PATH configuration..."
$cargoPath = "$env:USERPROFILE\.cargo\bin"
$currentPath = $env:PATH
if ($currentPath -like "*$cargoPath*") {
    Write-Success "Cargo bin directory is already in PATH"
} else {
    Write-Warning "Cargo bin directory ($cargoPath) is not in your PATH"
    Write-Info "To add it permanently:"
    Write-Info "1. Open System Properties > Advanced > Environment Variables"
    Write-Info "2. Add '$cargoPath' to your PATH variable"
    Write-Info "3. Restart your terminal/PowerShell session"
    Write-Info ""
    Write-Info "Or run this command in an elevated PowerShell:"
    Write-Info "[Environment]::SetEnvironmentVariable('PATH', `$env:PATH + ';$cargoPath', 'User')"
    
    # Offer to add to PATH automatically
    $response = Read-Host "Would you like to add it to PATH now? (y/N)"
    if ($response -eq "y" -or $response -eq "Y") {
        try {
            [Environment]::SetEnvironmentVariable('PATH', $env:PATH + ";$cargoPath", 'User')
            $env:PATH += ";$cargoPath"  # Update current session
            Write-Success "Added to PATH. Restart PowerShell to ensure it takes effect."
        } catch {
            Write-Error "Failed to add to PATH: $_"
            Write-Info "Please add manually as described above."
        }
    }
}

# Install vulnerability scanning tools
Write-Step "Setting up vulnerability scanning tools..."
Write-Info "Installing common security tools for better analysis..."

# Install tools that work well on Windows
$tools = @(
    @{Name="cargo-audit"; Command="cargo install cargo-audit"; Check="cargo audit --version"},
    @{Name="pip-audit"; Command="pip install --user pip-audit"; Check="pip-audit --version"}
)

foreach ($tool in $tools) {
    Write-Info "Installing $($tool.Name)..."
    try {
        # Check if already installed
        $checkResult = Invoke-Expression $tool.Check 2>$null
        if ($LASTEXITCODE -eq 0) {
            Write-Success "$($tool.Name) is already installed"
            continue
        }
        
        # Install the tool
        $installResult = Invoke-Expression $tool.Command 2>&1
        if ($LASTEXITCODE -eq 0) {
            Write-Success "$($tool.Name) installed successfully"
        } else {
            Write-Warning "Failed to install $($tool.Name): $installResult"
        }
    } catch {
        Write-Warning "Error installing $($tool.Name): $_"
    }
}

# Additional Windows-specific tools
Write-Info "For additional security tools on Windows, consider:"
Write-Info "  • Scoop: scoop install grype"
Write-Info "  • Chocolatey: choco install grype"
Write-Info "  • Manual downloads from GitHub releases"

# Final instructions
Write-Host @"

🎉 Installation Complete!
========================
"@ -ForegroundColor Green

Write-Success "Syncable CLI is now installed and ready to use!"
Write-Info ""
Write-Info "Quick Start:"
Write-Info "  sync-ctl analyze .              # Analyze current directory"
Write-Info "  sync-ctl generate --all .       # Generate all IaC files"
Write-Info "  sync-ctl security .             # Run security analysis"
Write-Info "  sync-ctl tools status           # Check security tools"
Write-Info ""
Write-Info "For help: sync-ctl --help"
Write-Info "Documentation: https://github.com/syncable-dev/syncable-cli"
Write-Info ""
Write-Warning "Remember to restart your PowerShell session if PATH was modified!" 


================================================
FILE: install.sh
================================================
#!/bin/bash
# Syncable CLI Installation Script

set -e

echo "🚀 Installing Syncable IaC CLI..."
echo ""

# Color codes for better output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
print_success() {
    echo -e "${GREEN}✅ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}⚠️  $1${NC}"
}

print_error() {
    echo -e "${RED}❌ $1${NC}"
}

print_info() {
    echo -e "${BLUE}ℹ️  $1${NC}"
}

print_step() {
    echo -e "${BLUE}🔧 $1${NC}"
}

# Check if Rust is installed
if ! command -v cargo &> /dev/null; then
    print_error "Rust is not installed. Please install Rust first:"
    echo "   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"
    exit 1
fi

# Check Rust version
RUST_VERSION=$(rustc --version | cut -d' ' -f2)
MIN_VERSION="1.70.0"

if [ "$(printf '%s\n' "$MIN_VERSION" "$RUST_VERSION" | sort -V | head -n1)" != "$MIN_VERSION" ]; then
    print_error "Rust version $RUST_VERSION is too old. Please update to at least $MIN_VERSION"
    echo "   rustup update"
    exit 1
fi

print_success "Rust $RUST_VERSION detected"
echo ""

# Clone repository if not already in it
if [ ! -f "Cargo.toml" ] || [ ! -d "src" ]; then
    print_step "Cloning Syncable CLI repository..."
    git clone https://github.com/syncable-dev/syncable-cli.git
    cd syncable-cli
fi

print_step "Building Syncable CLI (this may take a few minutes)..."
cargo build --release

echo ""
print_step "Installing Syncable CLI..."
cargo install --path .

echo ""
print_success "Syncable CLI installed successfully!"

# Now install vulnerability scanning tools
echo ""
echo "🛡️  Setting up vulnerability scanning tools..."
echo "================================================"

# Function to check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Function to install tools based on platform
install_vulnerability_tools() {
    print_step "Checking and installing vulnerability scanning tools..."
    
    # 1. Rust - cargo-audit
    if command_exists cargo; then
        if ! cargo audit --version >/dev/null 2>&1; then
            print_step "Installing cargo-audit for Rust vulnerability scanning..."
            if cargo install cargo-audit; then
                print_success "cargo-audit installed"
            else
                print_warning "Failed to install cargo-audit"
            fi
        else
            print_success "cargo-audit already installed"
        fi
    fi
    
    # 2. Node.js/JavaScript - npm (comes with Node.js)
    if command_exists npm; then
        print_success "npm detected (Node.js vulnerability scanning available)"
    else
        print_warning "npm not found. Install Node.js for JavaScript/TypeScript vulnerability scanning:"
        echo "   • Download from: https://nodejs.org/"
        echo "   • Or use package manager:"
        echo "     - macOS: brew install node"
        echo "     - Ubuntu/Debian: sudo apt install nodejs npm"
        echo "     - CentOS/RHEL: sudo yum install nodejs npm"
    fi
    
    # 3. Python - pip-audit
    if command_exists python3 || command_exists python; then
        if ! command_exists pip-audit; then
            print_step "Installing pip-audit for Python vulnerability scanning..."
            
            # Try different installation methods
            if command_exists pipx; then
                if pipx install pip-audit; then
                    print_success "pip-audit installed via pipx"
                fi
            elif command_exists pip3; then
                if pip3 install --user pip-audit; then
                    print_success "pip-audit installed via pip3"
                fi
            elif command_exists pip; then
                if pip install --user pip-audit; then
                    print_success "pip-audit installed via pip"
                fi
            else
                print_warning "Could not install pip-audit automatically. Install manually:"
                echo "   • pipx install pip-audit (recommended)"
                echo "   • pip3 install --user pip-audit"
            fi
        else
            print_success "pip-audit already installed"
        fi
    else
        print_warning "Python not found. Install Python for Python vulnerability scanning:"
        echo "   • Download from: https://python.org/"
        echo "   • Or use package manager:"
        echo "     - macOS: brew install python"
        echo "     - Ubuntu/Debian: sudo apt install python3 python3-pip"
    fi
    
    # 4. Go - govulncheck
    if command_exists go; then
        if ! command_exists govulncheck && ! test -f "$HOME/go/bin/govulncheck"; then
            print_step "Installing govulncheck for Go vulnerability scanning..."
            if go install golang.org/x/vuln/cmd/govulncheck@latest; then
                print_success "govulncheck installed"
                print_info "Make sure ~/go/bin is in your PATH"
            else
                print_warning "Failed to install govulncheck"
            fi
        else
            print_success "govulncheck already installed"
        fi
    else
        print_warning "Go not found. Install Go for Go vulnerability scanning:"
        echo "   • Download from: https://golang.org/"
        echo "   • Or use package manager:"
        echo "     - macOS: brew install go"
        echo "     - Ubuntu/Debian: sudo apt install golang-go"
    fi
    
    # 5. Java/Kotlin - grype (universal vulnerability scanner)
    if ! command_exists grype && ! test -f "$HOME/.local/bin/grype"; then
        print_step "Installing grype for universal vulnerability scanning (Java, containers, etc.)..."
        
        case "$(uname -s)" in
            Darwin)  # macOS
                if command_exists brew; then
                    if brew install anchore/grype/grype; then
                        print_success "grype installed via Homebrew"
                    else
                        install_grype_manually
                    fi
                else
                    install_grype_manually
                fi
                ;;
            Linux)
                install_grype_manually
                ;;
            *)
                print_warning "Platform not supported for automatic grype installation"
                print_info "Please install grype manually: https://github.com/anchore/grype"
                ;;
        esac
    else
        print_success "grype already installed"
    fi
}

# Function to manually install grype
install_grype_manually() {
    print_step "Installing grype manually..."
    
    # Create local bin directory
    mkdir -p "$HOME/.local/bin"
    
    # Detect platform
    case "$(uname -s)" in
        Darwin)
            case "$(uname -m)" in
                x86_64) PLATFORM="darwin_amd64" ;;
                arm64|aarch64) PLATFORM="darwin_arm64" ;;
                *) 
                    print_warning "Unsupported macOS architecture"
                    return 1
                    ;;
            esac
            ;;
        Linux)
            case "$(uname -m)" in
                x86_64) PLATFORM="linux_amd64" ;;
                aarch64|arm64) PLATFORM="linux_arm64" ;;
                *) 
                    print_warning "Unsupported Linux architecture"
                    return 1
                    ;;
            esac
            ;;
        *)
            print_warning "Unsupported operating system"
            return 1
            ;;
    esac
    
    # Download and install
    VERSION="0.92.2"
    URL="https://github.com/anchore/grype/releases/download/v${VERSION}/grype_${VERSION}_${PLATFORM}.tar.gz"
    
    if command_exists curl; then
        print_info "Downloading grype v${VERSION} for ${PLATFORM}..."
        if curl -L "$URL" | tar -xz -C "$HOME/.local/bin" grype; then
            chmod +x "$HOME/.local/bin/grype"
            print_success "grype installed to ~/.local/bin/grype"
            print_info "Make sure ~/.local/bin is in your PATH"
        else
            print_warning "Failed to download grype automatically"
            print_info "Please install manually: https://github.com/anchore/grype#installation"
        fi
    else
        print_warning "curl not found. Please install grype manually: https://github.com/anchore/grype#installation"
    fi
}

# Install vulnerability scanning tools
install_vulnerability_tools

echo ""
echo "🎯 Installation Complete!"
echo "========================"
print_success "Syncable CLI is ready to use!"

echo ""
echo "📚 Quick Start Guide:"
echo "  sync-ctl --help                    # Show all commands"
echo "  sync-ctl analyze .                 # Analyze current directory"
echo "  sync-ctl generate .                # Generate IaC files"
echo "  sync-ctl vuln-check .              # Check for vulnerabilities"
echo "  sync-ctl security-scan .           # Comprehensive security analysis"

echo ""
echo "🔧 Environment Setup:"

# Check if common directories are in PATH
PATH_ADDITIONS=""
if [ -d "$HOME/.local/bin" ] && [[ ":$PATH:" != *":$HOME/.local/bin:"* ]]; then
    PATH_ADDITIONS="$PATH_ADDITIONS$HOME/.local/bin:"
fi
if [ -d "$HOME/go/bin" ] && [[ ":$PATH:" != *":$HOME/go/bin:"* ]]; then
    PATH_ADDITIONS="$PATH_ADDITIONS$HOME/go/bin:"
fi

if [ -n "$PATH_ADDITIONS" ]; then
    print_warning "Some tools may not be in your PATH. Add these to your shell profile:"
    echo "  export PATH=\"${PATH_ADDITIONS%:}:\$PATH\""
    echo ""
    echo "For current session, run:"
    echo "  export PATH=\"${PATH_ADDITIONS%:}:\$PATH\""
fi

echo ""
print_info "For more information and examples, see:"
echo "  • README.md - General usage and examples"
echo "  • CONTRIBUTING.md - Development guide"
echo "  • https://github.com/syncable-dev/syncable-cli"

echo ""
print_success "Happy coding! 🚀" 


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Syncable Team

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE. 


================================================
FILE: ROADMAP.md
================================================
# Syncable IaC CLI - Development Roadmap

This roadmap outlines the development phases and features for the Syncable IaC CLI. The tool will leverage AI to intelligently generate Infrastructure as Code configurations based on project analysis.

## 🎯 Core Vision

Build an AI-powered CLI that analyzes codebases and generates production-ready Infrastructure as Code with intelligent optimizations, security best practices, and framework-specific configurations.

---

#### Analysis Engine ✅
- [x] **Project Structure Setup**
  - [x] Initialize Rust project with proper module organization
  - [x] Set up CI/CD pipeline (GitHub Actions)
  - [x] Configure testing framework and code quality tools
  - [x] Create comprehensive project documentation structure

- [x] **Language Detection Engine**
  - [x] Implement file extension mapping
  - [x] Add content-based language detection
  - [x] Support version detection for major languages
  - [x] Create confidence scoring system

- [x] **Framework Detection**
  - [x] Detect 70+ frameworks across 5 languages
  - [x] Pattern-based detection with confidence scoring
  - [x] Support for:
    - [x] Rust: 15 frameworks (Actix-web, Rocket, Axum, etc.)
    - [x] JavaScript/TypeScript: 25 frameworks (Express, Next.js, React, etc.)
    - [x] Python: 15 frameworks (Django, FastAPI, Flask, etc.)
    - [x] Go: 10 frameworks (Gin, Echo, Fiber, etc.)
    - [x] Java/Kotlin: 8 frameworks (Spring Boot, Micronaut, etc.)

### Dependency/Vulnerability Analysis & Context Extraction ✅
- [x] **Dependency Parser** ✅
  - [x] Parse package manifests (package.json, Cargo.toml, requirements.txt, go.mod, pom.xml)
  - [x] Extract version constraints and dependency trees
  - [x] Identify dev vs production dependencies
  - [x] Detect package managers and lock files
  - [x] License detection and summary

- [x] **Vulnerability Checking** ✅
  - [x] Integrate with vulnerability databases:
    - [x] Rust: rustsec (cargo-audit CLI integration)
    - [x] JavaScript: npm audit (CLI integration)
    - [x] Python: pip-audit (CLI integration)
    - [x] Go: govulncheck (CLI integration)
    - [x] Java: OWASP dependency check & grype integration
  - [x] Severity classification (Critical, High, Medium, Low)
  - [x] Vulnerability report generation
  - [x] CLI commands for vulnerability scanning
  - [x] Automatic vulnerability checking in dependency analysis

- [x] **Security Analysis** ✅
  - [x] Comprehensive security analyzer module
  - [x] Multi-layered security assessment:
    - [x] Configuration security analysis (secrets, insecure settings)
    - [ ] Code security patterns (language/framework-specific issues)
    - [ ] Infrastructure security analysis framework
    - [ ] Security policy recommendations
  - [x] Context-aware secret detection with false positive filtering
  - [x] Risk-based severity classification and scoring
  - [x] CLI command for security analysis with configurable options

- [x] **Project Context Analyzer** ✅
  - [x] Detect entry points and main files
  - [x] Identify exposed ports and services
  - [x] Extract environment variables
  - [x] Analyze build scripts and commands
  - [x] Determine project type (web app, API, CLI tool, library)

---

## 🤖 Phase 2: AI Integration & Smart Generation

### 🧠 AI Engine Setup
- [ ] **AI Provider Integration**
  - [ ] OpenAI GPT-4 integration for IaC generation
  - [ ] Anthropic Claude integration as fallback
  - [ ] Local LLM support (Ollama) for offline generation
  - [ ] AI model configuration and selection
- [ ] **Prompt Engineering System**
  - [ ] Template-based prompt generation
  - [ ] Context-aware prompt construction
  - [ ] Framework-specific prompt optimization
  - [ ] Security-focused prompt guidelines
- [ ] **AI Response Processing**
  - [ ] Generated code validation and sanitization
  - [ ] Multi-attempt generation with fallbacks
  - [ ] AI confidence scoring
  - [ ] Human-readable explanation generation

### 🐳 Smart Dockerfile Generation
- [ ] **AI-Powered Base Image Selection**
  - [ ] Language-specific base image recommendations
  - [ ] Security-hardened image preferences
  - [ ] Size-optimized image selection
  - [ ] Version compatibility analysis
- [ ] **Intelligent Multi-Stage Builds**
  - [ ] Build stage optimization based on language
  - [ ] Dependency caching strategies
  - [ ] Production image minimization
  - [ ] Security scanning integration
- [ ] **Context-Aware Optimizations**
  - [ ] Development vs production configurations
  - [ ] Performance optimization hints
  - [ ] Resource usage optimization
  - [ ] Health check generation

### 🐙 Smart Docker Compose Generation
- [ ] **Service Dependency Analysis**
  - [ ] Database dependency detection
  - [ ] Cache service requirements (Redis, Memcached)
  - [ ] Message queue needs (RabbitMQ, Kafka)
  - [ ] Service mesh considerations
- [ ] **Network Configuration**
  - [ ] Port mapping optimization
  - [ ] Internal network setup
  - [ ] Load balancer configuration
  - [ ] SSL/TLS termination
- [ ] **Volume and Storage**
  - [ ] Persistent data identification
  - [ ] Volume mount optimization
  - [ ] Backup strategy suggestions
  - [ ] Development volume mounts

### 🏗️ Smart Terraform Generation
- [ ] **Provider-Specific Generation**
  - [ ] AWS ECS/Fargate configurations
  - [ ] Google Cloud Run setups
  - [ ] Azure Container Instances
  - [ ] Kubernetes deployments
- [ ] **Infrastructure Best Practices**
  - [ ] Resource tagging strategies
  - [ ] Security group generation
  - [ ] IAM role optimization
  - [ ] Cost optimization recommendations
- [ ] **Monitoring and Observability**
  - [ ] CloudWatch/Prometheus integration
  - [ ] Log aggregation setup
  - [ ] Alerting configuration
  - [ ] Performance monitoring

---

## 🔧 Phase 3: Advanced Features & Intelligence

### 🎯 Context-Aware Generation
- [ ] **Framework-Specific Optimizations**
  - [ ] Next.js: Static generation vs SSR detection
  - [ ] React: Build optimization and routing
  - [ ] Express.js: Middleware and routing analysis
  - [ ] Spring Boot: Profile-based configurations
  - [ ] Actix Web: Async runtime optimization
- [ ] **Performance Profiling**
  - [ ] Resource requirement estimation
  - [ ] Scaling recommendations
  - [ ] Bottleneck identification
  - [ ] Load testing configuration generation
- [x] **Security Analysis** ✅
  - [x] Vulnerability assessment integration
  - [x] Multi-layered security analysis (secrets, code patterns, infrastructure)
  - [x] Context-aware secret detection with false positive filtering
  - [x] Risk-based severity classification and security scoring
  - [x] Security policy recommendations and compliance frameworks
  - [ ] Security header configuration
  - [ ] Network security policies

### 🔄 Continuous Improvement
- [ ] **Learning from Feedback**
  - [ ] User feedback collection system
  - [ ] Generation quality metrics
  - [ ] Success rate tracking
  - [ ] Performance benchmarking
- [ ] **Template Evolution**
  - [ ] Community template sharing
  - [ ] Best practice updates
  - [ ] Security patch integration
  - [ ] Performance optimization updates

---

## 🚀 Phase 4: Advanced Integrations & Ecosystem

### 🔗 CI/CD Integration
- [ ] **GitHub Actions Generation**
  - [ ] Build and test workflows
  - [ ] Deployment pipelines
  - [ ] Security scanning workflows
  - [ ] Multi-environment deployments
- [ ] **GitLab CI Integration**
  - [ ] Pipeline configuration
  - [ ] Docker registry integration
  - [ ] Auto-deployment setup
  - [ ] Environment-specific configs
- [ ] **Jenkins Pipeline Support**
  - [ ] Jenkinsfile generation
  - [ ] Pipeline-as-code
  - [ ] Multi-branch strategies
  - [ ] Artifact management

### ☁️ Cloud Platform Integration
- [ ] **AWS Integration**
  - [ ] ECS/Fargate deployment
  - [ ] Lambda function packaging
  - [ ] RDS database setup
  - [ ] S3 storage configuration
- [ ] **Google Cloud Integration**
  - [ ] Cloud Run deployment
  - [ ] GKE cluster setup
  - [ ] Cloud SQL integration
  - [ ] Cloud Storage configuration
- [ ] **Azure Integration**
  - [ ] Container Instances
  - [ ] Azure Kubernetes Service
  - [ ] Azure Database setup
  - [ ] Blob Storage configuration

### 📊 Monitoring & Observability
- [ ] **Metrics Generation**
  - [ ] Prometheus configuration
  - [ ] Grafana dashboard templates
  - [ ] Application metrics setup
  - [ ] Infrastructure metrics
- [ ] **Logging Configuration**
  - [ ] Structured logging setup
  - [ ] Log aggregation (ELK, Fluentd)
  - [ ] Log routing and filtering
  - [ ] Log retention policies
- [ ] **Distributed Tracing**
  - [ ] Jaeger configuration
  - [ ] OpenTelemetry setup
  - [ ] Trace sampling strategies
  - [ ] Performance analysis

---

## 🔧 Phase 5: Developer Experience & Tooling

### 🎨 Interactive Features
- [ ] **Interactive Configuration Wizard**
  - [ ] Step-by-step project setup
  - [ ] Technology stack selection
  - [ ] Environment configuration
  - [ ] Deployment target selection
- [ ] **Visual Project Analysis**
  - [ ] Dependency graph visualization
  - [ ] Architecture diagram generation
  - [ ] Performance bottleneck visualization
  - [ ] Security risk assessment display

### 🔄 Live Development Features
- [ ] **Watch Mode**
  - [ ] File change detection
  - [ ] Automatic re-analysis
  - [ ] Hot-reload configuration updates
  - [ ] Real-time feedback
- [ ] **Development Environment Setup**
  - [ ] Local development Docker configurations
  - [ ] Database seeding scripts
  - [ ] Test data generation
  - [ ] Development tooling setup

### 🧪 Testing & Validation
- [ ] **Generated Configuration Testing**
  - [ ] Docker build validation
  - [ ] Compose service verification
  - [ ] Terraform plan validation
  - [ ] Security compliance checking
- [ ] **Integration Testing**
  - [ ] End-to-end deployment testing
  - [ ] Performance benchmarking
  - [ ] Load testing scenarios
  - [ ] Failure mode testing

---

## 🎯 AI Enhancement Roadmap

### 🤖 Model Improvements
- [ ] **Custom Model Training**
  - [ ] Domain-specific fine-tuning
  - [ ] IaC best practices training
  - [ ] Security-focused training data
  - [ ] Performance optimization training
- [ ] **Multi-Modal AI**
  - [ ] Architecture diagram analysis
  - [ ] Code comment understanding
  - [ ] Documentation integration
  - [ ] Visual configuration interfaces

### 🧠 Intelligence Features
- [ ] **Predictive Analysis**
  - [ ] Resource usage prediction
  - [ ] Scaling requirement forecasting
  - [ ] Cost estimation and optimization
  - [ ] Performance bottleneck prediction
- [ ] **Automated Optimization**
  - [ ] Continuous configuration improvement
  - [ ] A/B testing for configurations
  - [ ] Performance-based optimization
  - [ ] Cost-based optimization suggestions

---

## 📈 Success Metrics

### 🎯 Quality Metrics
- [ ] **Generation Accuracy**
  - [ ] Target: 95% buildable configurations
  - [ ] Target: 90% production-ready without modification
  - [ ] Target: 85% security best practices compliance
  - [ ] Target: 80% performance optimization coverage

### ⚡ Performance Metrics
- [ ] **Analysis Speed**
  - [ ] Target: <5 seconds for 1000-file projects
  - [ ] Target: <10 seconds for full IaC generation
  - [ ] Target: <1 second for incremental updates
  - [ ] Target: <2 seconds for AI response processing

### 👥 User Experience Metrics
- [ ] **Adoption Metrics**
  - [ ] Target: 90% user satisfaction score
  - [ ] Target: 70% configuration acceptance rate
  - [ ] Target: 50% reduction in deployment setup time
  - [ ] Target: 85% user retention after 30 days

---

## 🛡️ Security & Compliance

### 🔒 Security Features
- [ ] **Security Best Practices**
  - [ ] Non-root user configurations
  - [ ] Minimal base images
  - [ ] Secret management integration
  - [ ] Network security policies
- [ ] **Compliance Standards**
  - [ ] SOC 2 compliance configurations
  - [ ] GDPR data protection setups
  - [ ] HIPAA compliance templates
  - [ ] PCI DSS security configurations

### 🔍 Vulnerability Management
- [ ] **Automated Scanning**
  - [ ] Base image vulnerability scanning
  - [ ] Dependency vulnerability assessment
  - [ ] Configuration security analysis
  - [ ] Runtime security monitoring setup

---

## 🌟 Innovation Areas

### 🚀 Future Technologies
- [ ] **Emerging Platforms**
  - [ ] WebAssembly (WASM) deployments
  - [ ] Edge computing configurations
  - [ ] Serverless container platforms
  - [ ] Quantum computing preparations
- [ ] **Next-Gen AI**
  - [ ] Code generation improvements
  - [ ] Natural language configuration
  - [ ] Visual configuration interfaces
  - [ ] Automated testing generation

---

*This roadmap is a living document and will be updated as we progress through development and gather user feedback.* 


================================================
FILE: test_update_check.sh
================================================
#!/bin/bash

echo "🧪 Testing Syncable CLI Smart Update Check"
echo "==========================================="

# Test 1: Clear cache and check with debug
echo -e "\n📋 Test 1: Clear cache and check with debug mode"
SYNC_CTL_DEBUG=1 cargo run -- --clear-update-cache analyze . 2>&1 | grep -E "(Checking for updates|Current version|Latest version|Update check skipped|Update available in cache)"

# Test 2: Check if intelligent cache works
echo -e "\n📋 Test 2: Second run should use smart cache (2-hour window)"
sleep 1
SYNC_CTL_DEBUG=1 cargo run -- analyze . 2>&1 | grep -E "(Update check skipped|Checking for updates|Update available in cache)"

# Test 3: Show cache contents
echo -e "\n📋 Test 3: Examining cache contents"
if [[ "$OSTYPE" == "darwin"* ]]; then
    CACHE_FILE="$HOME/Library/Caches/syncable-cli/version_cache.json"
else
    CACHE_FILE="$HOME/.cache/syncable-cli/version_cache.json"
fi

if [ -f "$CACHE_FILE" ]; then
    echo "Cache file found at: $CACHE_FILE"
    echo "Cache contents:"
    cat "$CACHE_FILE" | jq . 2>/dev/null || cat "$CACHE_FILE"
else
    echo "No cache file found at: $CACHE_FILE"
fi

# Test 4: Force check again
echo -e "\n📋 Test 4: Force check with --clear-update-cache"
SYNC_CTL_DEBUG=1 cargo run -- --clear-update-cache analyze . 2>&1 | grep -E "(Update cache cleared|Checking for updates|Removed update cache)"

echo -e "\n✅ Test complete!"
echo "Smart update system features:"
echo "  • Checks every 2 hours when no update available"
echo "  • Shows update immediately if cached version is newer" 
echo "  • Stores detailed version info in JSON cache"
echo "  • Enhanced notification with clear update instructions"
echo "  • Multiple update methods (Cargo, direct download, install script)"
echo "  • To test with a real update notification, the GitHub release needs to have a newer version than 0.5.0" 


================================================
FILE: .rustfmt.toml
================================================
edition = "2021"
max_width = 100
tab_spaces = 4
newline_style = "Unix"
use_small_heuristics = "Default"
reorder_imports = true
reorder_modules = true
remove_nested_parens = true
merge_derives = true
use_try_shorthand = true
use_field_init_shorthand = true
force_explicit_abi = true
empty_item_single_line = true
struct_lit_single_line = true
fn_single_line = false
where_single_line = false
imports_layout = "Vertical"
imports_granularity = "Crate" 


================================================
FILE: docs/cli-display-modes.md
================================================
# CLI Display Modes

The `sync-ctl analyze` command now offers multiple display modes to present analysis results in different formats optimized for various use cases.

## Display Options

### 1. Matrix View (Default) - `--display matrix`

The matrix view provides a modern, compact dashboard that's easy to scan and compare projects side-by-side. This is the new default display mode.

```bash
sync-ctl analyze . --display matrix
# or simply
sync-ctl analyze .
```

**Example Output:**
```
═══════════════════════════════════════════════════════════════════════════════════════════════════
📊 PROJECT ANALYSIS DASHBOARD
═══════════════════════════════════════════════════════════════════════════════════════════════════

┌─ Architecture Overview ─────────────────────────────────────────────────────────────────────────┐
│ Type:               Monorepo (3 projects)                                                       │
│ Pattern:            Fullstack                                                                   │
│                     Full-stack app with frontend/backend separation                             │
└─────────────────────────────────────────────────────────────────────────────────────────────────┘

┌─ Technology Stack ──────────────────────────────────────────────────────────────────────────────┐
│ Languages:      TypeScript                                                                      │
│ Frameworks:     Encore, Tanstack Start                                                          │
│ Databases:      Drizzle ORM                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────────────────┘

┌─ Projects Matrix ──────────────────────────────────────────────────────────────────────────────┐
│ ┌─────────────────┬──────────────┬───────────┬─────────────────┬───────┬────────┬──────────┐   │
│ │ Project         │ Type         │ Languages │ Main Tech       │ Ports │ Docker │ Deps     │   │
│ ├─────────────────┼──────────────┼───────────┼─────────────────┼───────┼────────┼──────────┤   │
│ │  backend        │ Backend        │ TypeScript│ Encore            │ 4000  │ ✓      │ 32   │   │
│ │  devops-agent   │ Infrastructure │ TypeScript │ -                │ -     │ ✗      │ 5    │   │
│ │  frontend       │ Frontend       │ TypeScript│ Tanstack Start    │ 3000  │ ✓      │ 123  │   │
│ └─────────────────┴──────────────┴───────────┴─────────────────┴───────┴────────┴──────────┘   │ 
└────────────────────────────────────────────────────────────────────────────────────────────────┘

┌─ Docker Infrastructure ─────────────────────────────────────────────────────────────────────────┐
│ Dockerfiles:              2                                                                     │
│ Compose Files:            2                                                                     │
│ Total Services:           5                                                                     │
│ Orchestration Patterns:   Microservices                                                         │
│ ────────────────────────────────────────────────────────────────────────────────────────────────│
│ Service Connectivity:                                                                           │
│   encore-postgres: 5431:5432                                                                    │
│   encore: 4000:8080 → encore-postgres                                                           │
│   intellitask-app: 3000:3000                                                                    │
└─────────────────────────────────────────────────────────────────────────────────────────────────┘

┌─ Analysis Metrics ─────────────────────────────────────────────────────────────────────────────┐
│  Duration: 57ms    Files: 294         Score: 87%         Version: 0.3.0                        │
└────────────────────────────────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════════════════════════════════
```

### 2. Summary View - `--display summary`

A brief overview of the analysis results, perfect for quick checks or CI/CD pipelines.

```bash
sync-ctl analyze . --display summary
```

**Example Output:**
```
▶ PROJECT ANALYSIS SUMMARY
──────────────────────────────────────────────────
│ Architecture: Monorepo (3 projects)
│ Pattern: Fullstack
│ Stack: TypeScript
│ Frameworks: Encore, Tanstack Start
│ Analysis Time: 57ms
│ Confidence: 87%
──────────────────────────────────────────────────
```

### 3. Detailed View (Legacy) - `--display detailed` or `-d`

The traditional verbose output with all details in a vertical layout. Useful when you need to see everything about each project.

```bash
sync-ctl analyze . --display detailed
# or for backward compatibility
sync-ctl analyze . -d
```

This produces the traditional long-form output with all details about each project.

### 4. JSON Output - `--json`

Machine-readable JSON output for integration with other tools or programmatic processing.

```bash
sync-ctl analyze . --json
```

## Choosing the Right Display Mode

- **Matrix View**: Best for daily use, comparing multiple projects, and getting a quick overview with key metrics
- **Summary View**: Ideal for CI/CD pipelines, scripts, or when you just need basic information
- **Detailed View**: Use when you need to see every detail about the analysis, including all dependencies, scripts, and configurations
- **JSON**: Perfect for integration with other tools, creating reports, or feeding data to dashboards

## Benefits of the New Matrix View

1. **Reduced Scrolling**: All important information fits on one screen
2. **Easy Comparison**: Projects are displayed side-by-side in a table
3. **Visual Hierarchy**: Box-drawing characters and colors create clear sections
4. **Key Metrics Focus**: Shows only the most important information by default
5. **Modern Appearance**: Clean, professional look with proper alignment
6. **LLM-Friendly**: The structured format is easy for AI assistants to parse and understand

## Color Coding

The matrix view uses colors strategically:
- **Blue**: Headers and structural elements
- **Yellow**: Important values and counts
- **Green**: Success indicators and positive metrics
- **Magenta**: Frameworks and technologies
- **Cyan**: Interactive elements and services
- **Red**: Error states or missing components

## Tips

- The matrix view automatically adjusts based on terminal width
- Use `--no-color` to disable colors if needed
- Pipe to `less` for scrolling in detailed view: `sync-ctl analyze . -d | less -R`
- Combine with `jq` for JSON processing: `sync-ctl analyze . --json | jq '.projects[].name'` 


================================================
FILE: docs/command-overview.md
================================================
# 🚀 Syncable CLI - Complete Command Overview

This document provides a comprehensive overview of all available commands and their different display modes.

## 📊 Analysis Commands

### 1. Basic Project Analysis

```bash
# Modern matrix view (default) - compact dashboard
sync-ctl analyze .

# Detailed view with full Docker analysis 
sync-ctl analyze . --display detailed
# Or use the legacy flag
sync-ctl analyze . -d

# Summary view for CI/CD pipelines
sync-ctl analyze . --display summary

# JSON output for scripts
sync-ctl analyze . --json

# Analyze specific project path
sync-ctl analyze /path/to/project
```

### 2. Display Mode Comparison

#### Matrix View (Default) 🆕
- **Best for**: Quick overview, comparing multiple projects
- **Features**: Modern dashboard with box-drawing characters, side-by-side project comparison, key metrics
- **Docker Info**: Overview with service counts and orchestration patterns
- **Note**: Box alignment improvements in progress for better visual consistency

#### Detailed View 
- **Best for**: In-depth analysis, debugging, comprehensive reports
- **Features**: Full Docker analysis, complete technology breakdown, all metadata
- **Docker Info**: Complete Docker infrastructure analysis including:
  - Dockerfile analysis with base images, ports, stages
  - Docker Compose services with dependencies and networking
  - Orchestration patterns and service discovery
  - Port mappings and volume configurations
- **Usage**: Use this view when you need complete information about your project

#### Summary View
- **Best for**: CI/CD pipelines, quick status checks
- **Features**: Brief overview with essential information only
- **Usage**: Perfect for automated scripts and quick validation

## 🔍 Security & Vulnerability Commands

### 3. Security Analysis (Turbo Engine - 10-100x Faster)

```bash
# Comprehensive security scan (default: thorough mode)
sync-ctl security .

# Different scan modes for speed vs coverage
sync-ctl security . --mode lightning    # Fastest - critical files only
sync-ctl security . --mode fast        # Smart sampling
sync-ctl security . --mode balanced    # Good coverage
sync-ctl security . --mode thorough    # Comprehensive (default)
sync-ctl security . --mode paranoid    # Maximum coverage

# Include low-severity findings
sync-ctl security . --include-low

# Skip specific checks
sync-ctl security . --no-secrets --no-code-patterns

# Export security report
sync-ctl security . --output security-report.json --format json

# Fail CI/CD on security findings
sync-ctl security . --fail-on-findings
```

#### Security Scan Modes

| Mode | Speed | Coverage | Use Case |
|------|-------|----------|----------|
| **Lightning** | 🚀 Fastest | Critical files only | Pre-commit hooks, CI checks |
| **Fast** | ⚡ Very Fast | Smart sampling | Development workflow |
| **Balanced** | 🎯 Optimized | Good coverage | Regular security checks |
| **Thorough** | 🔍 Complete | Comprehensive | Security audits (default) |
| **Paranoid** | 🕵️ Maximum | Everything + low severity | Compliance, releases |

### 4. Vulnerability Scanning

```bash
# Scan all dependencies for vulnerabilities
sync-ctl vulnerabilities .

# Filter by severity
sync-ctl vulnerabilities . --severity high
sync-ctl vulnerabilities . --severity critical

# Export vulnerability report
sync-ctl vulnerabilities . --format json --output vulns.json

# Check specific project path
sync-ctl vulnerabilities /path/to/project
```

### 5. Dependency Analysis

```bash
# Analyze dependencies with licenses
sync-ctl dependencies . --licenses

# Include vulnerability checking
sync-ctl dependencies . --vulnerabilities

# Production dependencies only
sync-ctl dependencies . --prod-only

# Development dependencies only  
sync-ctl dependencies . --dev-only

# JSON output
sync-ctl dependencies . --format json
```

## 🛠️ Tool Management Commands

### 6. Vulnerability Scanning Tools

```bash
# Check tool installation status
sync-ctl tools status

# Install missing tools
sync-ctl tools install

# Install for specific languages
sync-ctl tools install --languages rust,python

# Include OWASP Dependency Check (large download)
sync-ctl tools install --include-owasp

# Verify tool functionality
sync-ctl tools verify

# Get installation guide
sync-ctl tools guide

# Platform-specific guides
sync-ctl tools guide --platform linux
```

## 🏗️ Generation Commands

### 7. IaC Generation

```bash
# Generate all IaC files
sync-ctl generate .
sync-ctl generate . --all

# Generate specific types
sync-ctl generate . --dockerfile --compose
sync-ctl generate . --terraform

# Dry run (preview only)
sync-ctl generate . --dry-run

# Custom output directory
sync-ctl generate . --output ./infrastructure/

# Overwrite existing files
sync-ctl generate . --force
```

## 🔄 Validation Commands (Coming Soon)

### 8. IaC Validation

```bash
# Validate generated IaC files (not yet implemented)
sync-ctl validate .

# Validate specific types (planned)
sync-ctl validate . --types dockerfile,compose

# Auto-fix issues (planned)
sync-ctl validate . --fix
```

## 📋 Information Commands

### 9. Support Information

```bash
# Show supported languages
sync-ctl support --languages

# Show supported frameworks
sync-ctl support --frameworks

# Show all supported technologies
sync-ctl support

# Detailed support information
sync-ctl support --detailed
```

## 🎯 Advanced Usage Examples

### Complete Project Analysis Workflow

```bash
# 1. Quick overview
sync-ctl analyze .

# 2. Detailed analysis with Docker
sync-ctl analyze . --display detailed

# 3. Security scan
sync-ctl security .

# 4. Vulnerability check
sync-ctl vulnerabilities . --severity medium

# 5. Generate IaC
sync-ctl generate . --all
```

### CI/CD Integration

```bash
# Quick check for CI/CD
sync-ctl analyze . --display summary

# Security scan that fails on findings
sync-ctl security . --fail-on-findings

# Vulnerability scan with threshold
sync-ctl vulnerabilities . --severity high

# JSON reports for processing
sync-ctl dependencies . --vulnerabilities --format json > deps.json
```

### Monorepo Analysis

```bash
# Analyze entire monorepo
sync-ctl analyze .

# Matrix view shows all projects side-by-side
sync-ctl analyze . --display matrix

# Individual project analysis
cd frontend && sync-ctl analyze . --display detailed
cd ../backend && sync-ctl analyze . --display detailed
```

## 🔧 Global Configuration Options

### Global Flags (Available for all commands)
- `--config <file>` - Custom configuration file
- `--verbose` / `-v` - Verbose output (-v info, -vv debug, -vvv trace)
- `--quiet` - Suppress all output except errors
- `--json` - JSON output format where applicable
- `--clear-update-cache` - Force update check

### Command-Specific Options

#### Analysis Options
- `--display <mode>` - matrix (default), detailed, summary
- `--only <components>` - Analyze specific components only
- `--json` - JSON output for the analyze command

#### Security Options
- `--mode <scan-mode>` - lightning, fast, balanced, thorough, paranoid
- `--include-low` - Include low-severity findings
- `--no-secrets` - Skip secret detection
- `--no-code-patterns` - Skip code pattern analysis
- `--fail-on-findings` - Exit with error on security issues

#### Generation Options
- `--output <directory>` - Custom output directory
- `--dry-run` - Preview without creating files
- `--force` - Overwrite existing files
- `--all` - Generate all IaC types

#### Tool Options
- `--languages <list>` - Target specific languages
- `--include-owasp` - Include OWASP Dependency Check
- `--dry-run` - Preview installation
- `--yes` - Skip confirmation prompts

## 💡 Pro Tips

1. **For Development**: Use `--display detailed` to see complete Docker analysis
2. **For CI/CD**: Use `--display summary` for quick checks
3. **For Security**: Run `sync-ctl security . --fail-on-findings` in CI/CD
4. **For Performance**: Use `--mode lightning` for fastest security scans
5. **For Debugging**: Use `--verbose` for detailed logs
6. **For Automation**: Use `--json` output with other tools
7. **For Teams**: Share vulnerability reports with `--output` option
8. **For Updates**: Use `--clear-update-cache` to force update checks

## 🚀 Implementation Status

### ✅ Fully Implemented
- **analyze** - Project analysis with multiple display modes
- **security** - Turbo security engine with 5 scan modes
- **vulnerabilities** - Dependency vulnerability scanning
- **dependencies** - Comprehensive dependency analysis
- **support** - Technology support information
- **tools** - Vulnerability tool management

### 🚧 In Development
- **validate** - IaC validation and best practices checking
- **generate** - IaC file generation (Dockerfile, Compose, Terraform)
- Enhanced monorepo generation with per-project IaC files
- Advanced compliance framework checking

### 🔮 Coming Soon
- **Cloud Integration** - Deploy directly to cloud platforms
- **Monitoring Setup** - Automated monitoring configuration
- **Performance Analysis** - Resource optimization recommendations
- **Interactive Mode** - Guided setup and configuration wizard

## 📖 Getting Help

```bash
# Get help with any command
sync-ctl --help                     # Show all available commands
sync-ctl analyze --help            # Show analyze command options
sync-ctl security --help           # Show security scanning options
sync-ctl vulnerabilities --help    # Show vulnerability check options
sync-ctl generate --help           # Show generation options
sync-ctl tools --help              # Show tool management options
``` 


================================================
FILE: examples/check_vulnerabilities.rs
================================================
use syncable_cli::analyzer::dependency_parser::{DependencyParser};
use syncable_cli::analyzer::vulnerability_checker::VulnerabilityChecker;
use std::path::Path;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    env_logger::init();
    
    let project_path = Path::new(".");
    println!("🔍 Checking vulnerabilities in: {}", project_path.display());
    
    // Parse dependencies
    let parser = DependencyParser::new();
    let dependencies = parser.parse_all_dependencies(project_path)?;
    
    if dependencies.is_empty() {
        println!("No dependencies found.");
        return Ok(());
    }
    
    // Print found dependencies
    for (lang, deps) in &dependencies {
        println!("\n{:?} dependencies: {}", lang, deps.len());
        for dep in deps.iter().take(5) {
            println!("  - {} v{}", dep.name, dep.version);
        }
        if deps.len() > 5 {
            println!("  ... and {} more", deps.len() - 5);
        }
    }
    
    // Check vulnerabilities
    println!("\n🛡️ Checking for vulnerabilities...");
    let checker = VulnerabilityChecker::new();
    let report = checker.check_all_dependencies(&dependencies, project_path).await?;
    
    println!("\n📊 Vulnerability Report");
    println!("Checked at: {}", report.checked_at.format("%Y-%m-%d %H:%M:%S UTC"));
    println!("Total vulnerabilities: {}", report.total_vulnerabilities);
    
    if report.total_vulnerabilities > 0 {
        println!("\nSeverity breakdown:");
        if report.critical_count > 0 {
            println!("  CRITICAL: {}", report.critical_count);
        }
        if report.high_count > 0 {
            println!("  HIGH: {}", report.high_count);
        }
        if report.medium_count > 0 {
            println!("  MEDIUM: {}", report.medium_count);
        }
        if report.low_count > 0 {
            println!("  LOW: {}", report.low_count);
        }
        
        println!("\nVulnerable dependencies:");
        for vuln_dep in &report.vulnerable_dependencies {
            println!("\n  📦 {} v{} ({:?})", vuln_dep.name, vuln_dep.version, vuln_dep.language);
            for vuln in &vuln_dep.vulnerabilities {
                println!("    ⚠️  {} [{:?}] - {}", vuln.id, vuln.severity, vuln.title);
                if let Some(ref cve) = vuln.cve {
                    println!("       CVE: {}", cve);
                }
                if let Some(ref patched) = vuln.patched_versions {
                    println!("       Fix: Upgrade to {}", patched);
                }
            }
        }
    } else {
        println!("\n✅ No known vulnerabilities found!");
    }
    
    Ok(())
} 


================================================
FILE: examples/debug_java_vulnerabilities.rs
================================================
use env_logger;
use log::{info, error};
use syncable_cli::analyzer::dependency_parser::{DependencyParser, Language};
use syncable_cli::analyzer::vulnerability_checker::VulnerabilityChecker;
use std::path::Path;
use std::env;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Enable debug logging
    env_logger::Builder::from_default_env()
        .filter_level(log::LevelFilter::Debug)
        .init();
    
    // Get project path from command line args or use current directory
    let args: Vec<String> = env::args().collect();
    let project_path = if args.len() > 1 {
        Path::new(&args[1])
    } else {
        Path::new(".")
    };
    
    info!("🔍 Debug Java vulnerability scanning in: {}", project_path.display());
    
    // Parse dependencies
    let parser = DependencyParser::new();
    info!("📦 Parsing dependencies...");
    let dependencies = parser.parse_all_dependencies(project_path)?;
    
    if dependencies.is_empty() {
        error!("❌ No dependencies found!");
        info!("Make sure you're in a Java project directory with:");
        info!("  - pom.xml (Maven project)");
        info!("  - build.gradle or build.gradle.kts (Gradle project)");
        return Ok(());
    }
    
    // Show detailed dependency information
    info!("📊 Found dependencies in {} languages:", dependencies.len());
    for (lang, deps) in &dependencies {
        info!("  {:?}: {} dependencies", lang, deps.len());
        if *lang == Language::Java {
            info!("    Java dependencies details:");
            for dep in deps.iter().take(10) {
                info!("      - {} v{} (source: {:?})", dep.name, dep.version, dep.source);
            }
            if deps.len() > 10 {
                info!("      ... and {} more", deps.len() - 10);
            }
        }
    }
    
    // Check if Java dependencies were found
    if !dependencies.contains_key(&Language::Java) {
        error!("❌ No Java dependencies detected!");
        info!("Troubleshooting steps:");
        info!("1. Make sure you're in a Java project directory");
        info!("2. For Maven projects: ensure pom.xml exists and has <dependencies> section");
        info!("3. For Gradle projects: ensure build.gradle exists with dependency declarations");
        info!("4. Run 'mvn dependency:resolve' or 'gradle build' to ensure dependencies are resolved");
        return Ok(());
    }
    
    // Check vulnerabilities
    info!("🛡️ Checking for vulnerabilities...");
    let checker = VulnerabilityChecker::new();
    
    match checker.check_all_dependencies(&dependencies, project_path).await {
        Ok(report) => {
            info!("✅ Vulnerability scan completed successfully!");
            info!("📊 Results:");
            info!("  Total vulnerabilities: {}", report.total_vulnerabilities);
            info!("  Critical: {}", report.critical_count);
            info!("  High: {}", report.high_count);
            info!("  Medium: {}", report.medium_count);
            info!("  Low: {}", report.low_count);
            
            if report.total_vulnerabilities > 0 {
                info!("🚨 Vulnerable dependencies:");
                for vuln_dep in &report.vulnerable_dependencies {
                    info!("  - {} v{} ({} vulnerabilities)", 
                          vuln_dep.name, vuln_dep.version, vuln_dep.vulnerabilities.len());
                    for vuln in &vuln_dep.vulnerabilities {
                        info!("    • {} [{:?}] - {}", vuln.id, vuln.severity, vuln.title);
                    }
                }
            } else {
                info!("✅ No vulnerabilities found!");
                info!("This could mean:");
                info!("  - Your dependencies are up to date and secure");
                info!("  - The vulnerability scanner (grype) didn't find any issues");
                info!("  - The dependency versions couldn't be matched with vulnerability databases");
            }
        }
        Err(e) => {
            error!("❌ Vulnerability scanning failed: {}", e);
            info!("Common issues:");
            info!("  - grype not installed: brew install grype");
            info!("  - Project not built: run 'mvn compile' or 'gradle build'");
            info!("  - Dependencies not resolved: run 'mvn dependency:resolve'");
        }
    }
    
    Ok(())
} 


================================================
FILE: examples/security_analysis.rs
================================================
use std::path::Path;
use syncable_cli::analyzer::{analyze_project, SecurityAnalyzer, SecurityAnalysisConfig};

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    env_logger::init();
    
    // Get project path from command line arguments or use current directory
    let project_path = std::env::args()
        .nth(1)
        .unwrap_or_else(|| ".".to_string());
    
    println!("🔍 Analyzing security for project: {}", project_path);
    
    // First perform a general project analysis
    let project_analysis = analyze_project(Path::new(&project_path))?;
    
    println!("📊 Project Analysis Summary:");
    println!("  Languages: {:?}", project_analysis.languages.iter().map(|l| &l.name).collect::<Vec<_>>());
    println!("  Technologies: {:?}", project_analysis.technologies.iter().map(|t| &t.name).collect::<Vec<_>>());
    println!("  Environment Variables: {}", project_analysis.environment_variables.len());
    
    // Create security analyzer with default configuration
    let security_config = SecurityAnalysisConfig {
        include_low_severity: true, // Include low severity findings for demonstration
        check_secrets: true,
        check_code_patterns: true,
        check_infrastructure: true,
        check_compliance: true,
        frameworks_to_check: vec![
            "SOC2".to_string(),
            "GDPR".to_string(),
            "OWASP".to_string(),
        ],
        ignore_patterns: vec![
            "node_modules".to_string(),
            ".git".to_string(),
            "target".to_string(),
        ],
        skip_gitignored_files: true,
        downgrade_gitignored_severity: false,
    };
    
    let mut security_analyzer = SecurityAnalyzer::with_config(security_config)?;
    
    // Perform security analysis
    println!("\n🛡️  Running comprehensive security analysis...");
    let security_report = security_analyzer.analyze_security(&project_analysis)?;
    
    // Display results
    println!("\n📋 Security Analysis Report");
    println!("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━");
    println!("🏆 Overall Security Score: {:.1}/100", security_report.overall_score);
    println!("⚠️  Risk Level: {:?}", security_report.risk_level);
    println!("🔍 Total Findings: {}", security_report.total_findings);
    
    if !security_report.findings_by_severity.is_empty() {
        println!("\n📊 Findings by Severity:");
        for (severity, count) in &security_report.findings_by_severity {
            let emoji = match severity {
                syncable_cli::analyzer::SecuritySeverity::Critical => "🚨",
                syncable_cli::analyzer::SecuritySeverity::High => "⚠️ ",
                syncable_cli::analyzer::SecuritySeverity::Medium => "⚡",
                syncable_cli::analyzer::SecuritySeverity::Low => "ℹ️ ",
                syncable_cli::analyzer::SecuritySeverity::Info => "💡",
            };
            println!("  {} {:?}: {}", emoji, severity, count);
        }
    }
    
    if !security_report.findings_by_category.is_empty() {
        println!("\n🗂️  Findings by Category:");
        for (category, count) in &security_report.findings_by_category {
            let emoji = match category {
                syncable_cli::analyzer::SecurityCategory::SecretsExposure => "🔐",
                syncable_cli::analyzer::SecurityCategory::InsecureConfiguration => "⚙️ ",
                syncable_cli::analyzer::SecurityCategory::CodeSecurityPattern => "💻",
                syncable_cli::analyzer::SecurityCategory::InfrastructureSecurity => "🏗️ ",
                syncable_cli::analyzer::SecurityCategory::AuthenticationSecurity => "🔑",
                syncable_cli::analyzer::SecurityCategory::DataProtection => "🛡️ ",
                syncable_cli::analyzer::SecurityCategory::NetworkSecurity => "🌐",
                syncable_cli::analyzer::SecurityCategory::Compliance => "📜",
            };
            println!("  {} {:?}: {}", emoji, category, count);
        }
    }
    
    // Display detailed findings
    if !security_report.findings.is_empty() {
        println!("\n🔍 Detailed Security Findings:");
        println!("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━");
        
        for (i, finding) in security_report.findings.iter().enumerate() {
            let severity_emoji = match finding.severity {
                syncable_cli::analyzer::SecuritySeverity::Critical => "🚨",
                syncable_cli::analyzer::SecuritySeverity::High => "⚠️ ",
                syncable_cli::analyzer::SecuritySeverity::Medium => "⚡",
                syncable_cli::analyzer::SecuritySeverity::Low => "ℹ️ ",
                syncable_cli::analyzer::SecuritySeverity::Info => "💡",
            };
            
            println!("\n{}. {} [{}] {}", i + 1, severity_emoji, finding.id, finding.title);
            println!("   📝 {}", finding.description);
            
            if let Some(file) = &finding.file_path {
                print!("   📁 File: {}", file.display());
                if let Some(line) = finding.line_number {
                    print!(" (line {})", line);
                }
                println!();
            }
            
            if let Some(evidence) = &finding.evidence {
                println!("   🔍 Evidence: {}", evidence);
            }
            
            if !finding.remediation.is_empty() {
                println!("   🔧 Remediation:");
                for remediation in &finding.remediation {
                    println!("      • {}", remediation);
                }
            }
            
            if let Some(cwe) = &finding.cwe_id {
                println!("   🏷️  CWE: {}", cwe);
            }
        }
    }
    
    // Display recommendations
    if !security_report.recommendations.is_empty() {
        println!("\n💡 Security Recommendations:");
        println!("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━");
        for (i, recommendation) in security_report.recommendations.iter().enumerate() {
            println!("{}. {}", i + 1, recommendation);
        }
    }
    
    // Display compliance status
    if !security_report.compliance_status.is_empty() {
        println!("\n📜 Compliance Status:");
        println!("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━");
        for (framework, status) in &security_report.compliance_status {
            println!("🏛️  {}: {:.1}% coverage", framework, status.coverage);
            if !status.missing_controls.is_empty() {
                println!("   Missing controls: {}", status.missing_controls.join(", "));
            }
        }
    }
    
    println!("\n✅ Security analysis completed!");
    
    // Exit with appropriate code based on findings
    if security_report.findings_by_severity.contains_key(&syncable_cli::analyzer::SecuritySeverity::Critical) {
        println!("❌ Critical security issues found. Please address immediately.");
        std::process::exit(1);
    } else if security_report.findings_by_severity.contains_key(&syncable_cli::analyzer::SecuritySeverity::High) {
        println!("⚠️  High severity security issues found. Review recommended.");
        std::process::exit(2);
    }
    
    Ok(())
} 


================================================
FILE: examples/test_project_context.rs
================================================
//! Example: Test Project Context Analyzer
//! 
//! This example demonstrates the Project Context Analyzer functionality
//! by analyzing the current project.

use syncable_cli::analyzer::{analyze_project, ProjectType};
use std::env;
use std::path::Path;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logger
    env_logger::init();
    
    // Get the project path from command line or use current directory
    let path = env::args()
        .nth(1)
        .unwrap_or_else(|| ".".to_string());
    
    let project_path = Path::new(&path);
    
    println!("🔍 Analyzing project at: {}", project_path.display());
    println!("{}", "=".repeat(60));
    
    // Run the analysis
    let analysis = analyze_project(project_path)?;
    
    // Display Project Context Analysis Results
    println!("\n📊 PROJECT CONTEXT ANALYSIS RESULTS");
    println!("{}", "=".repeat(60));
    
    // Project Type (Roadmap Requirement #5)
    println!("\n🎯 Project Type: {:?}", analysis.project_type);
    match analysis.project_type {
        ProjectType::WebApplication => println!("   This is a web application with UI"),
        ProjectType::ApiService => println!("   This is an API service without UI"),
        ProjectType::CliTool => println!("   This is a command-line tool"),
        ProjectType::Library => println!("   This is a library/package"),
        ProjectType::Microservice => println!("   This is a microservice"),
        ProjectType::StaticSite => println!("   This is a static website"),
        _ => println!("   Project type details not available"),
    }
    
    // Entry Points (Roadmap Requirement #1)
    println!("\n📍 Entry Points ({}):", analysis.entry_points.len());
    for (i, entry) in analysis.entry_points.iter().enumerate() {
        println!("   {}. File: {}", i + 1, entry.file.display());
        if let Some(func) = &entry.function {
            println!("      Function: {}", func);
        }
        if let Some(cmd) = &entry.command {
            println!("      Command: {}", cmd);
        }
    }
    
    // Ports (Roadmap Requirement #2)
    println!("\n🔌 Exposed Ports ({}):", analysis.ports.len());
    for port in &analysis.ports {
        println!("   - Port {}: {:?}", port.number, port.protocol);
        if let Some(desc) = &port.description {
            println!("     {}", desc);
        }
    }
    
    // Environment Variables (Roadmap Requirement #3)
    println!("\n🔐 Environment Variables ({}):", analysis.environment_variables.len());
    let required_vars: Vec<_> = analysis.environment_variables.iter()
        .filter(|ev| ev.required)
        .collect();
    let optional_vars: Vec<_> = analysis.environment_variables.iter()
        .filter(|ev| !ev.required)
        .collect();
    
    if !required_vars.is_empty() {
        println!("   Required:");
        for var in required_vars {
            println!("     - {} {}", 
                var.name,
                if let Some(desc) = &var.description { 
                    format!("({})", desc) 
                } else { 
                    String::new() 
                }
            );
        }
    }
    
    if !optional_vars.is_empty() {
        println!("   Optional:");
        for var in optional_vars {
            println!("     - {} = {:?}", 
                var.name, 
                var.default_value.as_deref().unwrap_or("no default")
            );
        }
    }
    
    // Build Scripts (Roadmap Requirement #4)
    println!("\n🔨 Build Scripts ({}):", analysis.build_scripts.len());
    let default_scripts: Vec<_> = analysis.build_scripts.iter()
        .filter(|bs| bs.is_default)
        .collect();
    let other_scripts: Vec<_> = analysis.build_scripts.iter()
        .filter(|bs| !bs.is_default)
        .collect();
    
    if !default_scripts.is_empty() {
        println!("   Default scripts:");
        for script in default_scripts {
            println!("     - {}: {}", script.name, script.command);
        }
    }
    
    if !other_scripts.is_empty() {
        println!("   Other scripts:");
        for script in other_scripts {
            println!("     - {}: {}", script.name, script.command);
        }
    }
    
    // Summary
    println!("\n📋 SUMMARY");
    println!("{}", "=".repeat(60));
    println!("✅ All 5 Project Context Analyzer requirements verified:");
    println!("   1. Entry points detected: {}", 
        if analysis.entry_points.is_empty() { "❌ None" } else { "✅ Yes" });
    println!("   2. Ports identified: {}", 
        if analysis.ports.is_empty() { "❌ None" } else { "✅ Yes" });
    println!("   3. Environment variables extracted: {}", 
        if analysis.environment_variables.is_empty() { "❌ None" } else { "✅ Yes" });
    println!("   4. Build scripts analyzed: {}", 
        if analysis.build_scripts.is_empty() { "❌ None" } else { "✅ Yes" });
    println!("   5. Project type determined: {}", 
        if matches!(analysis.project_type, ProjectType::Unknown) { "❌ Unknown" } else { "✅ Yes" });
    
    println!("\n✨ Project Context Analysis Complete!");
    
    Ok(())
} 


================================================
FILE: scripts/install-vuln-tools.sh
================================================
#!/bin/bash
# Vulnerability Scanning Tools Installation Script
# This script installs the necessary tools for vulnerability scanning across different languages

set -e

# Color codes for better output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
print_success() {
    echo -e "${GREEN}✅ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}⚠️  $1${NC}"
}

print_error() {
    echo -e "${RED}❌ $1${NC}"
}

print_info() {
    echo -e "${BLUE}ℹ️  $1${NC}"
}

print_step() {
    echo -e "${BLUE}🔧 $1${NC}"
}

# Function to check if a command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Function to check tool installation
check_tool_status() {
    local tool="$1"
    local description="$2"
    
    case "$tool" in
        "cargo-audit")
            if cargo audit --version >/dev/null 2>&1; then
                print_success "$description"
                return 0
            fi
            ;;
        "npm")
            if command_exists npm; then
                print_success "$description"
                return 0
            fi
            ;;
        "pip-audit")
            if command_exists pip-audit; then
                print_success "$description"
                return 0
            fi
            ;;
        "govulncheck")
            if command_exists govulncheck || test -f "$HOME/go/bin/govulncheck"; then
                print_success "$description"
                return 0
            fi
            ;;
        "grype")
            if command_exists grype || test -f "$HOME/.local/bin/grype"; then
                print_success "$description"
                return 0
            fi
            ;;
        "safety")
            if command_exists safety; then
                print_success "$description"
                return 0
            fi
            ;;
        "bandit")
            if command_exists bandit; then
                print_success "$description"
                return 0
            fi
            ;;
        "dependency-check")
            if command_exists dependency-check || test -f "$HOME/.local/bin/dependency-check"; then
                print_success "$description"
                return 0
            fi
            ;;
    esac
    
    print_warning "$description (missing)"
    return 1
}

# Function to manually install grype
install_grype_manually() {
    print_step "Installing grype manually..."
    
    # Create local bin directory
    mkdir -p "$HOME/.local/bin"
    
    # Detect platform
    case "$(uname -s)" in
        Darwin)
            case "$(uname -m)" in
                x86_64) PLATFORM="darwin_amd64" ;;
                arm64|aarch64) PLATFORM="darwin_arm64" ;;
                *) 
                    print_warning "Unsupported macOS architecture"
                    return 1
                    ;;
            esac
            ;;
        Linux)
            case "$(uname -m)" in
                x86_64) PLATFORM="linux_amd64" ;;
                aarch64|arm64) PLATFORM="linux_arm64" ;;
                *) 
                    print_warning "Unsupported Linux architecture"
                    return 1
                    ;;
            esac
            ;;
        *)
            print_warning "Unsupported operating system"
            return 1
            ;;
    esac
    
    # Download and install
    VERSION="0.92.2"
    URL="https://github.com/anchore/grype/releases/download/v${VERSION}/grype_${VERSION}_${PLATFORM}.tar.gz"
    
    if command_exists curl; then
        print_info "Downloading grype v${VERSION} for ${PLATFORM}..."
        if curl -L "$URL" | tar -xz -C "$HOME/.local/bin" grype; then
            chmod +x "$HOME/.local/bin/grype"
            print_success "grype installed to ~/.local/bin/grype"
            return 0
        else
            print_warning "Failed to download grype automatically"
            return 1
        fi
    else
        print_warning "curl not found"
        return 1
    fi
}

# Function to install OWASP Dependency Check
install_dependency_check() {
    print_step "Installing OWASP Dependency Check..."
    
    # Create installation directory
    mkdir -p "$HOME/.local/dependency-check"
    mkdir -p "$HOME/.local/bin"
    
    VERSION="10.0.4"
    URL="https://github.com/jeremylong/DependencyCheck/releases/download/v${VERSION}/dependency-check-${VERSION}-release.zip"
    
    if command_exists curl && command_exists unzip; then
        print_info "Downloading OWASP Dependency Check v${VERSION}..."
        
        # Download and extract
        if curl -L "$URL" -o "/tmp/dependency-check.zip" && \
           unzip -o "/tmp/dependency-check.zip" -d "$HOME/.local/" && \
           ln -sf "$HOME/.local/dependency-check/bin/dependency-check.sh" "$HOME/.local/bin/dependency-check"; then
            
            chmod +x "$HOME/.local/bin/dependency-check"
            print_success "OWASP Dependency Check installed"
            rm -f "/tmp/dependency-check.zip"
            return 0
        else
            print_warning "Failed to install OWASP Dependency Check"
            return 1
        fi
    else
        print_warning "curl or unzip not found"
        return 1
    fi
}

# Main installation function
install_vulnerability_tools() {
    echo "🛡️  Vulnerability Scanning Tools Installation"
    echo "=============================================="
    echo ""
    
    # Check current status first
    print_step "Checking current tool status..."
    echo ""
    
    # Language-specific tools
    echo "📋 Language-Specific Tools:"
    check_tool_status "cargo-audit" "Rust - cargo-audit"
    check_tool_status "npm" "JavaScript/TypeScript - npm audit"
    check_tool_status "pip-audit" "Python - pip-audit"
    check_tool_status "safety" "Python - safety"
    check_tool_status "bandit" "Python - bandit"
    check_tool_status "govulncheck" "Go - govulncheck"
    
    echo ""
    echo "🔍 Universal Scanners:"
    check_tool_status "grype" "Grype (universal vulnerability scanner)"
    check_tool_status "dependency-check" "OWASP Dependency Check"
    
    echo ""
    print_step "Installing missing tools..."
    
    # 1. Rust - cargo-audit
    if command_exists cargo; then
        if ! cargo audit --version >/dev/null 2>&1; then
            print_step "Installing cargo-audit..."
            if cargo install cargo-audit; then
                print_success "cargo-audit installed"
            else
                print_warning "Failed to install cargo-audit"
            fi
        fi
    else
        print_info "Rust not found - skipping cargo-audit"
    fi
    
    # 2. Node.js/JavaScript - npm (informational only)
    if ! command_exists npm; then
        print_info "npm not found. Install Node.js for JavaScript/TypeScript scanning:"
        echo "  • macOS: brew install node"
        echo "  • Ubuntu/Debian: sudo apt install nodejs npm"
        echo "  • Download: https://nodejs.org/"
    fi
    
    # 3. Python tools
    if command_exists python3 || command_exists python; then
        # Install pip-audit
        if ! command_exists pip-audit; then
            print_step "Installing pip-audit..."
            if command_exists pipx; then
                pipx install pip-audit >/dev/null 2>&1 && print_success "pip-audit installed via pipx"
            elif command_exists pip3; then
                pip3 install --user pip-audit >/dev/null 2>&1 && print_success "pip-audit installed via pip3"
            elif command_exists pip; then
                pip install --user pip-audit >/dev/null 2>&1 && print_success "pip-audit installed via pip"
            else
                print_warning "Could not install pip-audit - no pip found"
            fi
        fi
        
        # Install safety (alternative Python scanner)
        if ! command_exists safety; then
            print_step "Installing safety (Python vulnerability scanner)..."
            if command_exists pipx; then
                pipx install safety >/dev/null 2>&1 && print_success "safety installed via pipx"
            elif command_exists pip3; then
                pip3 install --user safety >/dev/null 2>&1 && print_success "safety installed via pip3"
            elif command_exists pip; then
                pip install --user safety >/dev/null 2>&1 && print_success "safety installed via pip"
            fi
        fi
        
        # Install bandit (Python security linter)
        if ! command_exists bandit; then
            print_step "Installing bandit (Python security linter)..."
            if command_exists pipx; then
                pipx install bandit >/dev/null 2>&1 && print_success "bandit installed via pipx"
            elif command_exists pip3; then
                pip3 install --user bandit >/dev/null 2>&1 && print_success "bandit installed via pip3"
            elif command_exists pip; then
                pip install --user bandit >/dev/null 2>&1 && print_success "bandit installed via pip"
            fi
        fi
    else
        print_info "Python not found - skipping Python security tools"
    fi
    
    # 4. Go - govulncheck
    if command_exists go; then
        if ! command_exists govulncheck && ! test -f "$HOME/go/bin/govulncheck"; then
            print_step "Installing govulncheck..."
            if go install golang.org/x/vuln/cmd/govulncheck@latest; then
                print_success "govulncheck installed"
                print_info "Added to ~/go/bin/govulncheck"
            else
                print_warning "Failed to install govulncheck"
            fi
        fi
    else
        print_info "Go not found - skipping govulncheck"
    fi
    
    # 5. Universal scanners
    # Install grype
    if ! command_exists grype && ! test -f "$HOME/.local/bin/grype"; then
        case "$(uname -s)" in
            Darwin)  # macOS
                if command_exists brew; then
                    print_step "Installing grype via Homebrew..."
                    if brew install anchore/grype/grype; then
                        print_success "grype installed via Homebrew"
                    else
                        install_grype_manually
                    fi
                else
                    install_grype_manually
                fi
                ;;
            Linux)
                install_grype_manually
                ;;
            *)
                print_warning "Platform not supported for automatic grype installation"
                ;;
        esac
    fi
    
    # Install OWASP Dependency Check (optional - heavy tool)
    if [ "${INSTALL_OWASP_DC:-}" = "true" ] && ! command_exists dependency-check && ! test -f "$HOME/.local/bin/dependency-check"; then
        install_dependency_check
    fi
}

# Function to show usage
show_usage() {
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "Options:"
    echo "  --check-only    Only check tool status, don't install"
    echo "  --owasp-dc      Also install OWASP Dependency Check (large download)"
    echo "  --help          Show this help message"
    echo ""
    echo "This script installs vulnerability scanning tools for:"
    echo "  • Rust: cargo-audit"
    echo "  • JavaScript/TypeScript: npm audit (requires Node.js)"
    echo "  • Python: pip-audit, safety, bandit"
    echo "  • Go: govulncheck"
    echo "  • Universal: grype"
    echo "  • Java (optional): OWASP Dependency Check"
}

# Parse command line arguments
CHECK_ONLY=false
INSTALL_OWASP_DC=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --check-only)
            CHECK_ONLY=true
            shift
            ;;
        --owasp-dc)
            INSTALL_OWASP_DC=true
            shift
            ;;
        --help)
            show_usage
            exit 0
            ;;
        *)
            print_error "Unknown option: $1"
            show_usage
            exit 1
            ;;
    esac
done

# Export for use in functions
export INSTALL_OWASP_DC

# Main execution
if [ "$CHECK_ONLY" = true ]; then
    echo "🛡️  Checking Vulnerability Scanning Tools Status"
    echo "==============================================="
    echo ""
    
    # Check and display status only
    echo "📋 Language-Specific Tools:"
    check_tool_status "cargo-audit" "Rust - cargo-audit"
    check_tool_status "npm" "JavaScript/TypeScript - npm audit"
    check_tool_status "pip-audit" "Python - pip-audit"
    check_tool_status "safety" "Python - safety"
    check_tool_status "bandit" "Python - bandit"
    check_tool_status "govulncheck" "Go - govulncheck"
    
    echo ""
    echo "🔍 Universal Scanners:"
    check_tool_status "grype" "Grype (universal vulnerability scanner)"
    check_tool_status "dependency-check" "OWASP Dependency Check"
    
    echo ""
    print_info "Run without --check-only to install missing tools"
else
    # Install tools
    install_vulnerability_tools
    
    echo ""
    echo "🎯 Installation Summary"
    echo "======================"
    
    # Final status check
    echo ""
    echo "📋 Final Tool Status:"
    check_tool_status "cargo-audit" "Rust - cargo-audit"
    check_tool_status "npm" "JavaScript/TypeScript - npm audit"
    check_tool_status "pip-audit" "Python - pip-audit"
    check_tool_status "safety" "Python - safety"
    check_tool_status "bandit" "Python - bandit"
    check_tool_status "govulncheck" "Go - govulncheck"
    check_tool_status "grype" "Grype (universal scanner)"
    check_tool_status "dependency-check" "OWASP Dependency Check"
    
    # PATH recommendations
    echo ""
    print_info "PATH Configuration:"
    if [ -d "$HOME/.local/bin" ]; then
        echo "  • Add ~/.local/bin to your PATH for locally installed tools"
    fi
    if [ -d "$HOME/go/bin" ]; then
        echo "  • Add ~/go/bin to your PATH for Go tools"
    fi
    
    echo ""
    echo "Add to your shell profile (~/.bashrc, ~/.zshrc, etc.):"
    echo '  export PATH="$HOME/.local/bin:$HOME/go/bin:$PATH"'
    
    echo ""
    print_success "Vulnerability scanning tools setup complete!"
fi 


================================================
FILE: src/cli.rs
================================================
use clap::{Parser, Subcommand, ValueEnum};
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "sync-ctl")]
#[command(version = env!("CARGO_PKG_VERSION"))]
#[command(about = "Generate Infrastructure as Code from your codebase")]
#[command(long_about = "A powerful CLI tool that analyzes your codebase and automatically generates optimized Infrastructure as Code configurations including Dockerfiles, Docker Compose files, and Terraform configurations.")]
pub struct Cli {
    #[command(subcommand)]
    pub command: Commands,

    /// Path to configuration file
    #[arg(short, long, global = true, value_name = "FILE")]
    pub config: Option<PathBuf>,

    /// Enable verbose logging (-v for info, -vv for debug, -vvv for trace)
    #[arg(short, long, global = true, action = clap::ArgAction::Count)]
    pub verbose: u8,

    /// Suppress all output except errors
    #[arg(short, long, global = true)]
    pub quiet: bool,

    /// Output in JSON format where applicable
    #[arg(long, global = true)]
    pub json: bool,

    /// Clear the update check cache and force a new check
    #[arg(long, global = true)]
    pub clear_update_cache: bool,
}

#[derive(Subcommand)]
pub enum Commands {
    /// Analyze a project and display detected components
    Analyze {
        /// Path to the project directory to analyze
        #[arg(value_name = "PROJECT_PATH")]
        path: PathBuf,

        /// Output analysis results in JSON format
        #[arg(short, long)]
        json: bool,

        /// Show detailed analysis information (legacy format)
        #[arg(short, long, conflicts_with = "display")]
        detailed: bool,

        /// Display format for analysis results
        #[arg(long, value_enum, default_value = "matrix")]
        display: Option<DisplayFormat>,

        /// Only analyze specific aspects (languages, frameworks, dependencies)
        #[arg(long, value_delimiter = ',')]
        only: Option<Vec<String>>,
    },

    /// Generate IaC files for a project
    Generate {
        /// Path to the project directory to analyze
        #[arg(value_name = "PROJECT_PATH")]
        path: PathBuf,

        /// Output directory for generated files
        #[arg(short, long, value_name = "OUTPUT_DIR")]
        output: Option<PathBuf>,

        /// Generate Dockerfile
        #[arg(long)]
        dockerfile: bool,

        /// Generate Docker Compose file
        #[arg(long)]
        compose: bool,

        /// Generate Terraform configuration
        #[arg(long)]
        terraform: bool,

        /// Generate all supported IaC files
        #[arg(long, conflicts_with_all = ["dockerfile", "compose", "terraform"])]
        all: bool,

        /// Perform a dry run without creating files
        #[arg(long)]
        dry_run: bool,

        /// Overwrite existing files
        #[arg(long)]
        force: bool,
    },

    /// Validate existing IaC files against best practices
    Validate {
        /// Path to the directory containing IaC files
        #[arg(value_name = "PATH")]
        path: PathBuf,

        /// Types of files to validate
        #[arg(long, value_delimiter = ',')]
        types: Option<Vec<String>>,

        /// Fix issues automatically where possible
        #[arg(long)]
        fix: bool,
    },

    /// Show supported languages and frameworks
    Support {
        /// Show only languages
        #[arg(long)]
        languages: bool,

        /// Show only frameworks
        #[arg(long)]
        frameworks: bool,

        /// Show detailed information
        #[arg(short, long)]
        detailed: bool,
    },

    /// Analyze project dependencies in detail
    Dependencies {
        /// Path to the project directory to analyze
        #[arg(value_name = "PROJECT_PATH")]
        path: PathBuf,

        /// Show license information for dependencies
        #[arg(long)]
        licenses: bool,

        /// Check for known vulnerabilities
        #[arg(long)]
        vulnerabilities: bool,

        /// Show only production dependencies
        #[arg(long, conflicts_with = "dev_only")]
        prod_only: bool,

        /// Show only development dependencies
        #[arg(long, conflicts_with = "prod_only")]
        dev_only: bool,

        /// Output format
        #[arg(long, value_enum, default_value = "table")]
        format: OutputFormat,
    },

    /// Check dependencies for known vulnerabilities
    Vulnerabilities {
        /// Check vulnerabilities in a specific path
        #[arg(default_value = ".")]
        path: PathBuf,

        /// Show only vulnerabilities with severity >= threshold
        #[arg(long, value_enum)]
        severity: Option<SeverityThreshold>,

        /// Output format
        #[arg(long, value_enum, default_value = "table")]
        format: OutputFormat,

        /// Export report to file
        #[arg(long)]
        output: Option<PathBuf>,
    },

    /// Perform comprehensive security analysis
    Security {
        /// Path to the project directory to analyze
        #[arg(value_name = "PROJECT_PATH", default_value = ".")]
        path: PathBuf,

        /// Security scan mode (lightning, fast, balanced, thorough, paranoid)
        #[arg(long, value_enum, default_value = "thorough")]
        mode: SecurityScanMode,

        /// Include low severity findings
        #[arg(long)]
        include_low: bool,

        /// Skip secrets detection
        #[arg(long)]
        no_secrets: bool,

        /// Skip code pattern analysis
        #[arg(long)]
        no_code_patterns: bool,

        /// Skip infrastructure analysis (not implemented yet)
        #[arg(long, hide = true)]
        no_infrastructure: bool,

        /// Skip compliance checks (not implemented yet)
        #[arg(long, hide = true)]
        no_compliance: bool,

        /// Compliance frameworks to check (not implemented yet)
        #[arg(long, value_delimiter = ',', hide = true)]
        frameworks: Vec<String>,

        /// Output format
        #[arg(long, value_enum, default_value = "table")]
        format: OutputFormat,

        /// Export report to file
        #[arg(long)]
        output: Option<PathBuf>,

        /// Exit with error code on security findings
        #[arg(long)]
        fail_on_findings: bool,
    },

    /// Manage vulnerability scanning tools
    Tools {
        #[command(subcommand)]
        command: ToolsCommand,
    },
}

#[derive(Subcommand)]
pub enum ToolsCommand {
    /// Check which vulnerability scanning tools are installed
    Status {
        /// Output format
        #[arg(long, value_enum, default_value = "table")]
        format: OutputFormat,

        /// Check tools for specific languages only
        #[arg(long, value_delimiter = ',')]
        languages: Option<Vec<String>>,
    },

    /// Install missing vulnerability scanning tools
    Install {
        /// Install tools for specific languages only
        #[arg(long, value_delimiter = ',')]
        languages: Option<Vec<String>>,

        /// Also install OWASP Dependency Check (large download)
        #[arg(long)]
        include_owasp: bool,

        /// Perform a dry run to show what would be installed
        #[arg(long)]
        dry_run: bool,

        /// Skip confirmation prompts
        #[arg(short, long)]
        yes: bool,
    },

    /// Verify that installed tools are working correctly
    Verify {
        /// Test tools for specific languages only
        #[arg(long, value_delimiter = ',')]
        languages: Option<Vec<String>>,

        /// Show detailed verification output
        #[arg(short, long)]
        verbose: bool,
    },

    /// Show tool installation guides for manual setup
    Guide {
        /// Show guide for specific languages only
        #[arg(long, value_delimiter = ',')]
        languages: Option<Vec<String>>,

        /// Show platform-specific instructions
        #[arg(long)]
        platform: Option<String>,
    },
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, ValueEnum)]
pub enum OutputFormat {
    Table,
    Json,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, ValueEnum)]
pub enum DisplayFormat {
    /// Compact matrix/dashboard view (modern, easy to scan)
    Matrix,
    /// Detailed vertical view (legacy format with all details)
    Detailed,  
    /// Brief summary only
    Summary,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, ValueEnum)]
pub enum SeverityThreshold {
    Low,
    Medium,
    High,
    Critical,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, ValueEnum)]
pub enum SecurityScanMode {
    /// Lightning fast scan - critical files only (.env, configs)
    Lightning,
    /// Fast scan - smart sampling with priority patterns
    Fast,
    /// Balanced scan - good coverage with performance optimizations (recommended)
    Balanced,
    /// Thorough scan - comprehensive analysis of all files
    Thorough,
    /// Paranoid scan - most comprehensive including low-severity findings
    Paranoid,
}

impl Cli {
    /// Initialize logging based on verbosity level
    pub fn init_logging(&self) {
        if self.quiet {
            return;
        }

        let level = match self.verbose {
            0 => log::LevelFilter::Warn,
            1 => log::LevelFilter::Info,
            2 => log::LevelFilter::Debug,
            _ => log::LevelFilter::Trace,
        };

        env_logger::Builder::from_default_env()
            .filter_level(level)
            .init();
    }
} 


================================================
FILE: src/error.rs
================================================
use std::path::PathBuf;
use thiserror::Error;

#[derive(Error, Debug)]
pub enum IaCGeneratorError {
    #[error("Project analysis failed: {0}")]
    Analysis(#[from] AnalysisError),

    #[error("IaC generation failed: {0}")]
    Generation(#[from] GeneratorError),

    #[error("Configuration error: {0}")]
    Config(#[from] ConfigError),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("Walk directory error: {0}")]
    WalkDir(#[from] walkdir::Error),

    #[error("JSON serialization error: {0}")]
    Json(#[from] serde_json::Error),

    #[error("Security error: {0}")]
    Security(#[from] SecurityError),
}

#[derive(Error, Debug)]
pub enum AnalysisError {
    #[error("Unsupported project type: {0}")]
    UnsupportedProject(String),

    #[error("Failed to detect language in {path}")]
    LanguageDetection { path: PathBuf },

    #[error("Dependency parsing failed for {file}: {reason}")]
    DependencyParsing { file: String, reason: String },

    #[error("Framework detection failed: {0}")]
    FrameworkDetection(String),

    #[error("Invalid project structure: {0}")]
    InvalidStructure(String),
}

#[derive(Error, Debug)]
pub enum GeneratorError {
    #[error("Template rendering failed: {0}")]
    TemplateRendering(String),

    #[error("Unsupported generator type: {0}")]
    UnsupportedGenerator(String),

    #[error("Output file creation failed: {path}")]
    OutputCreation { path: PathBuf },

    #[error("Invalid generation context: {0}")]
    InvalidContext(String),
}

#[derive(Error, Debug)]
pub enum ConfigError {
    #[error("Invalid configuration file: {0}")]
    InvalidFile(String),

    #[error("Missing required configuration: {0}")]
    MissingConfig(String),

    #[error("Configuration parsing failed: {0}")]
    ParsingFailed(String),
}

#[derive(Error, Debug)]
pub enum SecurityError {
    #[error("Invalid path: path traversal detected")]
    PathTraversal,

    #[error("Invalid path: {0}")]
    InvalidPath(String),

    #[error("Insufficient permissions: {0}")]
    InsufficientPermissions(String),
}

pub type Result<T> = std::result::Result<T, IaCGeneratorError>; 


================================================
FILE: src/lib.rs
================================================
//! # Syncable IaC CLI
//!
//! A Rust-based command-line application that analyzes code repositories and automatically
//! generates Infrastructure as Code configurations including Dockerfiles, Docker Compose
//! files, and Terraform configurations.
//!
//! ## Features
//!
//! - **Language Detection**: Automatically detects programming languages and their versions
//! - **Framework Analysis**: Identifies frameworks and libraries used in the project
//! - **Smart Generation**: Creates optimized IaC configurations based on project analysis
//! - **Multiple Formats**: Supports Docker, Docker Compose, and Terraform generation
//! - **Security-First**: Generates secure configurations following best practices
//!
//! ## Example
//!
//! ```rust,no_run
//! use syncable_cli::{analyze_project, generate_dockerfile};
//! use std::path::Path;
//!
//! # fn main() -> Result<(), Box<dyn std::error::Error>> {
//! let project_path = Path::new("./my-project");
//! let analysis = analyze_project(project_path)?;
//! let dockerfile = generate_dockerfile(&analysis)?;
//! println!("{}", dockerfile);
//! # Ok(())
//! # }
//! ```

pub mod analyzer;
pub mod cli;
pub mod common;
pub mod config;
pub mod error;
pub mod generator;

// Re-export commonly used types and functions
pub use analyzer::{analyze_project, ProjectAnalysis};
pub use error::{IaCGeneratorError, Result};
pub use generator::{generate_dockerfile, generate_compose, generate_terraform};

/// The current version of the CLI tool
pub const VERSION: &str = env!("CARGO_PKG_VERSION"); 


================================================
FILE: src/main.rs
================================================
use clap::Parser;
use syncable_cli::{
    analyzer::{
        self, vulnerability_checker::VulnerabilitySeverity, DetectedTechnology, TechnologyCategory, LibraryType, 
        analyze_monorepo, ProjectCategory,
        // Import new modular security types
        security::{TurboSecurityAnalyzer, TurboConfig, ScanMode},
    },
    cli::{Cli, Commands, ToolsCommand, OutputFormat, SeverityThreshold, DisplayFormat, SecurityScanMode},
    config,
    generator,
};

// Use alias for the turbo SecuritySeverity to avoid conflicts
use syncable_cli::analyzer::security::SecuritySeverity as TurboSecuritySeverity;
use syncable_cli::analyzer::display::{display_analysis, DisplayMode, BoxDrawer};
use std::process;
use std::collections::HashMap;
use std::fs;
use std::path::PathBuf;
use std::time::{SystemTime, Duration};
use dirs::cache_dir;

#[tokio::main]
async fn main() {
    if let Err(e) = run().await {
        eprintln!("Error: {}", e);
        process::exit(1);
    }
}

async fn run() -> syncable_cli::Result<()> {
    let cli = Cli::parse();
    
    // Handle update cache clearing
    if cli.clear_update_cache {
        clear_update_cache();
        println!("✅ Update cache cleared. Checking for updates now...");
    }
    
    check_for_update().await;
    
    // Initialize logging
    cli.init_logging();
    
    // Load configuration
    let _config = match config::load_config(cli.config.as_deref()) {
        Ok(config) => config,
        Err(e) => {
            eprintln!("Failed to load configuration: {}", e);
            process::exit(1);
        }
    };
    
    // Execute command
    let result = match cli.command {
        Commands::Analyze { path, json, detailed, display, only } => {
            handle_analyze(path, json, detailed, display, only)
        }
        Commands::Generate { 
            path, 
            output, 
            dockerfile, 
            compose, 
            terraform, 
            all,
            dry_run,
            force 
        } => {
            handle_generate(path, output, dockerfile, compose, terraform, all, dry_run, force)
        }
        Commands::Validate { path, types, fix } => {
            handle_validate(path, types, fix)
        }
        Commands::Support { languages, frameworks, detailed } => {
            handle_support(languages, frameworks, detailed)
        }
        Commands::Dependencies { path, licenses, vulnerabilities, prod_only, dev_only, format } => {
            handle_dependencies(path, licenses, vulnerabilities, prod_only, dev_only, format).await
        }
        Commands::Vulnerabilities { path, severity, format, output } => {
            handle_vulnerabilities(path, severity, format, output).await
        }
        Commands::Security { 
            path, 
            mode,
            include_low, 
            no_secrets, 
            no_code_patterns, 
            no_infrastructure, 
            no_compliance, 
            frameworks, 
            format, 
            output, 
            fail_on_findings 
        } => {
            handle_security(
                path, 
                mode,
                include_low, 
                no_secrets, 
                no_code_patterns, 
                no_infrastructure, 
                no_compliance, 
                frameworks, 
                format, 
                output, 
                fail_on_findings
            )
        }
        Commands::Tools { command } => {
            handle_tools(command).await
        }
    };
    
    if let Err(e) = result {
        eprintln!("Error: {}", e);
        process::exit(1);
    }
    
    Ok(())
}

fn clear_update_cache() {
    let cache_dir_path = cache_dir()
        .unwrap_or_else(|| PathBuf::from("."))
        .join("syncable-cli");
    let cache_file = cache_dir_path.join("version_cache.json");
    
    if cache_file.exists() {
        match fs::remove_file(&cache_file) {
            Ok(_) => {
                if std::env::var("SYNC_CTL_DEBUG").is_ok() {
                    eprintln!("🗑️  Removed update cache file: {}", cache_file.display());
                }
            }
            Err(e) => {
                eprintln!("⚠️  Failed to remove update cache: {}", e);
            }
        }
    } else {
        if std::env::var("SYNC_CTL_DEBUG").is_ok() {
            eprintln!("🗑️  No update cache file found at: {}", cache_file.display());
        }
    }
}

async fn check_for_update() {
    let cache_dir_path = cache_dir()
        .unwrap_or_else(|| PathBuf::from("."))
        .join("syncable-cli");
    let cache_file = cache_dir_path.join("version_cache.json");
    let now = SystemTime::now();

    // Smart cache system: only cache when no update is available
    // Check every 2 hours when no update was found, immediately when an update might be available
    let should_check = if let Ok(metadata) = fs::metadata(&cache_file) {
        if let Ok(modified) = metadata.modified() {
            let cache_duration = now.duration_since(modified).unwrap_or(Duration::ZERO);
            
            // Read cached data to determine cache strategy
            if let Ok(cache_content) = fs::read_to_string(&cache_file) {
                if let Ok(cache_data) = serde_json::from_str::<serde_json::Value>(&cache_content) {
                    let cached_latest = cache_data["latest_version"].as_str().unwrap_or("");
                    let current = env!("CARGO_PKG_VERSION");
                    
                    // If cached version is newer than current, check immediately
                    if !cached_latest.is_empty() && is_version_newer(current, cached_latest) {
                        if std::env::var("SYNC_CTL_DEBUG").is_ok() {
                            eprintln!("🔍 Update available in cache, showing immediately");
                        }
                        show_update_notification(current, cached_latest);
                        return;
                    }
                    
                    // If no update in cache, check every 2 hours
                    cache_duration >= Duration::from_secs(60 * 60 * 2)
                } else {
                    true // Invalid cache, check now
                }
            } else {
                true // Can't read cache, check now
            }
        } else {
            true // Can't get modified time, check now
        }
    } else {
        true // No cache file, check now
    };

    if !should_check {
        if std::env::var("SYNC_CTL_DEBUG").is_ok() {
            eprintln!("🔍 Update check skipped - checked recently and no update available");
        }
        return;
    }

    // Debug logging
    if std::env::var("SYNC_CTL_DEBUG").is_ok() {
        eprintln!("🔍 Checking for updates...");
    }

    // Query GitHub releases API
    let client = reqwest::Client::builder()
        .user_agent(format!("syncable-cli/{}", env!("CARGO_PKG_VERSION")))
        .timeout(std::time::Duration::from_secs(5))
        .build();
    
    match client {
        Ok(client) => {
            let result = client
                .get("https://api.github.com/repos/syncable-dev/syncable-cli/releases/latest")
                .send()
                .await;
                
            match result {
                Ok(response) => {
                    if !response.status().is_success() {
                        if std::env::var("SYNC_CTL_DEBUG").is_ok() {
                            eprintln!("⚠️  GitHub API returned status: {}", response.status());
                        }
                        return;
                    }
                    
                    match response.json::<serde_json::Value>().await {
                        Ok(json) => {
                            let latest = json["tag_name"].as_str().unwrap_or("")
                                .trim_start_matches('v'); // Remove 'v' prefix if present
                            let current = env!("CARGO_PKG_VERSION");
                            
                            if std::env::var("SYNC_CTL_DEBUG").is_ok() {
                                eprintln!("📦 Current version: {}, Latest version: {}", current, latest);
                            }
                            
                            // Update cache with latest version info
                            let cache_data = serde_json::json!({
                                "latest_version": latest,
                                "current_version": current,
                                "checked_at": now.duration_since(SystemTime::UNIX_EPOCH).unwrap().as_secs(),
                                "update_available": is_version_newer(current, latest)
                            });
                            
                            let _ = fs::create_dir_all(&cache_dir_path);
                            let _ = fs::write(&cache_file, serde_json::to_string_pretty(&cache_data).unwrap_or_default());
                            
                            // Show update notification if newer version is available
                            if !latest.is_empty() && latest != current && is_version_newer(current, latest) {
                                show_update_notification(current, latest);
                            }
                        }
                        Err(e) => {
                            if std::env::var("SYNC_CTL_DEBUG").is_ok() {
                                eprintln!("⚠️  Failed to parse GitHub API response: {}", e);
                            }
                        }
                    }
                }
                Err(e) => {
                    if std::env::var("SYNC_CTL_DEBUG").is_ok() {
                        eprintln!("⚠️  Failed to check for updates: {}", e);
                    }
                }
            }
        }
        Err(e) => {
            if std::env::var("SYNC_CTL_DEBUG").is_ok() {
                eprintln!("⚠️  Failed to create HTTP client: {}", e);
            }
        }
    }
}

fn show_update_notification(current: &str, latest: &str) {
    use colored::*;
    
    let mut box_drawer = BoxDrawer::new(&"UPDATE AVAILABLE".bright_red().bold().to_string());
    
    // Version info line with prominent colors
    let version_info = format!("New version: {} | Current: {}", 
                              latest.bright_green().bold(), 
                              current.bright_red());
    box_drawer.add_value_only(&version_info);
    
    // Empty line for spacing
    box_drawer.add_value_only("");
    
    // Instructions header with emphasis
    box_drawer.add_value_only(&"To update, run one of these commands:".bright_cyan().bold().to_string());
    box_drawer.add_value_only("");
    
    // Recommended method - highlighted as primary option
    box_drawer.add_line(&"RECOMMENDED".bright_green().bold().to_string(), &"(via Cargo)".green().to_string(), false);
    let cargo_cmd = "cargo install syncable-cli".bright_white().on_blue().bold().to_string();
    box_drawer.add_value_only(&format!("  {}", cargo_cmd));
    box_drawer.add_value_only("");
    
    // Alternative method - neutral coloring
    box_drawer.add_line(&"ALTERNATIVE".yellow().bold().to_string(), &"(direct download)".yellow().to_string(), false);
    let github_url = format!("  Visit: {}", 
                            format!("github.com/syncable-dev/syncable-cli/releases/v{}", latest).bright_blue().underline());
    box_drawer.add_value_only(&github_url);
    box_drawer.add_value_only("");
    
    // Install script method - secondary option
    box_drawer.add_line(&"SCRIPT".magenta().bold().to_string(), &"(automated installer)".magenta().to_string(), false);
    let script_cmd = "curl -sSL install.syncable.dev | sh".bright_white().on_magenta().bold().to_string();
    box_drawer.add_value_only(&format!("  {}", script_cmd));
    
    // Add a helpful note
    box_drawer.add_value_only("");
    box_drawer.add_value_only(&"Tip: The Cargo method is fastest for existing Rust users".dimmed().italic().to_string());
    
    println!("\n{}", box_drawer.draw());
}

// Helper function to compare semantic versions
fn is_version_newer(current: &str, latest: &str) -> bool {
    let current_parts: Vec<u32> = current.split('.')
        .filter_map(|s| s.parse().ok())
        .collect();
    let latest_parts: Vec<u32> = latest.split('.')
        .filter_map(|s| s.parse().ok())
        .collect();
    
    for i in 0..3 {
        let current_part = current_parts.get(i).unwrap_or(&0);
        let latest_part = latest_parts.get(i).unwrap_or(&0);
        
        if latest_part > current_part {
            return true;
        } else if latest_part < current_part {
            return false;
        }
    }
    
    false
}

fn handle_analyze(
    path: std::path::PathBuf,
    json: bool,
    detailed: bool,
    display: Option<DisplayFormat>,
    _only: Option<Vec<String>>,
) -> syncable_cli::Result<()> {
    println!("🔍 Analyzing project: {}", path.display());
    
    let monorepo_analysis = analyze_monorepo(&path)?;
    
    if json {
        display_analysis(&monorepo_analysis, DisplayMode::Json);
    } else {
        // Determine display mode
        let mode = if detailed {
            // Legacy flag for backward compatibility
            DisplayMode::Detailed
        } else {
            match display {
                Some(DisplayFormat::Matrix) | None => DisplayMode::Matrix,
                Some(DisplayFormat::Detailed) => DisplayMode::Detailed,
                Some(DisplayFormat::Summary) => DisplayMode::Summary,
            }
        };
        
        display_analysis(&monorepo_analysis, mode);
    }
    
    Ok(())
}

fn handle_generate(
    path: std::path::PathBuf,
    _output: Option<std::path::PathBuf>,
    dockerfile: bool,
    compose: bool,
    terraform: bool,
    all: bool,
    dry_run: bool,
    _force: bool,
) -> syncable_cli::Result<()> {
    println!("🔍 Analyzing project for generation: {}", path.display());
    
    let monorepo_analysis = analyze_monorepo(&path)?;
    
    println!("✅ Analysis complete. Generating IaC files...");
    
    if monorepo_analysis.is_monorepo {
        println!("📦 Detected monorepo with {} projects", monorepo_analysis.projects.len());
        println!("🚧 Monorepo IaC generation is coming soon! For now, generating for the overall structure.");
        println!("💡 Tip: You can run generate commands on individual project directories for now.");
    }
    
    // For now, use the first/main project for generation
    // TODO: Implement proper monorepo IaC generation
    let main_project = &monorepo_analysis.projects[0];
    
    let generate_all = all || (!dockerfile && !compose && !terraform);
    
    if generate_all || dockerfile {
        println!("\n🐳 Generating Dockerfile...");
        let dockerfile_content = generator::generate_dockerfile(&main_project.analysis)?;
        
        if dry_run {
            println!("--- Dockerfile (dry run) ---");
            println!("{}", dockerfile_content);
        } else {
            std::fs::write("Dockerfile", dockerfile_content)?;
            println!("✅ Dockerfile generated successfully!");
        }
    }
    
    if generate_all || compose {
        println!("\n🐙 Generating Docker Compose file...");
        let compose_content = generator::generate_compose(&main_project.analysis)?;
        
        if dry_run {
            println!("--- docker-compose.yml (dry run) ---");
            println!("{}", compose_content);
        } else {
            std::fs::write("docker-compose.yml", compose_content)?;
            println!("✅ Docker Compose file generated successfully!");
        }
    }
    
    if generate_all || terraform {
        println!("\n🏗️  Generating Terraform configuration...");
        let terraform_content = generator::generate_terraform(&main_project.analysis)?;
        
        if dry_run {
            println!("--- main.tf (dry run) ---");
            println!("{}", terraform_content);
        } else {
            std::fs::write("main.tf", terraform_content)?;
            println!("✅ Terraform configuration generated successfully!");
        }
    }
    
    if !dry_run {
        println!("\n🎉 Generation complete! IaC files have been created in the current directory.");
        
        if monorepo_analysis.is_monorepo {
            println!("🔧 Note: Generated files are based on the main project structure.");
            println!("   Advanced monorepo support with per-project generation is coming soon!");
        }
    }
    
    Ok(())
}

fn handle_validate(
    _path: std::path::PathBuf,
    _types: Option<Vec<String>>,
    _fix: bool,
) -> syncable_cli::Result<()> {
    println!("🔍 Validating IaC files...");
    println!("⚠️  Validation feature is not yet implemented.");
    Ok(())
}

fn handle_support(
    languages: bool,
    frameworks: bool,
    _detailed: bool,
) -> syncable_cli::Result<()> {
    if languages || (!languages && !frameworks) {
        println!("🌐 Supported Languages:");
        println!("├── Rust");
        println!("├── JavaScript/TypeScript");
        println!("├── Python");
        println!("├── Go");
        println!("├── Java");
        println!("└── (More coming soon...)");
    }
    
    if frameworks || (!languages && !frameworks) {
        println!("\n🚀 Supported Frameworks:");
        println!("├── Web: Express.js, Next.js, React, Vue.js, Actix Web");
        println!("├── Database: PostgreSQL, MySQL, MongoDB, Redis");
        println!("├── Build Tools: npm, yarn, cargo, maven, gradle");
        println!("└── (More coming soon...)");
    }
    
    Ok(())
}

async fn handle_dependencies(
    path: std::path::PathBuf,
    licenses: bool,
    vulnerabilities: bool,
    _prod_only: bool,
    _dev_only: bool,
    format: OutputFormat,
) -> syncable_cli::Result<()> {
    let project_path = path.canonicalize()
        .unwrap_or_else(|_| path.clone());
    
    println!("🔍 Analyzing dependencies: {}", project_path.display());
    
    // First, analyze the project using monorepo analysis
    let monorepo_analysis = analyze_monorepo(&project_path)?;
    
    // Collect all languages from all projects
    let mut all_languages = Vec::new();
    for project in &monorepo_analysis.projects {
        all_languages.extend(project.analysis.languages.clone());
    }
    
    // Then perform detailed dependency analysis using the collected languages
    let dep_analysis = analyzer::dependency_parser::parse_detailed_dependencies(
        &project_path,
        &all_languages,
        &analyzer::AnalysisConfig::default(),
    ).await?;
    
    if format == OutputFormat::Table {
        // Table output
        use termcolor::{ColorChoice, StandardStream, WriteColor, ColorSpec, Color};
        
        let mut stdout = StandardStream::stdout(ColorChoice::Always);
        
        // Print summary
        println!("\n📦 Dependency Analysis Report");
        println!("{}", "=".repeat(80));
        
        let total_deps: usize = dep_analysis.dependencies.len();
        println!("Total dependencies: {}", total_deps);
        
        if monorepo_analysis.is_monorepo {
            println!("Projects analyzed: {}", monorepo_analysis.projects.len());
            for project in &monorepo_analysis.projects {
                println!("  • {} ({})", project.name, format_project_category(&project.project_category));
            }
        }
        
        for (name, info) in &dep_analysis.dependencies {
            print!("  {} v{}", name, info.version);
            
            // Color code by type
            stdout.set_color(ColorSpec::new().set_fg(Some(
                if info.is_dev { Color::Yellow } else { Color::Green }
            )))?;
            
            print!(" [{}]", if info.is_dev { "dev" } else { "prod" });
            
            stdout.reset()?;
            
            if licenses && info.license.is_some() {
                print!(" - License: {}", info.license.as_ref().unwrap_or(&"Unknown".to_string()));
            }
            
            println!();
        }
        
        if licenses {
            // License summary
            println!("\n📋 License Summary");
            println!("{}", "-".repeat(80));
            
            use std::collections::HashMap;
            let mut license_counts: HashMap<String, usize> = HashMap::new();
            
            for (_name, info) in &dep_analysis.dependencies {
                if let Some(license) = &info.license {
                    *license_counts.entry(license.clone()).or_insert(0) += 1;
                }
            }
            
            let mut licenses: Vec<_> = license_counts.into_iter().collect();
            licenses.sort_by(|a, b| b.1.cmp(&a.1));
            
            for (license, count) in licenses {
                println!("  {}: {} packages", license, count);
            }
        }
        
        if vulnerabilities {
            println!("\n🔍 Checking for vulnerabilities...");
            
            // Convert DetailedDependencyMap to the format expected by VulnerabilityChecker
            let mut deps_by_language: HashMap<analyzer::dependency_parser::Language, Vec<analyzer::dependency_parser::DependencyInfo>> = HashMap::new();
            
            // Group dependencies by detected languages
            for language in &all_languages {
                let mut lang_deps = Vec::new();
                
                // Filter dependencies that belong to this language
                for (name, info) in &dep_analysis.dependencies {
                    // Simple heuristic to determine language based on source
                    let matches_language = match language.name.as_str() {
                        "Rust" => info.source == "crates.io",
                        "JavaScript" | "TypeScript" => info.source == "npm",
                        "Python" => info.source == "pypi",
                        "Go" => info.source == "go modules",
                        "Java" | "Kotlin" => info.source == "maven" || info.source == "gradle",
                        _ => false,
                    };
                    
                    if matches_language {
                        // Convert to new DependencyInfo format expected by vulnerability checker
                        lang_deps.push(analyzer::dependency_parser::DependencyInfo {
                            name: name.clone(),
                            version: info.version.clone(),
                            dep_type: if info.is_dev { 
                                analyzer::dependency_parser::DependencyType::Dev 
                            } else { 
                                analyzer::dependency_parser::DependencyType::Production 
                            },
                            license: info.license.clone().unwrap_or_default(),
                            source: Some(info.source.clone()),
                            language: match language.name.as_str() {
                                "Rust" => analyzer::dependency_parser::Language::Rust,
                                "JavaScript" => analyzer::dependency_parser::Language::JavaScript,
                                "TypeScript" => analyzer::dependency_parser::Language::TypeScript,
                                "Python" => analyzer::dependency_parser::Language::Python,
                                "Go" => analyzer::dependency_parser::Language::Go,
                                "Java" => analyzer::dependency_parser::Language::Java,
                                "Kotlin" => analyzer::dependency_parser::Language::Kotlin,
                                _ => analyzer::dependency_parser::Language::Unknown,
                            },
                        });
                    }
                }
                
                if !lang_deps.is_empty() {
                    let lang_enum = match language.name.as_str() {
                        "Rust" => analyzer::dependency_parser::Language::Rust,
                        "JavaScript" => analyzer::dependency_parser::Language::JavaScript,
                        "TypeScript" => analyzer::dependency_parser::Language::TypeScript,
                        "Python" => analyzer::dependency_parser::Language::Python,
                        "Go" => analyzer::dependency_parser::Language::Go,
                        "Java" => analyzer::dependency_parser::Language::Java,
                        "Kotlin" => analyzer::dependency_parser::Language::Kotlin,
                        _ => analyzer::dependency_parser::Language::Unknown,
                    };
                    deps_by_language.insert(lang_enum, lang_deps);
                }
            }
            
            let checker = analyzer::vulnerability_checker::VulnerabilityChecker::new();
            match checker.check_all_dependencies(&deps_by_language, &project_path).await {
                Ok(report) => {
                    println!("\n🛡️ Vulnerability Report");
                    println!("{}", "-".repeat(80));
                    println!("Checked at: {}", report.checked_at.format("%Y-%m-%d %H:%M:%S UTC"));
                    println!("Total vulnerabilities: {}", report.total_vulnerabilities);
                    
                    if report.total_vulnerabilities > 0 {
                        println!("\nSeverity Breakdown:");
                        if report.critical_count > 0 {
                            stdout.set_color(ColorSpec::new().set_fg(Some(Color::Red)).set_bold(true))?;
                            println!("  CRITICAL: {}", report.critical_count);
                            stdout.reset()?;
                        }
                        if report.high_count > 0 {
                            stdout.set_color(ColorSpec::new().set_fg(Some(Color::Red)))?;
                            println!("  HIGH: {}", report.high_count);
                            stdout.reset()?;
                        }
                        if report.medium_count > 0 {
                            stdout.set_color(ColorSpec::new().set_fg(Some(Color::Yellow)))?;
                            println!("  MEDIUM: {}", report.medium_count);
                            stdout.reset()?;
                        }
                        if report.low_count > 0 {
                            stdout.set_color(ColorSpec::new().set_fg(Some(Color::Blue)))?;
                            println!("  LOW: {}", report.low_count);
                            stdout.reset()?;
                        }
                        
                        println!("\nVulnerable Dependencies:");
                        for vuln_dep in &report.vulnerable_dependencies {
                            println!("\n  📦 {} v{} ({})", 
                                vuln_dep.name, 
                                vuln_dep.version,
                                vuln_dep.language.as_str()
                            );
                            
                            for vuln in &vuln_dep.vulnerabilities {
                                print!("    ⚠️  {} ", vuln.id);
                                
                                // Color by severity
                                stdout.set_color(ColorSpec::new().set_fg(Some(
                                    match vuln.severity {
                                        VulnerabilitySeverity::Critical => Color::Red,
                                        VulnerabilitySeverity::High => Color::Red,
                                        VulnerabilitySeverity::Medium => Color::Yellow,
                                        VulnerabilitySeverity::Low => Color::Blue,
                                        VulnerabilitySeverity::Info => Color::Cyan,
                                    }
                                )).set_bold(vuln.severity == VulnerabilitySeverity::Critical))?;
                                
                                print!("[{}]", match vuln.severity {
                                    VulnerabilitySeverity::Critical => "CRITICAL",
                                    VulnerabilitySeverity::High => "HIGH",
                                    VulnerabilitySeverity::Medium => "MEDIUM",
                                    VulnerabilitySeverity::Low => "LOW",
                                    VulnerabilitySeverity::Info => "INFO",
                                });
                                
                                stdout.reset()?;
                                
                                println!(" - {}", vuln.title);
                                
                                if let Some(ref cve) = vuln.cve {
                                    println!("       CVE: {}", cve);
                                }
                                if let Some(ref patched) = vuln.patched_versions {
                                    stdout.set_color(ColorSpec::new().set_fg(Some(Color::Green)))?;
                                    println!("       Fix: Upgrade to {}", patched);
                                    stdout.reset()?;
                                }
                            }
                        }
                    } else {
                        stdout.set_color(ColorSpec::new().set_fg(Some(Color::Green)))?;
                        println!("\n✅ No known vulnerabilities found!");
                        stdout.reset()?;
                    }
                }
                Err(e) => {
                    eprintln!("Error checking vulnerabilities: {}", e);
                    process::exit(1);
                }
            }
        }
    } else if format == OutputFormat::Json {
        // JSON output
        let output = serde_json::json!({
            "dependencies": dep_analysis.dependencies,
            "total": dep_analysis.dependencies.len(),
        });
        println!("{}", serde_json::to_string_pretty(&output)?);
    }
    
    Ok(())
}

async fn handle_vulnerabilities(
    path: std::path::PathBuf,
    severity: Option<SeverityThreshold>,
    format: OutputFormat,
    output: Option<std::path::PathBuf>,
) -> syncable_cli::Result<()> {
    let project_path = path.canonicalize()
        .unwrap_or_else(|_| path.clone());
    
    println!("🔍 Scanning for vulnerabilities in: {}", project_path.display());
    
    // Parse dependencies
    let dependencies = analyzer::dependency_parser::DependencyParser::new().parse_all_dependencies(&project_path)?;
    
    if dependencies.is_empty() {
        println!("No dependencies found to check.");
        return Ok(());
    }
    
    // Check vulnerabilities
    let checker = analyzer::vulnerability_checker::VulnerabilityChecker::new();
    let report = checker.check_all_dependencies(&dependencies, &project_path).await
        .map_err(|e| syncable_cli::error::IaCGeneratorError::Analysis(
            syncable_cli::error::AnalysisError::DependencyParsing {
                file: "vulnerability check".to_string(),
                reason: e.to_string(),
            }
        ))?;
    
    // Filter by severity if requested
    let filtered_report = if let Some(threshold) = severity {
        let min_severity = match threshold {
            SeverityThreshold::Low => VulnerabilitySeverity::Low,
            SeverityThreshold::Medium => VulnerabilitySeverity::Medium,
            SeverityThreshold::High => VulnerabilitySeverity::High,
            SeverityThreshold::Critical => VulnerabilitySeverity::Critical,
        };
        
        let filtered_deps: Vec<_> = report.vulnerable_dependencies
            .into_iter()
            .filter_map(|mut dep| {
                dep.vulnerabilities.retain(|v| v.severity >= min_severity);
                if dep.vulnerabilities.is_empty() {
                    None
                } else {
                    Some(dep)
                }
            })
            .collect();
        
        use analyzer::vulnerability_checker::VulnerabilityReport;
        let mut filtered = VulnerabilityReport {
            checked_at: report.checked_at,
            total_vulnerabilities: 0,
            critical_count: 0,
            high_count: 0,
            medium_count: 0,
            low_count: 0,
            vulnerable_dependencies: filtered_deps,
        };
        
        // Recalculate counts
        for dep in &filtered.vulnerable_dependencies {
            for vuln in &dep.vulnerabilities {
                                 filtered.total_vulnerabilities += 1;
                 match vuln.severity {
                     VulnerabilitySeverity::Critical => filtered.critical_count += 1,
                     VulnerabilitySeverity::High => filtered.high_count += 1,
                     VulnerabilitySeverity::Medium => filtered.medium_count += 1,
                     VulnerabilitySeverity::Low => filtered.low_count += 1,
                     VulnerabilitySeverity::Info => {},
                 }
            }
        }
        
        filtered
    } else {
        report
    };
    
    // Format output
    let output_string = match format {
        OutputFormat::Table => {
            // Color formatting for output

            
            let mut output = String::new();
            
            output.push_str(&format!("\n🛡️  Vulnerability Scan Report\n"));
            output.push_str(&format!("{}\n", "=".repeat(80)));
            output.push_str(&format!("Scanned at: {}\n", filtered_report.checked_at.format("%Y-%m-%d %H:%M:%S UTC")));
            output.push_str(&format!("Path: {}\n", project_path.display()));
            
            if let Some(threshold) = severity {
                output.push_str(&format!("Severity filter: >= {:?}\n", threshold));
            }
            
            output.push_str(&format!("\nSummary:\n"));
            output.push_str(&format!("Total vulnerabilities: {}\n", filtered_report.total_vulnerabilities));
            
            if filtered_report.total_vulnerabilities > 0 {
                output.push_str("\nBy Severity:\n");
                if filtered_report.critical_count > 0 {
                    output.push_str(&format!("  🔴 CRITICAL: {}\n", filtered_report.critical_count));
                }
                if filtered_report.high_count > 0 {
                    output.push_str(&format!("  🔴 HIGH: {}\n", filtered_report.high_count));
                }
                if filtered_report.medium_count > 0 {
                    output.push_str(&format!("  🟡 MEDIUM: {}\n", filtered_report.medium_count));
                }
                if filtered_report.low_count > 0 {
                    output.push_str(&format!("  🔵 LOW: {}\n", filtered_report.low_count));
                }
                
                output.push_str(&format!("\n{}\n", "-".repeat(80)));
                output.push_str("Vulnerable Dependencies:\n\n");
                
                for vuln_dep in &filtered_report.vulnerable_dependencies {
                    output.push_str(&format!("📦 {} v{} ({})\n", 
                        vuln_dep.name, 
                        vuln_dep.version,
                        vuln_dep.language.as_str()
                    ));
                    
                    for vuln in &vuln_dep.vulnerabilities {
                        let severity_str = match vuln.severity {
                            VulnerabilitySeverity::Critical => "CRITICAL",
                            VulnerabilitySeverity::High => "HIGH",
                            VulnerabilitySeverity::Medium => "MEDIUM",
                            VulnerabilitySeverity::Low => "LOW",
                            VulnerabilitySeverity::Info => "INFO",
                        };
                        
                        output.push_str(&format!("\n  ⚠️  {} [{}]\n", vuln.id, severity_str));
                        output.push_str(&format!("     {}\n", vuln.title));
                        
                        if !vuln.description.is_empty() && vuln.description != vuln.title {
                            // Wrap description
                            let wrapped = textwrap::fill(&vuln.description, 70);
                            for line in wrapped.lines() {
                                output.push_str(&format!("     {}\n", line));
                            }
                        }
                        
                        if let Some(ref cve) = vuln.cve {
                            output.push_str(&format!("     CVE: {}\n", cve));
                        }
                        
                        if let Some(ref ghsa) = vuln.ghsa {
                            output.push_str(&format!("     GHSA: {}\n", ghsa));
                        }
                        
                        output.push_str(&format!("     Affected: {}\n", vuln.affected_versions));
                        
                        if let Some(ref patched) = vuln.patched_versions {
                            output.push_str(&format!("     ✅ Fix: Upgrade to {}\n", patched));
                        }
                    }
                    output.push_str("\n");
                }
            } else {
                output.push_str("\n✅ No vulnerabilities found!\n");
            }
            
            output
        }
        OutputFormat::Json => {
            serde_json::to_string_pretty(&filtered_report)?
        }
    };
    
    // Output results
    if let Some(output_path) = output {
        std::fs::write(&output_path, output_string)?;
        println!("Report saved to: {}", output_path.display());
    } else {
        println!("{}", output_string);
    }
    
    // Exit with non-zero code if critical/high vulnerabilities found
    if filtered_report.critical_count > 0 || filtered_report.high_count > 0 {
        std::process::exit(1);
    }
    
    Ok(())
}

/// Display technologies in detailed format with proper categorization
fn display_technologies_detailed(technologies: &[DetectedTechnology]) {
    if technologies.is_empty() {
        println!("\n🛠️  Technologies Detected: None");
        return;
    }

    // Group technologies by IaC-relevant categories
    let mut meta_frameworks = Vec::new();
    let mut backend_frameworks = Vec::new();
    let mut frontend_frameworks = Vec::new();
    let mut ui_libraries = Vec::new();
    let mut build_tools = Vec::new();
    let mut databases = Vec::new();
    let mut testing = Vec::new();
    let mut runtimes = Vec::new();
    let mut other_libraries = Vec::new();

    for tech in technologies {
        match &tech.category {
            TechnologyCategory::MetaFramework => meta_frameworks.push(tech),
            TechnologyCategory::BackendFramework => backend_frameworks.push(tech),
            TechnologyCategory::FrontendFramework => frontend_frameworks.push(tech),
            TechnologyCategory::Library(lib_type) => match lib_type {
                LibraryType::UI => ui_libraries.push(tech),
                _ => other_libraries.push(tech),
            },
            TechnologyCategory::BuildTool => build_tools.push(tech),
            TechnologyCategory::Database => databases.push(tech),
            TechnologyCategory::Testing => testing.push(tech),
            TechnologyCategory::Runtime => runtimes.push(tech),
            _ => other_libraries.push(tech),
        }
    }

    println!("\n🛠️  Technology Stack:");
    
    // Primary Framework (highlighted)
    if let Some(primary) = technologies.iter().find(|t| t.is_primary) {
        println!("   🎯 PRIMARY: {} (confidence: {:.1}%)", primary.name, primary.confidence * 100.0);
        println!("      Architecture driver for this project");
    }

    // Meta-frameworks
    if !meta_frameworks.is_empty() {
        println!("\n   🏗️  Meta-Frameworks:");
        for tech in meta_frameworks {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // Backend frameworks
    if !backend_frameworks.is_empty() {
        println!("\n   🖥️  Backend Frameworks:");
        for tech in backend_frameworks {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // Frontend frameworks
    if !frontend_frameworks.is_empty() {
        println!("\n   🌐 Frontend Frameworks:");
        for tech in frontend_frameworks {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // UI Libraries
    if !ui_libraries.is_empty() {
        println!("\n   🎨 UI Libraries:");
        for tech in ui_libraries {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // Note: Removed utility library categories (Data Fetching, Routing, State Management)
    // as they don't provide value for IaC generation

    // Build Tools
    if !build_tools.is_empty() {
        println!("\n   🔨 Build Tools:");
        for tech in build_tools {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // Databases
    if !databases.is_empty() {
        println!("\n   🗃️  Database & ORM:");
        for tech in databases {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // Testing
    if !testing.is_empty() {
        println!("\n   🧪 Testing:");
        for tech in testing {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // Runtimes
    if !runtimes.is_empty() {
        println!("\n   ⚡ Runtimes:");
        for tech in runtimes {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }

    // Other Libraries
    if !other_libraries.is_empty() {
        println!("\n   📚 Other Libraries:");
        for tech in other_libraries {
            println!("      • {} (confidence: {:.1}%)", tech.name, tech.confidence * 100.0);
        }
    }
}

/// Display technologies in summary format for simple view
fn display_technologies_summary(technologies: &[DetectedTechnology]) {
    println!("├── Technologies detected: {}", technologies.len());
    
    // Show primary technology first
    if let Some(primary) = technologies.iter().find(|t| t.is_primary) {
        println!("│   ├── 🎯 {} (PRIMARY, {:.1}%)", primary.name, primary.confidence * 100.0);
    }
    
    // Show other technologies
    for tech in technologies.iter().filter(|t| !t.is_primary) {
        let icon = match &tech.category {
            TechnologyCategory::MetaFramework => "🏗️",
            TechnologyCategory::BackendFramework => "🖥️",
            TechnologyCategory::FrontendFramework => "🌐",
            TechnologyCategory::Library(LibraryType::UI) => "🎨",
            TechnologyCategory::BuildTool => "🔨",
            TechnologyCategory::Database => "🗃️",
            TechnologyCategory::Testing => "🧪",
            TechnologyCategory::Runtime => "⚡",
            _ => "📚",
        };
        println!("│   ├── {} {} (confidence: {:.1}%)", icon, tech.name, tech.confidence * 100.0);
    }
}

fn handle_security(
    path: std::path::PathBuf,
    mode: SecurityScanMode,
    include_low: bool,
    no_secrets: bool,
    no_code_patterns: bool,
    no_infrastructure: bool,
    no_compliance: bool,
    frameworks: Vec<String>,
    format: OutputFormat,
    output: Option<std::path::PathBuf>,
    fail_on_findings: bool,
) -> syncable_cli::Result<()> {
    let project_path = path.canonicalize()
        .unwrap_or_else(|_| path.clone());
    
    println!("🛡️  Running security analysis on: {}", project_path.display());
    
    // Convert CLI mode to internal ScanMode, with flag overrides
    let scan_mode = if no_secrets && no_code_patterns {
        // Override: if both secrets and code patterns are disabled, use lightning
        ScanMode::Lightning
    } else if include_low {
        // Override: if including low findings, force paranoid mode
        ScanMode::Paranoid
    } else {
        // Use the requested mode from CLI
        match mode {
            SecurityScanMode::Lightning => ScanMode::Lightning,
            SecurityScanMode::Fast => ScanMode::Fast,
            SecurityScanMode::Balanced => ScanMode::Balanced,
            SecurityScanMode::Thorough => ScanMode::Thorough,
            SecurityScanMode::Paranoid => ScanMode::Paranoid,
        }
    };
    
    // Configure turbo analyzer
    let config = TurboConfig {
        scan_mode,
        max_file_size: 10 * 1024 * 1024, // 10MB
        worker_threads: 0, // Auto-detect
        use_mmap: true,
        enable_cache: true,
        cache_size_mb: 100,
        max_critical_findings: if fail_on_findings { Some(1) } else { None },
        timeout_seconds: Some(60),
        skip_gitignored: true,
        priority_extensions: vec![
            "env".to_string(), "key".to_string(), "pem".to_string(),
            "json".to_string(), "yml".to_string(), "yaml".to_string(),
            "toml".to_string(), "ini".to_string(), "conf".to_string(),
            "config".to_string(), "js".to_string(), "ts".to_string(),
            "py".to_string(), "rs".to_string(), "go".to_string(),
        ],
        pattern_sets: if no_secrets {
            vec![]
        } else {
            vec!["default".to_string(), "aws".to_string(), "gcp".to_string()]
        },
    };
    
    // Initialize and run analyzer
    let analyzer = TurboSecurityAnalyzer::new(config)
        .map_err(|e| syncable_cli::error::IaCGeneratorError::Analysis(
            syncable_cli::error::AnalysisError::InvalidStructure(
                format!("Failed to create turbo security analyzer: {}", e)
            )
        ))?;
    
    let start_time = std::time::Instant::now();
    let security_report = analyzer.analyze_project(&project_path)
        .map_err(|e| syncable_cli::error::IaCGeneratorError::Analysis(
            syncable_cli::error::AnalysisError::InvalidStructure(
                format!("Turbo security analysis failed: {}", e)
            )
        ))?;
    let scan_duration = start_time.elapsed();
    
    println!("⚡ Scan completed in {:.2}s", scan_duration.as_secs_f64());
    
    // Format output in the beautiful style requested
    let output_string = match format {
        OutputFormat::Table => {
            use syncable_cli::analyzer::display::BoxDrawer;
            use colored::*;
            
            let mut output = String::new();
            
            // Header
            output.push_str(&format!("\n{}\n", "🛡️  Security Analysis Results".bright_white().bold()));
            output.push_str(&format!("{}\n", "═".repeat(80).bright_blue()));
            
            // Security Score Box
            let mut score_box = BoxDrawer::new("Security Summary");
            score_box.add_line("Overall Score:", &format!("{:.0}/100", security_report.overall_score).bright_yellow(), true);
            score_box.add_line("Risk Level:", &format!("{:?}", security_report.risk_level).color(match security_report.risk_level {
                TurboSecuritySeverity::Critical => "bright_red",
                TurboSecuritySeverity::High => "red", 
                TurboSecuritySeverity::Medium => "yellow",
                TurboSecuritySeverity::Low => "green",
                TurboSecuritySeverity::Info => "blue",
            }), true);
            score_box.add_line("Total Findings:", &security_report.total_findings.to_string().cyan(), true);
            
            // Analysis scope
            let config_files = security_report.findings.iter()
                .filter_map(|f| f.file_path.as_ref())
                .collect::<std::collections::HashSet<_>>()
                .len();
            score_box.add_line("Files Analyzed:", &config_files.max(1).to_string().green(), true);
            score_box.add_line("Scan Mode:", &format!("{:?}", scan_mode).green(), true);
            
            output.push_str(&format!("\n{}\n", score_box.draw()));
            
            // Findings in Card Format  
            if !security_report.findings.is_empty() {
                // Get terminal width to determine optimal display width
                let terminal_width = if let Some((width, _)) = term_size::dimensions() {
                    width.saturating_sub(10) // Leave some margin
                } else {
                    120 // Fallback width
                };
                
                let mut findings_box = BoxDrawer::new("Security Findings");
                
                for (i, finding) in security_report.findings.iter().enumerate() {
                    let severity_color = match finding.severity {
                        TurboSecuritySeverity::Critical => "bright_red",
                        TurboSecuritySeverity::High => "red",
                        TurboSecuritySeverity::Medium => "yellow", 
                        TurboSecuritySeverity::Low => "blue",
                        TurboSecuritySeverity::Info => "green",
                    };
                    
                    // Extract relative file path from project root
                    let file_display = if let Some(file_path) = &finding.file_path {
                        // Cross-platform path normalization
                        let canonical_file = file_path.canonicalize().unwrap_or_else(|_| file_path.clone());
                        let canonical_project = path.canonicalize().unwrap_or_else(|_| path.clone());
                        
                        // Try to calculate relative path from project root
                        if let Ok(relative_path) = canonical_file.strip_prefix(&canonical_project) {
                            // Use forward slashes for consistency across platforms
                            let relative_str = relative_path.to_string_lossy().replace('\\', "/");
                            format!("./{}", relative_str)
                        } else {
                            // Fallback: try to find any common ancestor or use absolute path
                            let path_str = file_path.to_string_lossy();
                            if path_str.starts_with('/') {
                                // For absolute paths, try to extract meaningful relative portion
                                if let Some(project_name) = path.file_name().and_then(|n| n.to_str()) {
                                    if let Some(project_idx) = path_str.rfind(project_name) {
                                        let relative_part = &path_str[project_idx + project_name.len()..];
                                        if relative_part.starts_with('/') {
                                            format!(".{}", relative_part)
                                        } else if !relative_part.is_empty() {
                                            format!("./{}", relative_part)
                                        } else {
                                            format!("./{}", file_path.file_name().unwrap_or_default().to_string_lossy())
                                        }
                                    } else {
                                        // Last resort: show the full path
                                        path_str.to_string()
                                    }
                                } else {
                                    // Show full path if we can't determine project context
                                    path_str.to_string()
                                }
                            } else {
                                // For relative paths that don't strip properly, use as-is
                                if path_str.starts_with("./") {
                                    path_str.to_string()
                                } else {
                                    format!("./{}", path_str)
                                }
                            }
                        }
                    } else {
                        "N/A".to_string()
                    };
                    
                    // Parse gitignore status from description (clean colored text)
                    let gitignore_status = if finding.description.contains("is tracked by git") {
                        "TRACKED".bright_red().bold()
                    } else if finding.description.contains("is NOT in .gitignore") {
                        "EXPOSED".yellow().bold()
                    } else if finding.description.contains("is protected") || finding.description.contains("properly ignored") {
                        "SAFE".bright_green().bold()
                    } else if finding.description.contains("appears safe") {
                        "OK".bright_blue().bold()
                    } else {
                        "UNKNOWN".dimmed()
                    };
                    
                    // Determine finding type
                    let finding_type = if finding.title.contains("Environment Variable") {
                        "ENV VAR"
                    } else if finding.title.contains("Secret File") {
                        "SECRET FILE"
                    } else if finding.title.contains("API Key") || finding.title.contains("Stripe") || finding.title.contains("Firebase") {
                        "API KEY"
                    } else if finding.title.contains("Configuration") {
                        "CONFIG"
                    } else {
                        "OTHER"
                    };
                    
                    // Format position as "line:column" or just "line" if no column info
                    let position_display = match (finding.line_number, finding.column_number) {
                        (Some(line), Some(col)) => format!("{}:{}", line, col),
                        (Some(line), None) => format!("{}", line),
                        _ => "—".to_string(),
                    };
                    
                    // Card format: File path with intelligent display based on terminal width
                    let box_margin = 6; // Account for box borders and padding
                    let available_width = terminal_width.saturating_sub(box_margin);
                    let max_path_width = available_width.saturating_sub(20); // Leave space for numbering and spacing
                    
                    if file_display.len() + 3 <= max_path_width {
                        // Path fits on one line with numbering
                        findings_box.add_value_only(&format!("{}. {}", 
                            format!("{}", i + 1).bright_white().bold(),
                            file_display.cyan().bold()
                        ));
                    } else if file_display.len() <= available_width.saturating_sub(4) {
                        // Path fits on its own line with indentation
                        findings_box.add_value_only(&format!("{}.", 
                            format!("{}", i + 1).bright_white().bold()
                        ));
                        findings_box.add_value_only(&format!("   {}", 
                            file_display.cyan().bold()
                        ));
                    } else {
                        // Path is extremely long - use smart wrapping
                        findings_box.add_value_only(&format!("{}.", 
                            format!("{}", i + 1).bright_white().bold()
                        ));
                        
                        // Smart path wrapping - prefer breaking at directory separators
                        let wrap_width = available_width.saturating_sub(4);
                        let mut remaining = file_display.as_str();
                        let mut first_line = true;
                        
                        while !remaining.is_empty() {
                            let prefix = if first_line { "   " } else { "     " };
                            let line_width = wrap_width.saturating_sub(prefix.len());
                            
                            if remaining.len() <= line_width {
                                // Last chunk fits entirely
                                findings_box.add_value_only(&format!("{}{}", 
                                    prefix, remaining.cyan().bold()
                                ));
                                break;
                            } else {
                                // Find a good break point (prefer directory separator)
                                let chunk = &remaining[..line_width];
                                let break_point = chunk.rfind('/').unwrap_or(line_width.saturating_sub(1));
                                
                                findings_box.add_value_only(&format!("{}{}", 
                                    prefix, chunk[..break_point].cyan().bold()
                                ));
                                remaining = &remaining[break_point..];
                                if remaining.starts_with('/') {
                                    remaining = &remaining[1..]; // Skip the separator
                                }
                            }
                            first_line = false;
                        }
                    }
                    
                    findings_box.add_value_only(&format!("   {} {} | {} {} | {} {} | {} {}", 
                        "Type:".dimmed(),
                        finding_type.yellow(),
                        "Severity:".dimmed(),
                        format!("{:?}", finding.severity).color(severity_color).bold(),
                        "Position:".dimmed(),
                        position_display.bright_cyan(),
                        "Status:".dimmed(),
                        gitignore_status
                    ));
                    
                    // Add spacing between findings (except for the last one)
                    if i < security_report.findings.len() - 1 {
                        findings_box.add_value_only("");
                    }
                }
                
                output.push_str(&format!("\n{}\n", findings_box.draw()));
                
                // GitIgnore Status Legend  
                let mut legend_box = BoxDrawer::new("Git Status Legend");
                legend_box.add_line(&"TRACKED:".bright_red().bold().to_string(), "File is tracked by git - CRITICAL RISK", false);
                legend_box.add_line(&"EXPOSED:".yellow().bold().to_string(), "File contains secrets but not in .gitignore", false);
                legend_box.add_line(&"SAFE:".bright_green().bold().to_string(), "File is properly ignored by .gitignore", false);
                legend_box.add_line(&"OK:".bright_blue().bold().to_string(), "File appears safe for version control", false);
                output.push_str(&format!("\n{}\n", legend_box.draw()));
            } else {
                let mut no_findings_box = BoxDrawer::new("Security Status");
                no_findings_box.add_value_only(&"✅ No security issues detected".green());
                no_findings_box.add_value_only("💡 Regular security scanning recommended");
                output.push_str(&format!("\n{}\n", no_findings_box.draw()));
            }
            
            // Recommendations Box
            let mut rec_box = BoxDrawer::new("Key Recommendations");
            if !security_report.recommendations.is_empty() {
                for (i, rec) in security_report.recommendations.iter().take(5).enumerate() {
                    // Clean up recommendation text
                    let clean_rec = rec.replace("Add these patterns to your .gitignore:", "Add to .gitignore:");
                    rec_box.add_value_only(&format!("{}. {}", i + 1, clean_rec));
                }
                if security_report.recommendations.len() > 5 {
                    rec_box.add_value_only(&format!("... and {} more recommendations", 
                        security_report.recommendations.len() - 5).dimmed());
                }
            } else {
                rec_box.add_value_only("✅ No immediate security concerns detected");
                rec_box.add_value_only("💡 Consider implementing dependency scanning");
                rec_box.add_value_only("💡 Review environment variable security practices");
            }
            output.push_str(&format!("\n{}\n", rec_box.draw()));
            
            output
        }
        OutputFormat::Json => {
            serde_json::to_string_pretty(&security_report)?
        }
    };
    
    // Output results
    if let Some(output_path) = output {
        std::fs::write(&output_path, output_string)?;
        println!("Security report saved to: {}", output_path.display());
    } else {
        print!("{}", output_string);
    }
    
    // Exit with error code if requested and findings exist
    if fail_on_findings && security_report.total_findings > 0 {
        let critical_count = security_report.findings_by_severity
            .get(&TurboSecuritySeverity::Critical)
            .unwrap_or(&0);
        let high_count = security_report.findings_by_severity
            .get(&TurboSecuritySeverity::High)
            .unwrap_or(&0);
        
        if *critical_count > 0 {
            eprintln!("❌ Critical security issues found. Please address immediately.");
            std::process::exit(1);
        } else if *high_count > 0 {
            eprintln!("⚠️  High severity security issues found. Review recommended.");
            std::process::exit(2);
        } else {
            eprintln!("ℹ️  Security issues found but none are critical or high severity.");
            std::process::exit(3);
        }
    }
    
    Ok(())
}

async fn handle_tools(command: ToolsCommand) -> syncable_cli::Result<()> {
    use syncable_cli::analyzer::{tool_installer::ToolInstaller, dependency_parser::Language};
    use std::collections::HashMap;
    use termcolor::{ColorChoice, StandardStream, WriteColor, ColorSpec, Color};
    
    match command {
        ToolsCommand::Status { format, languages } => {
            let installer = ToolInstaller::new();
            
            // Determine which languages to check
            let langs_to_check = if let Some(lang_names) = languages {
                lang_names.iter()
                    .filter_map(|name| Language::from_string(name))
                    .collect()
            } else {
                vec![
                    Language::Rust,
                    Language::JavaScript,
                    Language::TypeScript,
                    Language::Python,
                    Language::Go,
                    Language::Java,
                    Language::Kotlin,
                ]
            };
            
            println!("🔧 Checking vulnerability scanning tools status...\n");
            
            match format {
                OutputFormat::Table => {
                    let mut stdout = StandardStream::stdout(ColorChoice::Always);
                    
                    println!("📋 Vulnerability Scanning Tools Status");
                    println!("{}", "=".repeat(50));
                    
                    for language in &langs_to_check {
                        let (tool_name, is_available) = match language {
                            Language::Rust => ("cargo-audit", installer.test_tool_availability("cargo-audit")),
                            Language::JavaScript | Language::TypeScript => ("npm", installer.test_tool_availability("npm")),
                            Language::Python => ("pip-audit", installer.test_tool_availability("pip-audit")),
                            Language::Go => ("govulncheck", installer.test_tool_availability("govulncheck")),
                            Language::Java | Language::Kotlin => ("grype", installer.test_tool_availability("grype")),
                            _ => continue,
                        };
                        
                        print!("  {} {:?}: ", 
                               if is_available { "✅" } else { "❌" }, 
                               language);
                        
                        if is_available {
                            stdout.set_color(ColorSpec::new().set_fg(Some(Color::Green)))?;
                            print!("{} installed", tool_name);
                        } else {
                            stdout.set_color(ColorSpec::new().set_fg(Some(Color::Red)))?;
                            print!("{} missing", tool_name);
                        }
                        
                        stdout.reset()?;
                        println!();
                    }
                    
                    // Check universal tools
                    println!("\n🔍 Universal Scanners:");
                    let grype_available = installer.test_tool_availability("grype");
                    print!("  {} Grype: ", if grype_available { "✅" } else { "❌" });
                    if grype_available {
                        stdout.set_color(ColorSpec::new().set_fg(Some(Color::Green)))?;
                        println!("installed");
                    } else {
                        stdout.set_color(ColorSpec::new().set_fg(Some(Color::Red)))?;
                        println!("missing");
                    }
                    stdout.reset()?;
                }
                OutputFormat::Json => {
                    let mut status = HashMap::new();
                    
                    for language in &langs_to_check {
                        let (tool_name, is_available) = match language {
                            Language::Rust => ("cargo-audit", installer.test_tool_availability("cargo-audit")),
                            Language::JavaScript | Language::TypeScript => ("npm", installer.test_tool_availability("npm")),
                            Language::Python => ("pip-audit", installer.test_tool_availability("pip-audit")),
                            Language::Go => ("govulncheck", installer.test_tool_availability("govulncheck")),
                            Language::Java | Language::Kotlin => ("grype", installer.test_tool_availability("grype")),
                            _ => continue,
                        };
                        
                        status.insert(format!("{:?}", language), serde_json::json!({
                            "tool": tool_name,
                            "available": is_available
                        }));
                    }
                    
                    println!("{}", serde_json::to_string_pretty(&status)?);
                }
            }
        }
        
        ToolsCommand::Install { languages, include_owasp, dry_run, yes } => {
            let mut installer = ToolInstaller::new();
            
            // Determine which languages to install tools for
            let langs_to_install = if let Some(lang_names) = languages {
                lang_names.iter()
                    .filter_map(|name| Language::from_string(name))
                    .collect()
            } else {
                vec![
                    Language::Rust,
                    Language::JavaScript,
                    Language::TypeScript,
                    Language::Python,
                    Language::Go,
                    Language::Java,
                ]
            };
            
            if dry_run {
                println!("🔍 Dry run: Tools that would be installed:");
                println!("{}", "=".repeat(50));
                
                for language in &langs_to_install {
                    let (tool_name, is_available) = match language {
                        Language::Rust => ("cargo-audit", installer.test_tool_availability("cargo-audit")),
                        Language::JavaScript | Language::TypeScript => ("npm", installer.test_tool_availability("npm")),
                        Language::Python => ("pip-audit", installer.test_tool_availability("pip-audit")),
                        Language::Go => ("govulncheck", installer.test_tool_availability("govulncheck")),
                        Language::Java | Language::Kotlin => ("grype", installer.test_tool_availability("grype")),
                        _ => continue,
                    };
                    
                    if !is_available {
                        println!("  📦 Would install {} for {:?}", tool_name, language);
                    } else {
                        println!("  ✅ {} already installed for {:?}", tool_name, language);
                    }
                }
                
                if include_owasp && !installer.test_tool_availability("dependency-check") {
                    println!("  📦 Would install OWASP Dependency Check (large download)");
                }
                
                return Ok(());
            }
            
            if !yes {
                use std::io::{self, Write};
                print!("🔧 Install missing vulnerability scanning tools? [y/N]: ");
                io::stdout().flush()?;
                
                let mut input = String::new();
                io::stdin().read_line(&mut input)?;
                
                if !input.trim().to_lowercase().starts_with('y') {
                    println!("Installation cancelled.");
                    return Ok(());
                }
            }
            
            println!("🛠️  Installing vulnerability scanning tools...");
            
            match installer.ensure_tools_for_languages(&langs_to_install) {
                Ok(()) => {
                    println!("✅ Tool installation completed!");
                    installer.print_tool_status(&langs_to_install);
                    
                    // Show PATH instructions if needed
                    println!("\n💡 Setup Instructions:");
                    println!("  • Add ~/.local/bin to your PATH for manually installed tools");
                    println!("  • Add ~/go/bin to your PATH for Go tools");
                    println!("  • Add to your shell profile (~/.bashrc, ~/.zshrc, etc.):");
                    println!("    export PATH=\"$HOME/.local/bin:$HOME/go/bin:$PATH\"");
                }
                Err(e) => {
                    eprintln!("❌ Tool installation failed: {}", e);
                    eprintln!("\n🔧 Manual installation may be required for some tools.");
                    eprintln!("   Run 'sync-ctl tools guide' for manual installation instructions.");
                    return Err(e);
                }
            }
        }
        
        ToolsCommand::Verify { languages, verbose } => {
            let installer = ToolInstaller::new();
            
            // Determine which languages to verify
            let langs_to_verify = if let Some(lang_names) = languages {
                lang_names.iter()
                    .filter_map(|name| Language::from_string(name))
                    .collect()
            } else {
                vec![
                    Language::Rust,
                    Language::JavaScript,
                    Language::TypeScript,
                    Language::Python,
                    Language::Go,
                    Language::Java,
                ]
            };
            
            println!("🔍 Verifying vulnerability scanning tools...\n");
            
            let mut all_working = true;
            
            for language in &langs_to_verify {
                let (tool_name, is_working) = match language {
                    Language::Rust => {
                        let working = installer.test_tool_availability("cargo-audit");
                        ("cargo-audit", working)
                    }
                    Language::JavaScript | Language::TypeScript => {
                        let working = installer.test_tool_availability("npm");
                        ("npm", working)
                    }
                    Language::Python => {
                        let working = installer.test_tool_availability("pip-audit");
                        ("pip-audit", working)
                    }
                    Language::Go => {
                        let working = installer.test_tool_availability("govulncheck");
                        ("govulncheck", working)
                    }
                    Language::Java | Language::Kotlin => {
                        let working = installer.test_tool_availability("grype");
                        ("grype", working)
                    }
                    _ => continue,
                };
                
                print!("  {} {:?}: {}", 
                       if is_working { "✅" } else { "❌" }, 
                       language,
                       tool_name);
                
                if is_working {
                    println!(" - working correctly");
                    
                    if verbose {
                        // Try to get version info
                        use std::process::Command;
                        let version_result = match tool_name {
                            "cargo-audit" => Command::new("cargo").args(&["audit", "--version"]).output(),
                            "npm" => Command::new("npm").arg("--version").output(),
                            "pip-audit" => Command::new("pip-audit").arg("--version").output(),
                            "govulncheck" => Command::new("govulncheck").arg("-version").output(),
                            "grype" => Command::new("grype").arg("version").output(),
                            _ => continue,
                        };
                        
                        if let Ok(output) = version_result {
                            if output.status.success() {
                                let version = String::from_utf8_lossy(&output.stdout);
                                println!("    Version: {}", version.trim());
                            }
                        }
                    }
                } else {
                    println!(" - not working or missing");
                    all_working = false;
                }
            }
            
            if all_working {
                println!("\n✅ All tools are working correctly!");
            } else {
                println!("\n❌ Some tools are missing or not working.");
                println!("   Run 'sync-ctl tools install' to install missing tools.");
            }
        }
        
        ToolsCommand::Guide { languages, platform } => {
            let target_platform = platform.unwrap_or_else(|| {
                match std::env::consts::OS {
                    "macos" => "macOS".to_string(),
                    "linux" => "Linux".to_string(),
                    "windows" => "Windows".to_string(),
                    other => other.to_string(),
                }
            });
            
            println!("📚 Vulnerability Scanning Tools Installation Guide");
            println!("Platform: {}", target_platform);
            println!("{}", "=".repeat(60));
            
            let langs_to_show = if let Some(lang_names) = languages {
                lang_names.iter()
                    .filter_map(|name| Language::from_string(name))
                    .collect()
            } else {
                vec![
                    Language::Rust,
                    Language::JavaScript,
                    Language::TypeScript,
                    Language::Python,
                    Language::Go,
                    Language::Java,
                ]
            };
            
            for language in &langs_to_show {
                match language {
                    Language::Rust => {
                        println!("\n🦀 Rust - cargo-audit");
                        println!("  Install: cargo install cargo-audit");
                        println!("  Usage: cargo audit");
                    }
                    Language::JavaScript | Language::TypeScript => {
                        println!("\n🌐 JavaScript/TypeScript - npm audit");
                        println!("  Install: Download Node.js from https://nodejs.org/");
                        match target_platform.as_str() {
                            "macOS" => println!("  Package manager: brew install node"),
                            "Linux" => println!("  Package manager: sudo apt install nodejs npm (Ubuntu/Debian)"),
                            _ => {}
                        }
                        println!("  Usage: npm audit");
                    }
                    Language::Python => {
                        println!("\n🐍 Python - pip-audit");
                        println!("  Install: pipx install pip-audit (recommended)");
                        println!("  Alternative: pip3 install --user pip-audit");
                        println!("  Also available: safety (pip install safety)");
                        println!("  Usage: pip-audit");
                    }
                    Language::Go => {
                        println!("\n🐹 Go - govulncheck");
                        println!("  Install: go install golang.org/x/vuln/cmd/govulncheck@latest");
                        println!("  Note: Make sure ~/go/bin is in your PATH");
                        println!("  Usage: govulncheck ./...");
                    }
                    Language::Java => {
                        println!("\n☕ Java - Multiple options");
                        println!("  Grype (recommended):");
                        match target_platform.as_str() {
                            "macOS" => println!("    Install: brew install anchore/grype/grype"),
                            "Linux" => println!("    Install: Download from https://github.com/anchore/grype/releases"),
                            _ => println!("    Install: Download from https://github.com/anchore/grype/releases"),
                        }
                        println!("    Usage: grype .");
                        println!("  OWASP Dependency Check:");
                        match target_platform.as_str() {
                            "macOS" => println!("    Install: brew install dependency-check"),
                            _ => println!("    Install: Download from https://github.com/jeremylong/DependencyCheck/releases"),
                        }
                        println!("    Usage: dependency-check --project myproject --scan .");
                    }
                    _ => {}
                }
            }
            
            println!("\n🔍 Universal Scanners:");
            println!("  Grype: Works with multiple ecosystems");
            println!("  Trivy: Container and filesystem scanning");
            println!("  Snyk: Commercial solution with free tier");
            
            println!("\n💡 Tips:");
            println!("  • Run 'sync-ctl tools status' to check current installation");
            println!("  • Run 'sync-ctl tools install' for automatic installation");
            println!("  • Add tool directories to your PATH for easier access");
        }
    }
    
    Ok(())
}

/// Format project category for display
fn format_project_category(category: &ProjectCategory) -> &'static str {
    match category {
        ProjectCategory::Frontend => "Frontend",
        ProjectCategory::Backend => "Backend",
        ProjectCategory::Api => "API",
        ProjectCategory::Service => "Service",
        ProjectCategory::Library => "Library",
        ProjectCategory::Tool => "Tool",
        ProjectCategory::Documentation => "Documentation",
        ProjectCategory::Infrastructure => "Infrastructure",
        ProjectCategory::Unknown => "Unknown",
    }
}



================================================
FILE: src/.DS_Store
================================================
[Non-text file]


================================================
FILE: src/analyzer/dependency_parser.rs
================================================
use crate::analyzer::{AnalysisConfig, DetectedLanguage, DependencyMap};
use crate::analyzer::vulnerability_checker::{VulnerabilityChecker, VulnerabilityInfo};
use crate::error::{Result, AnalysisError};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use std::fs;
use log::{debug, info, warn};

/// Detailed dependency information
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct DependencyInfo {
    pub name: String,
    pub version: String,
    pub dep_type: DependencyType,
    pub license: String,
    pub source: Option<String>,
    pub language: Language,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum DependencyType {
    Production,
    Dev,
    Optional,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum Language {
    Rust,
    JavaScript,
    TypeScript,
    Python,
    Go,
    Java,
    Kotlin,
    Unknown,
}

impl Language {
    pub fn as_str(&self) -> &str {
        match self {
            Language::Rust => "Rust",
            Language::JavaScript => "JavaScript",
            Language::TypeScript => "TypeScript",
            Language::Python => "Python",
            Language::Go => "Go",
            Language::Java => "Java",
            Language::Kotlin => "Kotlin",
            Language::Unknown => "Unknown",
        }
    }

    pub fn from_string(s: &str) -> Option<Language> {
        match s.to_lowercase().as_str() {
            "rust" => Some(Language::Rust),
            "javascript" | "js" => Some(Language::JavaScript),
            "typescript" | "ts" => Some(Language::TypeScript),
            "python" | "py" => Some(Language::Python),
            "go" | "golang" => Some(Language::Go),
            "java" => Some(Language::Java),
            "kotlin" => Some(Language::Kotlin),
            _ => None,
        }
    }
}

/// Vulnerability information
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct Vulnerability {
    pub id: String,
    pub severity: VulnerabilitySeverity,
    pub description: String,
    pub fixed_in: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum VulnerabilitySeverity {
    Critical,
    High,
    Medium,
    Low,
    Info,
}

/// Legacy dependency info for existing code
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct LegacyDependencyInfo {
    pub version: String,
    pub is_dev: bool,
    pub license: Option<String>,
    pub vulnerabilities: Vec<Vulnerability>,
    pub source: String, // npm, crates.io, pypi, etc.
}

/// Enhanced dependency map with detailed information
pub type DetailedDependencyMap = HashMap<String, LegacyDependencyInfo>;

/// Result of dependency analysis
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct DependencyAnalysis {
    pub dependencies: DetailedDependencyMap,
    pub total_count: usize,
    pub production_count: usize,
    pub dev_count: usize,
    pub vulnerable_count: usize,
    pub license_summary: HashMap<String, usize>,
}

/// New dependency parser for vulnerability checking
pub struct DependencyParser;

impl DependencyParser {
    pub fn new() -> Self {
        Self
    }
    
    /// Check vulnerabilities for dependencies using the vulnerability checker
    async fn check_vulnerabilities_for_dependencies(
        &self,
        dependencies: &HashMap<Language, Vec<DependencyInfo>>,
        project_path: &Path,
    ) -> HashMap<String, Vec<VulnerabilityInfo>> {
        let mut vulnerability_map = HashMap::new();
        
        let checker = VulnerabilityChecker::new();
        
        match checker.check_all_dependencies(dependencies, project_path).await {
            Ok(report) => {
                info!("Found {} total vulnerabilities across all dependencies", report.total_vulnerabilities);
                
                // Map vulnerabilities by dependency name
                for vuln_dep in report.vulnerable_dependencies {
                    vulnerability_map.insert(vuln_dep.name, vuln_dep.vulnerabilities);
                }
            }
            Err(e) => {
                warn!("Failed to check vulnerabilities: {}", e);
            }
        }
        
        vulnerability_map
    }
    
    /// Convert VulnerabilityInfo to legacy Vulnerability format
    fn convert_vulnerability_info(vuln_info: &VulnerabilityInfo) -> Vulnerability {
        Vulnerability {
            id: vuln_info.id.clone(),
            severity: match vuln_info.severity {
                crate::analyzer::vulnerability_checker::VulnerabilitySeverity::Critical => VulnerabilitySeverity::Critical,
                crate::analyzer::vulnerability_checker::VulnerabilitySeverity::High => VulnerabilitySeverity::High,
                crate::analyzer::vulnerability_checker::VulnerabilitySeverity::Medium => VulnerabilitySeverity::Medium,
                crate::analyzer::vulnerability_checker::VulnerabilitySeverity::Low => VulnerabilitySeverity::Low,
                crate::analyzer::vulnerability_checker::VulnerabilitySeverity::Info => VulnerabilitySeverity::Info,
            },
            description: vuln_info.description.clone(),
            fixed_in: vuln_info.patched_versions.clone(),
        }
    }
    
    pub fn parse_all_dependencies(&self, project_root: &Path) -> Result<HashMap<Language, Vec<DependencyInfo>>> {
        let mut dependencies = HashMap::new();
        
        // Check for Rust
        if project_root.join("Cargo.toml").exists() {
            let rust_deps = self.parse_rust_deps(project_root)?;
            if !rust_deps.is_empty() {
                dependencies.insert(Language::Rust, rust_deps);
            }
        }
        
        // Check for JavaScript/TypeScript
        if project_root.join("package.json").exists() {
            let js_deps = self.parse_js_deps(project_root)?;
            if !js_deps.is_empty() {
                dependencies.insert(Language::JavaScript, js_deps);
            }
        }
        
        // Check for Python
        if project_root.join("requirements.txt").exists() || 
           project_root.join("pyproject.toml").exists() ||
           project_root.join("Pipfile").exists() {
            let py_deps = self.parse_python_deps(project_root)?;
            if !py_deps.is_empty() {
                dependencies.insert(Language::Python, py_deps);
            }
        }
        
        // Check for Go
        if project_root.join("go.mod").exists() {
            let go_deps = self.parse_go_deps(project_root)?;
            if !go_deps.is_empty() {
                dependencies.insert(Language::Go, go_deps);
            }
        }
        
        // Check for Java/Kotlin
        if project_root.join("pom.xml").exists() || project_root.join("build.gradle").exists() {
            let java_deps = self.parse_java_deps(project_root)?;
            if !java_deps.is_empty() {
                dependencies.insert(Language::Java, java_deps);
            }
        }
        
        Ok(dependencies)
    }
    
    fn parse_rust_deps(&self, project_root: &Path) -> Result<Vec<DependencyInfo>> {
        let cargo_lock = project_root.join("Cargo.lock");
        let cargo_toml = project_root.join("Cargo.toml");
        
        let mut deps = Vec::new();
        
        // First try to parse from Cargo.lock (complete dependency tree)
        if cargo_lock.exists() {
            let content = fs::read_to_string(&cargo_lock)?;
            let parsed: toml::Value = toml::from_str(&content)
                .map_err(|e| AnalysisError::DependencyParsing {
                    file: "Cargo.lock".to_string(),
                    reason: e.to_string(),
                })?;
            
            // Parse package list from Cargo.lock
            if let Some(packages) = parsed.get("package").and_then(|p| p.as_array()) {
                for package in packages {
                    if let Some(package_table) = package.as_table() {
                        if let (Some(name), Some(version)) = (
                            package_table.get("name").and_then(|n| n.as_str()),
                            package_table.get("version").and_then(|v| v.as_str())
                        ) {
                            // Determine if it's a direct dependency by checking Cargo.toml
                            let dep_type = self.get_rust_dependency_type(name, &cargo_toml);
                            
                            deps.push(DependencyInfo {
                                name: name.to_string(),
                                version: version.to_string(),
                                dep_type,
                                license: detect_rust_license(name).unwrap_or_else(|| "Unknown".to_string()),
                                source: Some("crates.io".to_string()),
                                language: Language::Rust,
                            });
                        }
                    }
                }
            }
        } else if cargo_toml.exists() {
            // Fallback to Cargo.toml if Cargo.lock doesn't exist
            let content = fs::read_to_string(&cargo_toml)?;
            let parsed: toml::Value = toml::from_str(&content)
                .map_err(|e| AnalysisError::DependencyParsing {
                    file: "Cargo.toml".to_string(),
                    reason: e.to_string(),
                })?;
            
            // Parse regular dependencies
            if let Some(dependencies) = parsed.get("dependencies").and_then(|d| d.as_table()) {
                for (name, value) in dependencies {
                    let version = extract_version_from_toml_value(value);
                    deps.push(DependencyInfo {
                        name: name.clone(),
                        version,
                        dep_type: DependencyType::Production,
                        license: detect_rust_license(name).unwrap_or_else(|| "Unknown".to_string()),
                        source: Some("crates.io".to_string()),
                        language: Language::Rust,
                    });
                }
            }
            
            // Parse dev dependencies
            if let Some(dev_deps) = parsed.get("dev-dependencies").and_then(|d| d.as_table()) {
                for (name, value) in dev_deps {
                    let version = extract_version_from_toml_value(value);
                    deps.push(DependencyInfo {
                        name: name.clone(),
                        version,
                        dep_type: DependencyType::Dev,
                        license: detect_rust_license(name).unwrap_or_else(|| "Unknown".to_string()),
                        source: Some("crates.io".to_string()),
                        language: Language::Rust,
                    });
                }
            }
        }
        
        Ok(deps)
    }
    
    fn get_rust_dependency_type(&self, dep_name: &str, cargo_toml_path: &Path) -> DependencyType {
        if !cargo_toml_path.exists() {
            return DependencyType::Production;
        }
        
        if let Ok(content) = fs::read_to_string(cargo_toml_path) {
            if let Ok(parsed) = toml::from_str::<toml::Value>(&content) {
                // Check if it's in dev-dependencies
                if let Some(dev_deps) = parsed.get("dev-dependencies").and_then(|d| d.as_table()) {
                    if dev_deps.contains_key(dep_name) {
                        return DependencyType::Dev;
                    }
                }
                
                // Check if it's in regular dependencies
                if let Some(deps) = parsed.get("dependencies").and_then(|d| d.as_table()) {
                    if deps.contains_key(dep_name) {
                        return DependencyType::Production;
                    }
                }
            }
        }
        
        // Default to production for transitive dependencies
        DependencyType::Production
    }
    
    fn parse_js_deps(&self, project_root: &Path) -> Result<Vec<DependencyInfo>> {
        let package_json = project_root.join("package.json");
        let content = fs::read_to_string(&package_json)?;
        let parsed: serde_json::Value = serde_json::from_str(&content)
            .map_err(|e| AnalysisError::DependencyParsing {
                file: "package.json".to_string(),
                reason: e.to_string(),
            })?;
        
        let mut deps = Vec::new();
        
        // Parse regular dependencies
        if let Some(dependencies) = parsed.get("dependencies").and_then(|d| d.as_object()) {
            for (name, version) in dependencies {
                if let Some(ver_str) = version.as_str() {
                    deps.push(DependencyInfo {
                        name: name.clone(),
                        version: ver_str.to_string(),
                        dep_type: DependencyType::Production,
                        license: detect_npm_license(name).unwrap_or_else(|| "Unknown".to_string()),
                        source: Some("npm".to_string()),
                        language: Language::JavaScript,
                    });
                }
            }
        }
        
        // Parse dev dependencies
        if let Some(dev_deps) = parsed.get("devDependencies").and_then(|d| d.as_object()) {
            for (name, version) in dev_deps {
                if let Some(ver_str) = version.as_str() {
                    deps.push(DependencyInfo {
                        name: name.clone(),
                        version: ver_str.to_string(),
                        dep_type: DependencyType::Dev,
                        license: detect_npm_license(name).unwrap_or_else(|| "Unknown".to_string()),
                        source: Some("npm".to_string()),
                        language: Language::JavaScript,
                    });
                }
            }
        }
        
        Ok(deps)
    }
    
    fn parse_python_deps(&self, project_root: &Path) -> Result<Vec<DependencyInfo>> {
        let mut deps = Vec::new();
        
        // Try pyproject.toml first (modern Python packaging)
        let pyproject = project_root.join("pyproject.toml");
        if pyproject.exists() {
            debug!("Found pyproject.toml, parsing Python dependencies");
            let content = fs::read_to_string(&pyproject)?;
            if let Ok(parsed) = toml::from_str::<toml::Value>(&content) {
                // Poetry dependencies
                if let Some(poetry_deps) = parsed
                    .get("tool")
                    .and_then(|t| t.get("poetry"))
                    .and_then(|p| p.get("dependencies"))
                    .and_then(|d| d.as_table())
                {
                    debug!("Found Poetry dependencies in pyproject.toml");
                    for (name, value) in poetry_deps {
                        if name != "python" {
                            let version = extract_version_from_toml_value(value);
                            deps.push(DependencyInfo {
                                name: name.clone(),
                                version,
                                dep_type: DependencyType::Production,
                                license: detect_pypi_license(name).unwrap_or_else(|| "Unknown".to_string()),
                                source: Some("pypi".to_string()),
                                language: Language::Python,
                            });
                        }
                    }
                }
                
                // Poetry dev dependencies
                if let Some(poetry_dev_deps) = parsed
                    .get("tool")
                    .and_then(|t| t.get("poetry"))
                    .and_then(|p| p.get("group"))
                    .and_then(|g| g.get("dev"))
                    .and_then(|d| d.get("dependencies"))
                    .and_then(|d| d.as_table())
                    .or_else(|| {
                        // Fallback to older Poetry format
                        parsed
                            .get("tool")
                            .and_then(|t| t.get("poetry"))
                            .and_then(|p| p.get("dev-dependencies"))
                            .and_then(|d| d.as_table())
                    })
                {
                    debug!("Found Poetry dev dependencies in pyproject.toml");
                    for (name, value) in poetry_dev_deps {
                        let version = extract_version_from_toml_value(value);
                        deps.push(DependencyInfo {
                            name: name.clone(),
                            version,
                            dep_type: DependencyType::Dev,
                            license: detect_pypi_license(name).unwrap_or_else(|| "Unknown".to_string()),
                            source: Some("pypi".to_string()),
                            language: Language::Python,
                        });
                    }
                }
                
                // PEP 621 dependencies (setuptools, flit, hatch, pdm)
                if let Some(project_deps) = parsed
                    .get("project")
                    .and_then(|p| p.get("dependencies"))
                    .and_then(|d| d.as_array())
                {
                    debug!("Found PEP 621 dependencies in pyproject.toml");
                    for dep in project_deps {
                        if let Some(dep_str) = dep.as_str() {
                            let (name, version) = self.parse_python_requirement_spec(dep_str);
                            deps.push(DependencyInfo {
                                name: name.clone(),
                                version,
                                dep_type: DependencyType::Production,
                                license: detect_pypi_license(&name).unwrap_or_else(|| "Unknown".to_string()),
                                source: Some("pypi".to_string()),
                                language: Language::Python,
                            });
                        }
                    }
                }
                
                // PEP 621 optional dependencies (test, dev, etc.)
                if let Some(optional_deps) = parsed
                    .get("project")
                    .and_then(|p| p.get("optional-dependencies"))
                    .and_then(|d| d.as_table())
                {
                    debug!("Found PEP 621 optional dependencies in pyproject.toml");
                    for (group_name, group_deps) in optional_deps {
                        if let Some(deps_array) = group_deps.as_array() {
                            let is_dev = group_name.contains("dev") || group_name.contains("test");
                            for dep in deps_array {
                                if let Some(dep_str) = dep.as_str() {
                                    let (name, version) = self.parse_python_requirement_spec(dep_str);
                                    deps.push(DependencyInfo {
                                        name: name.clone(),
                                        version,
                                        dep_type: if is_dev { DependencyType::Dev } else { DependencyType::Optional },
                                        license: detect_pypi_license(&name).unwrap_or_else(|| "Unknown".to_string()),
                                        source: Some("pypi".to_string()),
                                        language: Language::Python,
                                    });
                                }
                            }
                        }
                    }
                }
                
                // PDM dependencies
                if let Some(pdm_deps) = parsed
                    .get("tool")
                    .and_then(|t| t.get("pdm"))
                    .and_then(|p| p.get("dev-dependencies"))
                    .and_then(|d| d.as_table())
                {
                    debug!("Found PDM dev dependencies in pyproject.toml");
                    for (_group_name, group_deps) in pdm_deps {
                        if let Some(deps_array) = group_deps.as_array() {
                            for dep in deps_array {
                                if let Some(dep_str) = dep.as_str() {
                                    let (name, version) = self.parse_python_requirement_spec(dep_str);
                                    deps.push(DependencyInfo {
                                        name: name.clone(),
                                        version,
                                        dep_type: DependencyType::Dev,
                                        license: detect_pypi_license(&name).unwrap_or_else(|| "Unknown".to_string()),
                                        source: Some("pypi".to_string()),
                                        language: Language::Python,
                                    });
                                }
                            }
                        }
                    }
                }
                
                // Setuptools dependencies (legacy)
                if let Some(setuptools_deps) = parsed
                    .get("tool")
                    .and_then(|t| t.get("setuptools"))
                    .and_then(|s| s.get("dynamic"))
                    .and_then(|d| d.get("dependencies"))
                    .and_then(|d| d.as_array())
                {
                    debug!("Found setuptools dependencies in pyproject.toml");
                    for dep in setuptools_deps {
                        if let Some(dep_str) = dep.as_str() {
                            let (name, version) = self.parse_python_requirement_spec(dep_str);
                            deps.push(DependencyInfo {
                                name: name.clone(),
                                version,
                                dep_type: DependencyType::Production,
                                license: detect_pypi_license(&name).unwrap_or_else(|| "Unknown".to_string()),
                                source: Some("pypi".to_string()),
                                language: Language::Python,
                            });
                        }
                    }
                }
            }
        }
        
        // Try Pipfile (pipenv)
        let pipfile = project_root.join("Pipfile");
        if pipfile.exists() && deps.is_empty() {
            debug!("Found Pipfile, parsing pipenv dependencies");
            let content = fs::read_to_string(&pipfile)?;
            if let Ok(parsed) = toml::from_str::<toml::Value>(&content) {
                // Production dependencies
                if let Some(packages) = parsed.get("packages").and_then(|p| p.as_table()) {
                    for (name, value) in packages {
                        let version = extract_version_from_toml_value(value);
                        deps.push(DependencyInfo {
                            name: name.clone(),
                            version,
                            dep_type: DependencyType::Production,
                            license: detect_pypi_license(name).unwrap_or_else(|| "Unknown".to_string()),
                            source: Some("pypi".to_string()),
                            language: Language::Python,
                        });
                    }
                }
                
                // Dev dependencies
                if let Some(dev_packages) = parsed.get("dev-packages").and_then(|p| p.as_table()) {
                    for (name, value) in dev_packages {
                        let version = extract_version_from_toml_value(value);
                        deps.push(DependencyInfo {
                            name: name.clone(),
                            version,
                            dep_type: DependencyType::Dev,
                            license: detect_pypi_license(name).unwrap_or_else(|| "Unknown".to_string()),
                            source: Some("pypi".to_string()),
                            language: Language::Python,
                        });
                    }
                }
            }
        }
        
        // Try requirements.txt (legacy, but still widely used)
        let requirements_txt = project_root.join("requirements.txt");
        if requirements_txt.exists() && deps.is_empty() {
            debug!("Found requirements.txt, parsing legacy Python dependencies");
            let content = fs::read_to_string(&requirements_txt)?;
            for line in content.lines() {
                let line = line.trim();
                if !line.is_empty() && !line.starts_with('#') && !line.starts_with('-') {
                    let (name, version) = self.parse_python_requirement_spec(line);
                    deps.push(DependencyInfo {
                        name: name.clone(),
                        version,
                        dep_type: DependencyType::Production,
                        license: detect_pypi_license(&name).unwrap_or_else(|| "Unknown".to_string()),
                        source: Some("pypi".to_string()),
                        language: Language::Python,
                    });
                }
            }
        }
        
        // Try requirements-dev.txt
        let requirements_dev = project_root.join("requirements-dev.txt");
        if requirements_dev.exists() {
            debug!("Found requirements-dev.txt, parsing dev dependencies");
            let content = fs::read_to_string(&requirements_dev)?;
            for line in content.lines() {
                let line = line.trim();
                if !line.is_empty() && !line.starts_with('#') && !line.starts_with('-') {
                    let (name, version) = self.parse_python_requirement_spec(line);
                    deps.push(DependencyInfo {
                        name: name.clone(),
                        version,
                        dep_type: DependencyType::Dev,
                        license: detect_pypi_license(&name).unwrap_or_else(|| "Unknown".to_string()),
                        source: Some("pypi".to_string()),
                        language: Language::Python,
                    });
                }
            }
        }
        
        debug!("Parsed {} Python dependencies", deps.len());
        if !deps.is_empty() {
            debug!("Sample Python dependencies:");
            for dep in deps.iter().take(5) {
                debug!("  - {} v{} ({:?})", dep.name, dep.version, dep.dep_type);
            }
        }
        
        Ok(deps)
    }
    
    fn parse_go_deps(&self, project_root: &Path) -> Result<Vec<DependencyInfo>> {
        let go_mod = project_root.join("go.mod");
        let content = fs::read_to_string(&go_mod)?;
        let mut deps = Vec::new();
        let mut in_require_block = false;
        
        for line in content.lines() {
            let trimmed = line.trim();
            
            if trimmed.starts_with("require (") {
                in_require_block = true;
                continue;
            }
            
            if in_require_block && trimmed == ")" {
                in_require_block = false;
                continue;
            }
            
            if in_require_block || trimmed.starts_with("require ") {
                let parts: Vec<&str> = trimmed
                    .trim_start_matches("require ")
                    .split_whitespace()
                    .collect();
                
                if parts.len() >= 2 {
                    let name = parts[0];
                    let version = parts[1];
                    
                    deps.push(DependencyInfo {
                        name: name.to_string(),
                        version: version.to_string(),
                        dep_type: DependencyType::Production,
                        license: detect_go_license(name).unwrap_or("Unknown".to_string()),
                        source: Some("go modules".to_string()),
                        language: Language::Go,
                    });
                }
            }
        }
        
        Ok(deps)
    }
    
    /// Parse a Python requirement specification string (e.g., "package>=1.0.0")
    fn parse_python_requirement_spec(&self, spec: &str) -> (String, String) {
        // Handle requirement specification formats like:
        // - package==1.0.0
        // - package>=1.0.0,<2.0.0
        // - package~=1.0.0
        // - package[extra]>=1.0.0
        // - package
        
        let spec = spec.trim();
        
        // Remove any index URLs or other options
        let spec = if let Some(index) = spec.find("--") {
            &spec[..index]
        } else {
            spec
        }.trim();
        
        // Find the package name (before any version operators)
        let version_operators = ['=', '>', '<', '~', '!'];
        let version_start = spec.find(&version_operators[..]);
        
        if let Some(pos) = version_start {
            // Extract package name (including any extras)
            let package_part = spec[..pos].trim();
            let version_part = spec[pos..].trim();
            
            // Handle extras like package[extra] - keep them as part of the name
            let package_name = if package_part.contains('[') && package_part.contains(']') {
                // For packages with extras, extract just the base name
                if let Some(bracket_start) = package_part.find('[') {
                    package_part[..bracket_start].trim().to_string()
                } else {
                    package_part.to_string()
                }
            } else {
                package_part.to_string()
            };
            
            (package_name, version_part.to_string())
        } else {
            // No version specified - handle potential extras
            let package_name = if spec.contains('[') && spec.contains(']') {
                if let Some(bracket_start) = spec.find('[') {
                    spec[..bracket_start].trim().to_string()
                } else {
                    spec.to_string()
                }
            } else {
                spec.to_string()
            };
            
            (package_name, "*".to_string())
        }
    }
    
    fn parse_java_deps(&self, project_root: &Path) -> Result<Vec<DependencyInfo>> {
        let mut deps = Vec::new();
        
        debug!("Parsing Java dependencies in: {}", project_root.display());
        
        // Check for Maven pom.xml
        let pom_xml = project_root.join("pom.xml");
        if pom_xml.exists() {
            debug!("Found pom.xml, parsing Maven dependencies");
            let content = fs::read_to_string(&pom_xml)?;
            
            // Try to use the dependency:list Maven command first for accurate results
            if let Ok(maven_deps) = self.parse_maven_dependencies_with_command(project_root) {
                if !maven_deps.is_empty() {
                    debug!("Successfully parsed {} Maven dependencies using mvn command", maven_deps.len());
                    deps.extend(maven_deps);
                }
            }
            
            // If no deps from command, fall back to XML parsing
            if deps.is_empty() {
                debug!("Falling back to XML parsing for Maven dependencies");
                let xml_deps = self.parse_pom_xml(&content)?;
                debug!("Parsed {} dependencies from pom.xml", xml_deps.len());
                deps.extend(xml_deps);
            }
        }
        
        // Check for Gradle build.gradle or build.gradle.kts
        let build_gradle = project_root.join("build.gradle");
        let build_gradle_kts = project_root.join("build.gradle.kts");
        
        if (build_gradle.exists() || build_gradle_kts.exists()) && deps.is_empty() {
            debug!("Found Gradle build file, parsing Gradle dependencies");
            
                         // Try to use the dependencies Gradle command first
             if let Ok(gradle_deps) = self.parse_gradle_dependencies_with_command(project_root) {
                if !gradle_deps.is_empty() {
                    debug!("Successfully parsed {} Gradle dependencies using gradle command", gradle_deps.len());
                    deps.extend(gradle_deps);
                }
            }
            
            // If no deps from command, fall back to build file parsing
            if deps.is_empty() {
                if build_gradle.exists() {
                    debug!("Falling back to build.gradle parsing");
                    let content = fs::read_to_string(&build_gradle)?;
                    let gradle_deps = self.parse_gradle_build(&content)?;
                    debug!("Parsed {} dependencies from build.gradle", gradle_deps.len());
                    deps.extend(gradle_deps);
                }
                
                if build_gradle_kts.exists() && deps.is_empty() {
                    debug!("Falling back to build.gradle.kts parsing");
                    let content = fs::read_to_string(&build_gradle_kts)?;
                    let gradle_deps = self.parse_gradle_build(&content)?; // Same logic works for .kts
                    debug!("Parsed {} dependencies from build.gradle.kts", gradle_deps.len());
                    deps.extend(gradle_deps);
                }
            }
        }
        
        debug!("Total Java dependencies found: {}", deps.len());
        if !deps.is_empty() {
            debug!("Sample dependencies:");
            for dep in deps.iter().take(5) {
                debug!("  - {} v{}", dep.name, dep.version);
            }
        }
        
        Ok(deps)
    }
    
    /// Parse Maven dependencies using mvn dependency:list command
    fn parse_maven_dependencies_with_command(&self, project_root: &Path) -> Result<Vec<DependencyInfo>> {
        use std::process::Command;
        
        let output = Command::new("mvn")
            .args(&["dependency:list", "-DoutputFile=deps.txt", "-DappendOutput=false", "-DincludeScope=compile"])
            .current_dir(project_root)
            .output();
            
        match output {
            Ok(result) if result.status.success() => {
                // Read the generated deps.txt file
                let deps_file = project_root.join("deps.txt");
                if deps_file.exists() {
                    let content = fs::read_to_string(&deps_file)?;
                    let deps = self.parse_maven_dependency_list(&content)?;
                    
                    // Clean up
                    let _ = fs::remove_file(&deps_file);
                    
                    return Ok(deps);
                }
            }
            _ => {
                debug!("Maven command failed or not available, falling back to XML parsing");
            }
        }
        
        Ok(vec![])
    }
    
    /// Parse Gradle dependencies using gradle dependencies command
    fn parse_gradle_dependencies_with_command(&self, project_root: &Path) -> Result<Vec<DependencyInfo>> {
        use std::process::Command;
        
        // Try gradle first, then gradlew
        let gradle_cmds = vec!["gradle", "./gradlew"];
        
        for gradle_cmd in gradle_cmds {
            let output = Command::new(gradle_cmd)
                .args(&["dependencies", "--configuration=runtimeClasspath", "--console=plain"])
                .current_dir(project_root)
                .output();
                
            match output {
                Ok(result) if result.status.success() => {
                    let output_str = String::from_utf8_lossy(&result.stdout);
                    let deps = self.parse_gradle_dependency_tree(&output_str)?;
                    if !deps.is_empty() {
                        return Ok(deps);
                    }
                }
                _ => {
                    debug!("Gradle command '{}' failed, trying next", gradle_cmd);
                    continue;
                }
            }
        }
        
        debug!("All Gradle commands failed, falling back to build file parsing");
        Ok(vec![])
    }
    
    /// Parse Maven dependency list output
    fn parse_maven_dependency_list(&self, content: &str) -> Result<Vec<DependencyInfo>> {
        let mut deps = Vec::new();
        
        for line in content.lines() {
            let trimmed = line.trim();
            if trimmed.is_empty() || trimmed.starts_with("The following") || trimmed.starts_with("---") {
                continue;
            }
            
            // Format: groupId:artifactId:type:version:scope
            let parts: Vec<&str> = trimmed.split(':').collect();
            if parts.len() >= 4 {
                let group_id = parts[0];
                let artifact_id = parts[1];
                let version = parts[3];
                let scope = if parts.len() > 4 { parts[4] } else { "compile" };
                
                let name = format!("{}:{}", group_id, artifact_id);
                let dep_type = match scope {
                    "test" | "provided" => DependencyType::Dev,
                    _ => DependencyType::Production,
                };
                
                deps.push(DependencyInfo {
                    name,
                    version: version.to_string(),
                    dep_type,
                    license: "Unknown".to_string(),
                    source: Some("maven".to_string()),
                    language: Language::Java,
                });
            }
        }
        
        Ok(deps)
    }
    
    /// Parse Gradle dependency tree output
    fn parse_gradle_dependency_tree(&self, content: &str) -> Result<Vec<DependencyInfo>> {
        let mut deps = Vec::new();
        
        for line in content.lines() {
            let trimmed = line.trim();
            
            // Look for dependency lines that match pattern: +--- group:artifact:version
            if (trimmed.starts_with("+---") || trimmed.starts_with("\\---") || trimmed.starts_with("|")) 
                && trimmed.contains(':') {
                
                // Extract the dependency part
                let dep_part = if let Some(pos) = trimmed.find(' ') {
                    &trimmed[pos + 1..]
                } else {
                    trimmed
                };
                
                // Remove additional markers and get clean dependency string
                let clean_dep = dep_part
                    .replace(" (*)", "")
                    .replace(" (c)", "")
                    .replace(" (n)", "")
                    .replace("(*)", "")
                    .trim()
                    .to_string();
                
                let parts: Vec<&str> = clean_dep.split(':').collect();
                if parts.len() >= 3 {
                    let group_id = parts[0];
                    let artifact_id = parts[1];
                    let version = parts[2];
                    
                    let name = format!("{}:{}", group_id, artifact_id);
                    
                    deps.push(DependencyInfo {
                        name,
                        version: version.to_string(),
                        dep_type: DependencyType::Production,
                        license: "Unknown".to_string(),
                        source: Some("gradle".to_string()),
                        language: Language::Java,
                    });
                }
            }
        }
        
        Ok(deps)
    }
    
    /// Parse pom.xml file directly (fallback method)
    fn parse_pom_xml(&self, content: &str) -> Result<Vec<DependencyInfo>> {
        let mut deps = Vec::new();
        let mut in_dependencies = false;
        let mut in_dependency = false;
        let mut current_group_id = String::new();
        let mut current_artifact_id = String::new();
        let mut current_version = String::new();
        let mut current_scope = String::new();
        
        for line in content.lines() {
            let trimmed = line.trim();
            
            if trimmed.contains("<dependencies>") {
                in_dependencies = true;
                continue;
            }
            
            if trimmed.contains("</dependencies>") {
                in_dependencies = false;
                continue;
            }
            
            if in_dependencies {
                if trimmed.contains("<dependency>") {
                    in_dependency = true;
                    current_group_id.clear();
                    current_artifact_id.clear();
                    current_version.clear();
                    current_scope.clear();
                    continue;
                }
                
                if trimmed.contains("</dependency>") && in_dependency {
                    in_dependency = false;
                    
                    if !current_group_id.is_empty() && !current_artifact_id.is_empty() {
                        let name = format!("{}:{}", current_group_id, current_artifact_id);
                        let version = if current_version.is_empty() { 
                            "unknown".to_string() 
                        } else { 
                            current_version.clone() 
                        };
                        
                        let dep_type = match current_scope.as_str() {
                            "test" | "provided" => DependencyType::Dev,
                            _ => DependencyType::Production,
                        };
                        
                        deps.push(DependencyInfo {
                            name,
                            version,
                            dep_type,
                            license: "Unknown".to_string(),
                            source: Some("maven".to_string()),
                            language: Language::Java,
                        });
                    }
                    continue;
                }
                
                if in_dependency {
                    if trimmed.contains("<groupId>") {
                        current_group_id = extract_xml_value(trimmed, "groupId").to_string();
                    } else if trimmed.contains("<artifactId>") {
                        current_artifact_id = extract_xml_value(trimmed, "artifactId").to_string();
                    } else if trimmed.contains("<version>") {
                        current_version = extract_xml_value(trimmed, "version").to_string();
                    } else if trimmed.contains("<scope>") {
                        current_scope = extract_xml_value(trimmed, "scope").to_string();
                    }
                }
            }
        }
        
        Ok(deps)
    }
    
    /// Parse Gradle build file directly (fallback method)
    fn parse_gradle_build(&self, content: &str) -> Result<Vec<DependencyInfo>> {
        let mut deps = Vec::new();
        
        for line in content.lines() {
            let trimmed = line.trim();
            
            // Look for dependency declarations
            if (trimmed.starts_with("implementation ") || 
                trimmed.starts_with("compile ") ||
                trimmed.starts_with("api ") ||
                trimmed.starts_with("runtimeOnly ") ||
                trimmed.starts_with("testImplementation ") ||
                trimmed.starts_with("testCompile ")) {
                
                if let Some(dep_str) = extract_gradle_dependency(trimmed) {
                    let parts: Vec<&str> = dep_str.split(':').collect();
                    if parts.len() >= 3 {
                        let group_id = parts[0];
                        let artifact_id = parts[1];
                        let version = parts[2].trim_matches('"').trim_matches('\'');
                        
                        let name = format!("{}:{}", group_id, artifact_id);
                        let dep_type = if trimmed.starts_with("test") {
                            DependencyType::Dev
                        } else {
                            DependencyType::Production
                        };
                        
                        deps.push(DependencyInfo {
                            name,
                            version: version.to_string(),
                            dep_type,
                            license: "Unknown".to_string(),
                            source: Some("gradle".to_string()),
                            language: Language::Java,
                        });
                    }
                }
            }
        }
        
        Ok(deps)
    }
}

/// Parses project dependencies from various manifest files
pub fn parse_dependencies(
    project_root: &Path,
    languages: &[DetectedLanguage],
    _config: &AnalysisConfig,
) -> Result<DependencyMap> {
    let mut all_dependencies = DependencyMap::new();
    
    for language in languages {
        let deps = match language.name.as_str() {
            "Rust" => parse_rust_dependencies(project_root)?,
            "JavaScript" | "TypeScript" | "JavaScript/TypeScript" => parse_js_dependencies(project_root)?,
            "Python" => parse_python_dependencies(project_root)?,
            "Go" => parse_go_dependencies(project_root)?,
            "Java" | "Kotlin" | "Java/Kotlin" => parse_jvm_dependencies(project_root)?,
            _ => DependencyMap::new(),
        };
        all_dependencies.extend(deps);
    }
    
    Ok(all_dependencies)
}

/// Parse detailed dependencies with vulnerability and license information
pub async fn parse_detailed_dependencies(
    project_root: &Path,
    languages: &[DetectedLanguage],
    _config: &AnalysisConfig,
) -> Result<DependencyAnalysis> {
    let mut detailed_deps = DetailedDependencyMap::new();
    let mut license_summary = HashMap::new();
    
    // First, get all dependencies without vulnerabilities
    for language in languages {
        let deps = match language.name.as_str() {
            "Rust" => parse_rust_dependencies_detailed(project_root)?,
            "JavaScript" | "TypeScript" | "JavaScript/TypeScript" => parse_js_dependencies_detailed(project_root)?,
            "Python" => parse_python_dependencies_detailed(project_root)?,
            "Go" => parse_go_dependencies_detailed(project_root)?,
            "Java" | "Kotlin" | "Java/Kotlin" => parse_jvm_dependencies_detailed(project_root)?,
            _ => DetailedDependencyMap::new(),
        };
        
        // Update license summary
        for (_, dep_info) in &deps {
            if let Some(license) = &dep_info.license {
                *license_summary.entry(license.clone()).or_insert(0) += 1;
            }
        }
        
        detailed_deps.extend(deps);
    }
    
    // Check vulnerabilities for all dependencies
    let parser = DependencyParser::new();
    let all_deps = parser.parse_all_dependencies(project_root)?;
    let vulnerability_map = parser.check_vulnerabilities_for_dependencies(&all_deps, project_root).await;
    
    // Update dependencies with vulnerability information
    for (dep_name, dep_info) in detailed_deps.iter_mut() {
        if let Some(vulns) = vulnerability_map.get(dep_name) {
            dep_info.vulnerabilities = vulns.iter()
                .map(|v| DependencyParser::convert_vulnerability_info(v))
                .collect();
        }
    }
    
    let total_count = detailed_deps.len();
    let production_count = detailed_deps.values().filter(|d| !d.is_dev).count();
    let dev_count = detailed_deps.values().filter(|d| d.is_dev).count();
    let vulnerable_count = detailed_deps.values().filter(|d| !d.vulnerabilities.is_empty()).count();
    
    Ok(DependencyAnalysis {
        dependencies: detailed_deps,
        total_count,
        production_count,
        dev_count,
        vulnerable_count,
        license_summary,
    })
}

/// Parse Rust dependencies from Cargo.toml
fn parse_rust_dependencies(project_root: &Path) -> Result<DependencyMap> {
    let cargo_toml = project_root.join("Cargo.toml");
    if !cargo_toml.exists() {
        return Ok(DependencyMap::new());
    }
    
    let content = fs::read_to_string(&cargo_toml)?;
    let parsed: toml::Value = toml::from_str(&content)
        .map_err(|e| AnalysisError::DependencyParsing {
            file: "Cargo.toml".to_string(),
            reason: e.to_string(),
        })?;
    
    let mut deps = DependencyMap::new();
    
    // Parse regular dependencies
    if let Some(dependencies) = parsed.get("dependencies").and_then(|d| d.as_table()) {
        for (name, value) in dependencies {
            let version = extract_version_from_toml_value(value);
            deps.insert(name.clone(), version);
        }
    }
    
    // Parse dev dependencies
    if let Some(dev_deps) = parsed.get("dev-dependencies").and_then(|d| d.as_table()) {
        for (name, value) in dev_deps {
            let version = extract_version_from_toml_value(value);
            deps.insert(format!("{} (dev)", name), version);
        }
    }
    
    Ok(deps)
}

/// Parse detailed Rust dependencies
fn parse_rust_dependencies_detailed(project_root: &Path) -> Result<DetailedDependencyMap> {
    let cargo_toml = project_root.join("Cargo.toml");
    if !cargo_toml.exists() {
        return Ok(DetailedDependencyMap::new());
    }
    
    let content = fs::read_to_string(&cargo_toml)?;
    let parsed: toml::Value = toml::from_str(&content)
        .map_err(|e| AnalysisError::DependencyParsing {
            file: "Cargo.toml".to_string(),
            reason: e.to_string(),
        })?;
    
    let mut deps = DetailedDependencyMap::new();
    
    // Parse regular dependencies
    if let Some(dependencies) = parsed.get("dependencies").and_then(|d| d.as_table()) {
        for (name, value) in dependencies {
            let version = extract_version_from_toml_value(value);
            deps.insert(name.clone(), LegacyDependencyInfo {
                version,
                is_dev: false,
                license: detect_rust_license(name),
                vulnerabilities: vec![], // Populated by vulnerability checker in parse_detailed_dependencies
                source: "crates.io".to_string(),
            });
        }
    }
    
    // Parse dev dependencies
    if let Some(dev_deps) = parsed.get("dev-dependencies").and_then(|d| d.as_table()) {
        for (name, value) in dev_deps {
            let version = extract_version_from_toml_value(value);
            deps.insert(name.clone(), LegacyDependencyInfo {
                version,
                is_dev: true,
                license: detect_rust_license(name),
                vulnerabilities: vec![], // Populated by vulnerability checker in parse_detailed_dependencies
                source: "crates.io".to_string(),
            });
        }
    }
    
    Ok(deps)
}

/// Parse JavaScript/Node.js dependencies from package.json
fn parse_js_dependencies(project_root: &Path) -> Result<DependencyMap> {
    let package_json = project_root.join("package.json");
    if !package_json.exists() {
        return Ok(DependencyMap::new());
    }
    
    let content = fs::read_to_string(&package_json)?;
    let parsed: serde_json::Value = serde_json::from_str(&content)
        .map_err(|e| AnalysisError::DependencyParsing {
            file: "package.json".to_string(),
            reason: e.to_string(),
        })?;
    
    let mut deps = DependencyMap::new();
    
    // Parse regular dependencies
    if let Some(dependencies) = parsed.get("dependencies").and_then(|d| d.as_object()) {
        for (name, version) in dependencies {
            if let Some(ver_str) = version.as_str() {
                deps.insert(name.clone(), ver_str.to_string());
            }
        }
    }
    
    // Parse dev dependencies
    if let Some(dev_deps) = parsed.get("devDependencies").and_then(|d| d.as_object()) {
        for (name, version) in dev_deps {
            if let Some(ver_str) = version.as_str() {
                deps.insert(format!("{} (dev)", name), ver_str.to_string());
            }
        }
    }
    
    Ok(deps)
}

/// Parse detailed JavaScript dependencies
fn parse_js_dependencies_detailed(project_root: &Path) -> Result<DetailedDependencyMap> {
    let package_json = project_root.join("package.json");
    if !package_json.exists() {
        return Ok(DetailedDependencyMap::new());
    }
    
    let content = fs::read_to_string(&package_json)?;
    let parsed: serde_json::Value = serde_json::from_str(&content)
        .map_err(|e| AnalysisError::DependencyParsing {
            file: "package.json".to_string(),
            reason: e.to_string(),
        })?;
    
    let mut deps = DetailedDependencyMap::new();
    
    // Parse regular dependencies
    if let Some(dependencies) = parsed.get("dependencies").and_then(|d| d.as_object()) {
        for (name, version) in dependencies {
            if let Some(ver_str) = version.as_str() {
                deps.insert(name.clone(), LegacyDependencyInfo {
                    version: ver_str.to_string(),
                    is_dev: false,
                    license: detect_npm_license(name),
                    vulnerabilities: vec![], // Populated by vulnerability checker in parse_detailed_dependencies
                    source: "npm".to_string(),
                });
            }
        }
    }
    
    // Parse dev dependencies
    if let Some(dev_deps) = parsed.get("devDependencies").and_then(|d| d.as_object()) {
        for (name, version) in dev_deps {
            if let Some(ver_str) = version.as_str() {
                deps.insert(name.clone(), LegacyDependencyInfo {
                    version: ver_str.to_string(),
                    is_dev: true,
                    license: detect_npm_license(name),
                    vulnerabilities: vec![], // Populated by vulnerability checker in parse_detailed_dependencies
                    source: "npm".to_string(),
                });
            }
        }
    }
    
    Ok(deps)
}

/// Parse Python dependencies from requirements.txt, Pipfile, or pyproject.toml
fn parse_python_dependencies(project_root: &Path) -> Result<DependencyMap> {
    let mut deps = DependencyMap::new();
    
    // Try requirements.txt first
    let requirements_txt = project_root.join("requirements.txt");
    if requirements_txt.exists() {
        let content = fs::read_to_string(&requirements_txt)?;
        for line in content.lines() {
            if !line.trim().is_empty() && !line.starts_with('#') {
                let parts: Vec<&str> = line.split(&['=', '>', '<', '~', '!'][..]).collect();
                if !parts.is_empty() {
                    let name = parts[0].trim();
                    let version = if parts.len() > 1 {
                        line[name.len()..].trim().to_string()
                    } else {
                        "*".to_string()
                    };
                    deps.insert(name.to_string(), version);
                }
            }
        }
    }
    
    // Try pyproject.toml
    let pyproject = project_root.join("pyproject.toml");
    if pyproject.exists() {
        let content = fs::read_to_string(&pyproject)?;
        if let Ok(parsed) = toml::from_str::<toml::Value>(&content) {
            // Poetry dependencies
            if let Some(poetry_deps) = parsed
                .get("tool")
                .and_then(|t| t.get("poetry"))
                .and_then(|p| p.get("dependencies"))
                .and_then(|d| d.as_table())
            {
                for (name, value) in poetry_deps {
                    if name != "python" {
                        let version = extract_version_from_toml_value(value);
                        deps.insert(name.clone(), version);
                    }
                }
            }
            
            // Poetry dev dependencies
            if let Some(poetry_dev_deps) = parsed
                .get("tool")
                .and_then(|t| t.get("poetry"))
                .and_then(|p| p.get("dev-dependencies"))
                .and_then(|d| d.as_table())
            {
                for (name, value) in poetry_dev_deps {
                    let version = extract_version_from_toml_value(value);
                    deps.insert(format!("{} (dev)", name), version);
                }
            }
            
            // PEP 621 dependencies
            if let Some(project_deps) = parsed
                .get("project")
                .and_then(|p| p.get("dependencies"))
                .and_then(|d| d.as_array())
            {
                for dep in project_deps {
                    if let Some(dep_str) = dep.as_str() {
                        let parts: Vec<&str> = dep_str.split(&['=', '>', '<', '~', '!'][..]).collect();
                        if !parts.is_empty() {
                            let name = parts[0].trim();
                            let version = if parts.len() > 1 {
                                dep_str[name.len()..].trim().to_string()
                            } else {
                                "*".to_string()
                            };
                            deps.insert(name.to_string(), version);
                        }
                    }
                }
            }
        }
    }
    
    Ok(deps)
}

/// Parse detailed Python dependencies
fn parse_python_dependencies_detailed(project_root: &Path) -> Result<DetailedDependencyMap> {
    let mut deps = DetailedDependencyMap::new();
    
    // Try requirements.txt first
    let requirements_txt = project_root.join("requirements.txt");
    if requirements_txt.exists() {
        let content = fs::read_to_string(&requirements_txt)?;
        for line in content.lines() {
            if !line.trim().is_empty() && !line.starts_with('#') {
                let parts: Vec<&str> = line.split(&['=', '>', '<', '~', '!'][..]).collect();
                if !parts.is_empty() {
                    let name = parts[0].trim();
                    let version = if parts.len() > 1 {
                        line[name.len()..].trim().to_string()
                    } else {
                        "*".to_string()
                    };
                    deps.insert(name.to_string(), LegacyDependencyInfo {
                        version,
                        is_dev: false,
                        license: detect_pypi_license(name),
                        vulnerabilities: vec![], // Populated by vulnerability checker in parse_detailed_dependencies
                        source: "pypi".to_string(),
                    });
                }
            }
        }
    }
    
    // Try pyproject.toml for more detailed info
    let pyproject = project_root.join("pyproject.toml");
    if pyproject.exists() {
        let content = fs::read_to_string(&pyproject)?;
        if let Ok(parsed) = toml::from_str::<toml::Value>(&content) {
            // Poetry dependencies
            if let Some(poetry_deps) = parsed
                .get("tool")
                .and_then(|t| t.get("poetry"))
                .and_then(|p| p.get("dependencies"))
                .and_then(|d| d.as_table())
            {
                for (name, value) in poetry_deps {
                    if name != "python" {
                        let version = extract_version_from_toml_value(value);
                        deps.insert(name.clone(), LegacyDependencyInfo {
                            version,
                            is_dev: false,
                            license: detect_pypi_license(name),
                            vulnerabilities: vec![],
                            source: "pypi".to_string(),
                        });
                    }
                }
            }
            
            // Poetry dev dependencies
            if let Some(poetry_dev_deps) = parsed
                .get("tool")
                .and_then(|t| t.get("poetry"))
                .and_then(|p| p.get("dev-dependencies"))
                .and_then(|d| d.as_table())
            {
                for (name, value) in poetry_dev_deps {
                    let version = extract_version_from_toml_value(value);
                    deps.insert(name.clone(), LegacyDependencyInfo {
                        version,
                        is_dev: true,
                        license: detect_pypi_license(name),
                        vulnerabilities: vec![],
                        source: "pypi".to_string(),
                    });
                }
            }
        }
    }
    
    Ok(deps)
}

/// Parse Go dependencies from go.mod
fn parse_go_dependencies(project_root: &Path) -> Result<DependencyMap> {
    let go_mod = project_root.join("go.mod");
    if !go_mod.exists() {
        return Ok(DependencyMap::new());
    }
    
    let content = fs::read_to_string(&go_mod)?;
    let mut deps = DependencyMap::new();
    let mut in_require_block = false;
    
    for line in content.lines() {
        let trimmed = line.trim();
        
        if trimmed.starts_with("require (") {
            in_require_block = true;
            continue;
        }
        
        if in_require_block && trimmed == ")" {
            in_require_block = false;
            continue;
        }
        
        if in_require_block || trimmed.starts_with("require ") {
            let parts: Vec<&str> = trimmed
                .trim_start_matches("require ")
                .split_whitespace()
                .collect();
            
            if parts.len() >= 2 {
                let name = parts[0];
                let version = parts[1];
                deps.insert(name.to_string(), version.to_string());
            }
        }
    }
    
    Ok(deps)
}

/// Parse detailed Go dependencies
fn parse_go_dependencies_detailed(project_root: &Path) -> Result<DetailedDependencyMap> {
    let go_mod = project_root.join("go.mod");
    if !go_mod.exists() {
        return Ok(DetailedDependencyMap::new());
    }
    
    let content = fs::read_to_string(&go_mod)?;
    let mut deps = DetailedDependencyMap::new();
    let mut in_require_block = false;
    
    for line in content.lines() {
        let trimmed = line.trim();
        
        if trimmed.starts_with("require (") {
            in_require_block = true;
            continue;
        }
        
        if in_require_block && trimmed == ")" {
            in_require_block = false;
            continue;
        }
        
        if in_require_block || trimmed.starts_with("require ") {
            let parts: Vec<&str> = trimmed
                .trim_start_matches("require ")
                .split_whitespace()
                .collect();
            
            if parts.len() >= 2 {
                let name = parts[0];
                let version = parts[1];
                let is_indirect = parts.len() > 2 && parts.contains(&"//") && parts.contains(&"indirect");
                
                deps.insert(name.to_string(), LegacyDependencyInfo {
                    version: version.to_string(),
                    is_dev: is_indirect,
                    license: detect_go_license(name),
                    vulnerabilities: vec![], // Populated by vulnerability checker in parse_detailed_dependencies
                    source: "go modules".to_string(),
                });
            }
        }
    }
    
    Ok(deps)
}

/// Parse JVM dependencies from pom.xml or build.gradle
fn parse_jvm_dependencies(project_root: &Path) -> Result<DependencyMap> {
    let mut deps = DependencyMap::new();
    
    // Try pom.xml (Maven)
    let pom_xml = project_root.join("pom.xml");
    if pom_xml.exists() {
        // Simple XML parsing for demonstration
        // In production, use a proper XML parser
        let content = fs::read_to_string(&pom_xml)?;
        let lines: Vec<&str> = content.lines().collect();
        
        for i in 0..lines.len() {
            if lines[i].contains("<dependency>") {
                let mut group_id = "";
                let mut artifact_id = "";
                let mut version = "";
                
                for j in i..lines.len() {
                    if lines[j].contains("</dependency>") {
                        break;
                    }
                    if lines[j].contains("<groupId>") {
                        group_id = extract_xml_value(lines[j], "groupId");
                    }
                    if lines[j].contains("<artifactId>") {
                        artifact_id = extract_xml_value(lines[j], "artifactId");
                    }
                    if lines[j].contains("<version>") {
                        version = extract_xml_value(lines[j], "version");
                    }
                }
                
                if !group_id.is_empty() && !artifact_id.is_empty() {
                    let name = format!("{}:{}", group_id, artifact_id);
                    deps.insert(name, version.to_string());
                }
            }
        }
    }
    
    // Try build.gradle (Gradle)
    let build_gradle = project_root.join("build.gradle");
    if build_gradle.exists() {
        let content = fs::read_to_string(&build_gradle)?;
        
        // Simple pattern matching for Gradle dependencies
        for line in content.lines() {
            let trimmed = line.trim();
            if trimmed.starts_with("implementation") || 
               trimmed.starts_with("compile") ||
               trimmed.starts_with("testImplementation") ||
               trimmed.starts_with("testCompile") {
                
                if let Some(dep_str) = extract_gradle_dependency(trimmed) {
                    let parts: Vec<&str> = dep_str.split(':').collect();
                    if parts.len() >= 3 {
                        let name = format!("{}:{}", parts[0], parts[1]);
                        let version = parts[2];
                        let is_test = trimmed.starts_with("test");
                        let key = if is_test { format!("{} (test)", name) } else { name };
                        deps.insert(key, version.to_string());
                    }
                }
            }
        }
    }
    
    Ok(deps)
}

/// Parse detailed JVM dependencies
fn parse_jvm_dependencies_detailed(project_root: &Path) -> Result<DetailedDependencyMap> {
    let mut deps = DetailedDependencyMap::new();
    
    // Try pom.xml (Maven)
    let pom_xml = project_root.join("pom.xml");
    if pom_xml.exists() {
        let content = fs::read_to_string(&pom_xml)?;
        let lines: Vec<&str> = content.lines().collect();
        
        for i in 0..lines.len() {
            if lines[i].contains("<dependency>") {
                let mut group_id = "";
                let mut artifact_id = "";
                let mut version = "";
                let mut scope = "compile";
                
                for j in i..lines.len() {
 